/home/sdsra/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[INFO|asb_main.py:763] 2018-10-19 17:54:47,872 > ### Loading data...
[INFO|preprocessing.py:27] 2018-10-19 17:54:49,178 > x_char_seq_ind=(21375, 500)
[INFO|preprocessing.py:28] 2018-10-19 17:54:49,178 > y shape=(21375,)
[INFO|preprocessing.py:27] 2018-10-19 17:54:49,190 > x_char_seq_ind=(147, 500)
[INFO|preprocessing.py:28] 2018-10-19 17:54:49,190 > y shape=(147,)
[INFO|asb_main.py:774] 2018-10-19 17:54:49,191 > Train/Validate split: 21375/147
[INFO|asb_main.py:540] 2018-10-19 17:54:49,191 > ### Learning Start!
2018-10-19 17:54:49.191179: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-19 17:54:49.289777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-19 17:54:49.290327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.70GiB
2018-10-19 17:54:49.290339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-10-19 17:54:49.464362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-19 17:54:49.464393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-10-19 17:54:49.464399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-10-19 17:54:49.464549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10347 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/sdsra/Documents/Char-level-tf/model.py:640: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

[INFO|asb_main.py:74] 2018-10-19 17:54:50,861 > Writing to /home/sdsra/Documents/Char-level-tf/runs/1539996890_model_5
[INFO|asb_main.py:47] 2018-10-19 17:54:50,895 > ### Parameters ----------------------------------
[INFO|asb_main.py:53] 2018-10-19 17:54:50,896 > 	epochs : 1000
[INFO|asb_main.py:53] 2018-10-19 17:54:50,896 > 	batch_size : 128
[INFO|asb_main.py:53] 2018-10-19 17:54:50,896 > 	learning_rate : 0.001
[INFO|asb_main.py:53] 2018-10-19 17:54:50,896 > 	dropout_rate : 0.5
[INFO|asb_main.py:53] 2018-10-19 17:54:50,896 > 	alphabet : abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'"/\|_@#$%^&*~`+-=<>()[]{}

[INFO|asb_main.py:53] 2018-10-19 17:54:50,896 > 	char_max_length : 500
[INFO|asb_main.py:53] 2018-10-19 17:54:50,896 > 	num_of_classes : 3
[INFO|asb_main.py:53] 2018-10-19 17:54:50,896 > 	input_num_of_rows : 3
[INFO|asb_main.py:57] 2018-10-19 17:54:50,896 > ### Values of Graph  ----------------------------------
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer1/conv2d/kernel:0' shape=(3, 3, 1, 32) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer1/conv2d/bias:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer1/batch_normalization/gamma:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer1/batch_normalization/beta:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer1/batch_normalization/moving_mean:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer1/batch_normalization/moving_variance:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer2/conv2d/kernel:0' shape=(3, 3, 32, 32) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer2/conv2d/bias:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer2/batch_normalization/gamma:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer2/batch_normalization/beta:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer2/batch_normalization/moving_mean:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer2/batch_normalization/moving_variance:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer3/conv2d/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer3/conv2d/bias:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer3/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer3/batch_normalization/beta:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer3/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,896 > 	<tf.Variable 'Layer3/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'Layer4/conv2d/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'Layer4/conv2d/bias:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'Layer4/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'Layer4/batch_normalization/beta:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'Layer4/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'Layer4/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'FC/dense/kernel:0' shape=(28672, 1024) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'FC/dense/bias:0' shape=(1024,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'dense/kernel:0' shape=(1024, 3) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-19 17:54:50,897 > 	<tf.Variable 'dense/bias:0' shape=(3,) dtype=float32_ref>
[INFO|asb_main.py:65] 2018-10-19 17:54:50,897 > --------------------------------------------------------------------
2018-10-19 17:54:51.818587: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.60GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-19 17:54:52.479790: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.52GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
[INFO|asb_main.py:613] 2018-10-19 17:56:00,216 > Epoch:   1,  train_loss=1.685357625,  val_loss=0.967366397,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-19 17:57:07,995 > Epoch:   2,  train_loss=0.140873692,  val_loss=0.970317662,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-19 17:58:15,836 > Epoch:   3,  train_loss=0.073321271,  val_loss=1.007772803,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-19 17:59:23,645 > Epoch:   4,  train_loss=0.029246580,  val_loss=1.019966602,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-19 18:00:31,451 > Epoch:   5,  train_loss=0.019715988,  val_loss=1.029558897,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 18:01:39,265 > Epoch:   6,  train_loss=0.010950572,  val_loss=1.038887978,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 18:02:47,069 > Epoch:   7,  train_loss=0.007178775,  val_loss=1.027135730,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-19 18:03:54,815 > Epoch:   8,  train_loss=0.003125311,  val_loss=1.011550426,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-19 18:05:02,676 > Epoch:   9,  train_loss=0.001752681,  val_loss=1.018895388,  accuracy=0.671875000
[INFO|asb_main.py:613] 2018-10-19 18:06:10,479 > Epoch:  10,  train_loss=0.000924380,  val_loss=1.021102905,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-19 18:07:18,249 > Epoch:  11,  train_loss=0.000661036,  val_loss=1.021845818,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-19 18:08:26,024 > Epoch:  12,  train_loss=0.000589383,  val_loss=1.013707399,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-19 18:09:33,811 > Epoch:  13,  train_loss=0.000608277,  val_loss=1.013941765,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-19 18:10:41,636 > Epoch:  14,  train_loss=0.000647983,  val_loss=1.022518873,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-19 18:11:49,478 > Epoch:  15,  train_loss=0.000613162,  val_loss=1.021726489,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-19 18:12:57,284 > Epoch:  16,  train_loss=0.000600371,  val_loss=1.032396317,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-19 18:14:05,097 > Epoch:  17,  train_loss=0.000651202,  val_loss=1.032957792,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-19 18:15:12,899 > Epoch:  18,  train_loss=0.000602505,  val_loss=1.034168243,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-19 18:16:20,681 > Epoch:  19,  train_loss=0.000600699,  val_loss=1.021563053,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-19 18:17:28,462 > Epoch:  20,  train_loss=0.000599127,  val_loss=1.023209691,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-19 18:18:36,223 > Epoch:  21,  train_loss=0.000643040,  val_loss=1.037455082,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-19 18:19:43,990 > Epoch:  22,  train_loss=0.000644022,  val_loss=1.036486387,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-19 18:20:51,807 > Epoch:  23,  train_loss=0.000679858,  val_loss=1.037967086,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-19 18:21:59,642 > Epoch:  24,  train_loss=0.000726454,  val_loss=1.037082434,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-19 18:23:07,459 > Epoch:  25,  train_loss=0.280689508,  val_loss=0.943363428,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-19 18:24:15,254 > Epoch:  26,  train_loss=0.089689104,  val_loss=0.948167145,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-19 18:25:23,021 > Epoch:  27,  train_loss=0.040514822,  val_loss=1.008639097,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-19 18:26:30,773 > Epoch:  28,  train_loss=0.019192635,  val_loss=1.040092230,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-19 18:27:38,544 > Epoch:  29,  train_loss=0.010246359,  val_loss=1.043386579,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-19 18:28:46,367 > Epoch:  30,  train_loss=0.003659509,  val_loss=1.063963294,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-19 18:29:54,161 > Epoch:  31,  train_loss=0.002295787,  val_loss=1.065215349,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-19 18:31:02,083 > Epoch:  32,  train_loss=0.001093156,  val_loss=1.073006272,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-19 18:32:09,871 > Epoch:  33,  train_loss=0.000639185,  val_loss=1.074331045,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-19 18:33:17,663 > Epoch:  34,  train_loss=0.000561385,  val_loss=1.074019909,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-19 18:34:25,443 > Epoch:  35,  train_loss=0.000462799,  val_loss=1.074986219,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-19 18:35:33,248 > Epoch:  36,  train_loss=0.000452486,  val_loss=1.073872566,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-19 18:36:41,103 > Epoch:  37,  train_loss=0.000433462,  val_loss=1.071461439,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-19 18:37:48,915 > Epoch:  38,  train_loss=0.000449022,  val_loss=1.066464782,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-19 18:38:56,715 > Epoch:  39,  train_loss=0.000452755,  val_loss=1.064581513,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-19 18:40:04,491 > Epoch:  40,  train_loss=0.000453295,  val_loss=1.063138485,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-19 18:41:12,309 > Epoch:  41,  train_loss=0.000429994,  val_loss=1.062615991,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-19 18:42:20,111 > Epoch:  42,  train_loss=0.000430414,  val_loss=1.062179327,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-19 18:43:27,916 > Epoch:  43,  train_loss=0.000423184,  val_loss=1.060332537,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-19 18:44:35,779 > Epoch:  44,  train_loss=0.000398420,  val_loss=1.060472608,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-19 18:45:43,599 > Epoch:  45,  train_loss=0.000397472,  val_loss=1.055859089,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-19 18:46:51,488 > Epoch:  46,  train_loss=0.000389174,  val_loss=1.055935979,  accuracy=0.671875000
[INFO|asb_main.py:613] 2018-10-19 18:47:59,310 > Epoch:  47,  train_loss=0.000478246,  val_loss=1.052258968,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-19 18:49:07,135 > Epoch:  48,  train_loss=0.000411938,  val_loss=1.052937031,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-19 18:50:14,938 > Epoch:  49,  train_loss=0.000404695,  val_loss=1.054073691,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-19 18:51:22,701 > Epoch:  50,  train_loss=0.000391213,  val_loss=1.051445723,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-19 18:52:30,515 > Epoch:  51,  train_loss=0.000376286,  val_loss=1.044705868,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-19 18:53:38,360 > Epoch:  52,  train_loss=0.000375647,  val_loss=1.043054342,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-19 18:54:46,138 > Epoch:  53,  train_loss=0.000373844,  val_loss=1.048236966,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-19 18:55:53,959 > Epoch:  54,  train_loss=0.141248582,  val_loss=0.588631034,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-19 18:57:01,758 > Epoch:  55,  train_loss=0.044872460,  val_loss=0.649574757,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-19 18:58:09,622 > Epoch:  56,  train_loss=0.006654310,  val_loss=0.622108877,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 18:59:17,399 > Epoch:  57,  train_loss=0.001677257,  val_loss=0.631832004,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:00:25,200 > Epoch:  58,  train_loss=0.000614225,  val_loss=0.632314384,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-19 19:01:33,016 > Epoch:  59,  train_loss=0.000379010,  val_loss=0.635600686,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-19 19:02:40,834 > Epoch:  60,  train_loss=0.000322780,  val_loss=0.644176722,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:03:48,618 > Epoch:  61,  train_loss=0.000279320,  val_loss=0.642784774,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:04:56,462 > Epoch:  62,  train_loss=0.000305583,  val_loss=0.639148951,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:06:04,259 > Epoch:  63,  train_loss=0.000265313,  val_loss=0.640915990,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-19 19:07:12,019 > Epoch:  64,  train_loss=0.000272836,  val_loss=0.634446323,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-19 19:08:19,842 > Epoch:  65,  train_loss=0.000253096,  val_loss=0.633203506,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:09:27,652 > Epoch:  66,  train_loss=0.000251374,  val_loss=0.630694151,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-19 19:10:35,454 > Epoch:  67,  train_loss=0.000256812,  val_loss=0.632743478,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:11:43,315 > Epoch:  68,  train_loss=0.000261769,  val_loss=0.628314316,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-19 19:12:51,124 > Epoch:  69,  train_loss=0.000274687,  val_loss=0.631134629,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:13:58,901 > Epoch:  70,  train_loss=0.000249055,  val_loss=0.628221273,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-19 19:15:06,667 > Epoch:  71,  train_loss=0.000250945,  val_loss=0.630405068,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-19 19:16:14,482 > Epoch:  72,  train_loss=0.000249219,  val_loss=0.629902780,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-19 19:17:22,308 > Epoch:  73,  train_loss=0.000248477,  val_loss=0.627990484,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-19 19:18:30,100 > Epoch:  74,  train_loss=0.000243431,  val_loss=0.628290832,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-19 19:19:37,898 > Epoch:  75,  train_loss=0.000245048,  val_loss=0.633494735,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-19 19:20:45,726 > Epoch:  76,  train_loss=0.000241438,  val_loss=0.643698335,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-19 19:21:53,528 > Epoch:  77,  train_loss=0.000280215,  val_loss=0.646127224,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:23:01,314 > Epoch:  78,  train_loss=0.000262826,  val_loss=0.645045400,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-19 19:24:09,106 > Epoch:  79,  train_loss=0.000239411,  val_loss=0.655855536,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:25:16,908 > Epoch:  80,  train_loss=0.000240841,  val_loss=0.649028182,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:26:24,711 > Epoch:  81,  train_loss=0.000245397,  val_loss=0.657343149,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:27:32,519 > Epoch:  82,  train_loss=0.000229591,  val_loss=0.657992244,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-19 19:28:40,312 > Epoch:  83,  train_loss=0.000244895,  val_loss=0.648278356,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-19 19:29:48,158 > Epoch:  84,  train_loss=0.000271778,  val_loss=0.647470951,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:30:55,962 > Epoch:  85,  train_loss=0.000214377,  val_loss=0.640774548,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:32:03,848 > Epoch:  86,  train_loss=0.000219150,  val_loss=0.650758982,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:33:11,626 > Epoch:  87,  train_loss=0.000238136,  val_loss=0.620408118,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:34:19,462 > Epoch:  88,  train_loss=0.000230479,  val_loss=0.630128264,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-19 19:35:27,243 > Epoch:  89,  train_loss=0.000204789,  val_loss=0.633123100,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:36:35,062 > Epoch:  90,  train_loss=0.000205935,  val_loss=0.637442708,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:37:42,858 > Epoch:  91,  train_loss=0.000201433,  val_loss=0.632413268,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:38:50,713 > Epoch:  92,  train_loss=0.000199220,  val_loss=0.657098353,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:39:58,512 > Epoch:  93,  train_loss=0.000270782,  val_loss=0.616843820,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-19 19:41:06,282 > Epoch:  94,  train_loss=0.000235714,  val_loss=0.661893725,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-19 19:42:14,089 > Epoch:  95,  train_loss=0.000167275,  val_loss=0.648664773,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-19 19:43:21,904 > Epoch:  96,  train_loss=0.000141989,  val_loss=0.652247071,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-19 19:44:29,728 > Epoch:  97,  train_loss=0.000146929,  val_loss=0.692027032,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-19 19:45:37,539 > Epoch:  98,  train_loss=0.053387789,  val_loss=0.630933046,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-19 19:46:45,352 > Epoch:  99,  train_loss=0.009966775,  val_loss=0.348770082,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-19 19:47:53,150 > Epoch: 100,  train_loss=0.000344691,  val_loss=0.348900169,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-19 19:49:00,982 > Epoch: 101,  train_loss=0.000225001,  val_loss=0.348739892,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 19:50:08,796 > Epoch: 102,  train_loss=0.000192707,  val_loss=0.348558575,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 19:51:16,583 > Epoch: 103,  train_loss=0.000186941,  val_loss=0.350085557,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 19:52:24,415 > Epoch: 104,  train_loss=0.000186959,  val_loss=0.349423200,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 19:53:32,235 > Epoch: 105,  train_loss=0.000184675,  val_loss=0.350549340,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 19:54:40,044 > Epoch: 106,  train_loss=0.000184498,  val_loss=0.350193918,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 19:55:47,833 > Epoch: 107,  train_loss=0.000185560,  val_loss=0.347239196,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 19:56:55,618 > Epoch: 108,  train_loss=0.000186555,  val_loss=0.343963623,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 19:58:03,420 > Epoch: 109,  train_loss=0.000188678,  val_loss=0.346337825,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 19:59:11,243 > Epoch: 110,  train_loss=0.000188570,  val_loss=0.347903788,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:00:19,044 > Epoch: 111,  train_loss=0.000181250,  val_loss=0.345806181,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:01:26,868 > Epoch: 112,  train_loss=0.000170375,  val_loss=0.343359590,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-19 20:02:34,685 > Epoch: 113,  train_loss=0.000183194,  val_loss=0.354778707,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-19 20:03:42,537 > Epoch: 114,  train_loss=0.000174171,  val_loss=0.352768242,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-19 20:04:50,322 > Epoch: 115,  train_loss=0.000173747,  val_loss=0.354405224,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:05:58,146 > Epoch: 116,  train_loss=0.000176099,  val_loss=0.360937446,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:07:05,914 > Epoch: 117,  train_loss=0.000186120,  val_loss=0.370594054,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:08:13,695 > Epoch: 118,  train_loss=0.000174745,  val_loss=0.367864937,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:09:21,496 > Epoch: 119,  train_loss=0.000173252,  val_loss=0.368184328,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:10:29,364 > Epoch: 120,  train_loss=0.000171836,  val_loss=0.373185813,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:11:37,196 > Epoch: 121,  train_loss=0.000170839,  val_loss=0.380621940,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:12:45,011 > Epoch: 122,  train_loss=0.000160034,  val_loss=0.386819690,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:13:52,816 > Epoch: 123,  train_loss=0.000140847,  val_loss=0.378946126,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-19 20:15:00,656 > Epoch: 124,  train_loss=0.000224411,  val_loss=0.389291167,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:16:08,466 > Epoch: 125,  train_loss=0.000267949,  val_loss=0.401015490,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:17:16,287 > Epoch: 126,  train_loss=0.000174542,  val_loss=0.403288066,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-19 20:18:24,093 > Epoch: 127,  train_loss=0.000153965,  val_loss=0.400080860,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:19:31,916 > Epoch: 128,  train_loss=0.000144063,  val_loss=0.406622678,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:20:39,750 > Epoch: 129,  train_loss=0.000137815,  val_loss=0.419078529,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-19 20:21:47,573 > Epoch: 130,  train_loss=0.000148232,  val_loss=0.432679921,  accuracy=0.921875000
[INFO|asb_main.py:613] 2018-10-19 20:22:55,351 > Epoch: 131,  train_loss=0.000129594,  val_loss=0.434892505,  accuracy=0.921875000
[INFO|asb_main.py:613] 2018-10-19 20:24:03,199 > Epoch: 132,  train_loss=0.000128416,  val_loss=0.424606889,  accuracy=0.921875000
[INFO|asb_main.py:613] 2018-10-19 20:25:11,042 > Epoch: 133,  train_loss=0.000126801,  val_loss=0.419541597,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-19 20:26:18,882 > Epoch: 134,  train_loss=0.000125378,  val_loss=0.409188569,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-19 20:27:26,726 > Epoch: 135,  train_loss=0.000127414,  val_loss=0.406050920,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:28:34,538 > Epoch: 136,  train_loss=0.000125338,  val_loss=0.408517957,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-19 20:29:42,329 > Epoch: 137,  train_loss=0.000121677,  val_loss=0.403493553,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-19 20:30:50,160 > Epoch: 138,  train_loss=0.000117424,  val_loss=0.400349468,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-19 20:31:57,987 > Epoch: 139,  train_loss=0.000139480,  val_loss=0.417392403,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:33:05,788 > Epoch: 140,  train_loss=0.000117022,  val_loss=0.424602866,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-19 20:34:13,595 > Epoch: 141,  train_loss=0.000123529,  val_loss=0.425489902,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-19 20:35:21,443 > Epoch: 142,  train_loss=0.000118445,  val_loss=0.428290129,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-19 20:36:29,253 > Epoch: 143,  train_loss=0.000117682,  val_loss=0.432835996,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-19 20:37:37,057 > Epoch: 144,  train_loss=0.000118351,  val_loss=0.438279927,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-19 20:38:44,843 > Epoch: 145,  train_loss=0.000133383,  val_loss=0.452875942,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 20:39:52,681 > Epoch: 146,  train_loss=0.000128487,  val_loss=0.448180377,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 20:41:00,501 > Epoch: 147,  train_loss=0.000140808,  val_loss=0.428475469,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 20:42:08,332 > Epoch: 148,  train_loss=0.116175384,  val_loss=4.146965504,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-19 20:43:16,152 > Epoch: 149,  train_loss=0.017772666,  val_loss=1.395316839,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 20:44:23,949 > Epoch: 150,  train_loss=0.000636165,  val_loss=1.293020606,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:45:31,740 > Epoch: 151,  train_loss=0.000305852,  val_loss=1.281243563,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:46:39,536 > Epoch: 152,  train_loss=0.000228397,  val_loss=1.276841164,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:47:47,404 > Epoch: 153,  train_loss=0.000202472,  val_loss=1.276752830,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:48:55,235 > Epoch: 154,  train_loss=0.000188808,  val_loss=1.277688622,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:50:03,069 > Epoch: 155,  train_loss=0.000180233,  val_loss=1.279073477,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:51:10,880 > Epoch: 156,  train_loss=0.000174260,  val_loss=1.282020926,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:52:18,788 > Epoch: 157,  train_loss=0.000170332,  val_loss=1.283597112,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:53:26,606 > Epoch: 158,  train_loss=0.000167360,  val_loss=1.285414219,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:54:34,426 > Epoch: 159,  train_loss=0.000165467,  val_loss=1.288337946,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:55:42,273 > Epoch: 160,  train_loss=0.000164258,  val_loss=1.290712476,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:56:50,121 > Epoch: 161,  train_loss=0.000163632,  val_loss=1.292637706,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:57:58,002 > Epoch: 162,  train_loss=0.000163381,  val_loss=1.294065475,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 20:59:05,774 > Epoch: 163,  train_loss=0.000163439,  val_loss=1.297160983,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:00:13,640 > Epoch: 164,  train_loss=0.000163967,  val_loss=1.299516678,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:01:21,491 > Epoch: 165,  train_loss=0.000164590,  val_loss=1.300009489,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:02:29,303 > Epoch: 166,  train_loss=0.000165749,  val_loss=1.300233126,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:03:37,136 > Epoch: 167,  train_loss=0.000165960,  val_loss=1.300481081,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:04:44,955 > Epoch: 168,  train_loss=0.000166701,  val_loss=1.302129507,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:05:52,766 > Epoch: 169,  train_loss=0.000167549,  val_loss=1.303344369,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:07:00,611 > Epoch: 170,  train_loss=0.000168144,  val_loss=1.301661849,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:08:08,457 > Epoch: 171,  train_loss=0.000169063,  val_loss=1.304762483,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:09:16,301 > Epoch: 172,  train_loss=0.000169606,  val_loss=1.301655412,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:10:24,084 > Epoch: 173,  train_loss=0.000170112,  val_loss=1.303021789,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:11:31,917 > Epoch: 174,  train_loss=0.000170333,  val_loss=1.306161880,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:12:39,708 > Epoch: 175,  train_loss=0.000170576,  val_loss=1.301604390,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 21:13:47,527 > Epoch: 176,  train_loss=0.000170501,  val_loss=1.292138696,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:14:55,368 > Epoch: 177,  train_loss=0.000169808,  val_loss=1.290341854,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:16:03,162 > Epoch: 178,  train_loss=0.000169797,  val_loss=1.281443357,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:17:11,021 > Epoch: 179,  train_loss=0.000168902,  val_loss=1.288074493,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:18:18,870 > Epoch: 180,  train_loss=0.000166457,  val_loss=1.297394037,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:19:26,748 > Epoch: 181,  train_loss=0.000178885,  val_loss=1.288449407,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:20:34,572 > Epoch: 182,  train_loss=0.000166411,  val_loss=1.284047365,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 21:21:42,393 > Epoch: 183,  train_loss=0.000165203,  val_loss=1.271580219,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 21:22:50,216 > Epoch: 184,  train_loss=0.000162922,  val_loss=1.279472709,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:23:58,032 > Epoch: 185,  train_loss=0.000161280,  val_loss=1.255477786,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 21:25:05,845 > Epoch: 186,  train_loss=0.000159765,  val_loss=1.266133785,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:26:13,662 > Epoch: 187,  train_loss=0.000157743,  val_loss=1.256182075,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:27:21,498 > Epoch: 188,  train_loss=0.000155257,  val_loss=1.268898726,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 21:28:29,323 > Epoch: 189,  train_loss=0.000150554,  val_loss=1.260226727,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:29:37,141 > Epoch: 190,  train_loss=0.000151699,  val_loss=1.258743286,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 21:30:44,951 > Epoch: 191,  train_loss=0.000144685,  val_loss=1.258062124,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:31:52,763 > Epoch: 192,  train_loss=0.000133323,  val_loss=1.273962021,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:33:00,625 > Epoch: 193,  train_loss=0.000167959,  val_loss=1.243455172,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:34:08,497 > Epoch: 194,  train_loss=0.000133598,  val_loss=1.238823771,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:35:16,375 > Epoch: 195,  train_loss=0.000146660,  val_loss=1.194481730,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 21:36:24,177 > Epoch: 196,  train_loss=0.000179984,  val_loss=1.175667882,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 21:37:32,035 > Epoch: 197,  train_loss=0.000141520,  val_loss=1.197278261,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:38:39,862 > Epoch: 198,  train_loss=0.000136211,  val_loss=1.202691913,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:39:47,704 > Epoch: 199,  train_loss=0.000130397,  val_loss=1.216177344,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 21:40:55,515 > Epoch: 200,  train_loss=0.000113830,  val_loss=1.307957411,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:42:03,388 > Epoch: 201,  train_loss=0.045647811,  val_loss=0.788473487,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-19 21:43:11,239 > Epoch: 202,  train_loss=0.001375143,  val_loss=0.814498127,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 21:44:19,173 > Epoch: 203,  train_loss=0.000235103,  val_loss=0.785758257,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:45:26,992 > Epoch: 204,  train_loss=0.000183544,  val_loss=0.783765674,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:46:34,820 > Epoch: 205,  train_loss=0.000166510,  val_loss=0.781083941,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:47:42,625 > Epoch: 206,  train_loss=0.000158183,  val_loss=0.779162586,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 21:48:50,443 > Epoch: 207,  train_loss=0.000153058,  val_loss=0.774884701,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:49:58,300 > Epoch: 208,  train_loss=0.000149576,  val_loss=0.774330437,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:51:06,118 > Epoch: 209,  train_loss=0.000146922,  val_loss=0.772549093,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:52:13,950 > Epoch: 210,  train_loss=0.000144797,  val_loss=0.783262193,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:53:21,839 > Epoch: 211,  train_loss=0.000142950,  val_loss=0.785196602,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:54:29,669 > Epoch: 212,  train_loss=0.000141326,  val_loss=0.795593023,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:55:37,515 > Epoch: 213,  train_loss=0.000139985,  val_loss=0.804747403,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:56:45,315 > Epoch: 214,  train_loss=0.000138754,  val_loss=0.808035612,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:57:53,165 > Epoch: 215,  train_loss=0.000137655,  val_loss=0.815386057,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 21:59:01,008 > Epoch: 216,  train_loss=0.000136541,  val_loss=0.817719340,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 22:00:08,880 > Epoch: 217,  train_loss=0.000135597,  val_loss=0.819775105,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-19 22:01:16,685 > Epoch: 218,  train_loss=0.000134500,  val_loss=0.832002759,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:02:24,527 > Epoch: 219,  train_loss=0.000181480,  val_loss=0.823828697,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:03:32,328 > Epoch: 220,  train_loss=0.000139972,  val_loss=0.821086884,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:04:40,192 > Epoch: 221,  train_loss=0.000135029,  val_loss=0.832101345,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:05:47,986 > Epoch: 222,  train_loss=0.000132540,  val_loss=0.846495271,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:06:55,841 > Epoch: 223,  train_loss=0.000130734,  val_loss=0.853754282,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:08:03,645 > Epoch: 224,  train_loss=0.000128904,  val_loss=0.855403066,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:09:11,464 > Epoch: 225,  train_loss=0.000127602,  val_loss=0.868106544,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:10:19,306 > Epoch: 226,  train_loss=0.000127421,  val_loss=0.798613667,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:11:27,214 > Epoch: 227,  train_loss=0.000127521,  val_loss=0.829857826,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:12:35,004 > Epoch: 228,  train_loss=0.000126782,  val_loss=0.852217793,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:13:42,825 > Epoch: 229,  train_loss=0.000125620,  val_loss=0.859434903,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:14:50,739 > Epoch: 230,  train_loss=0.000125422,  val_loss=0.878057003,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:15:58,597 > Epoch: 231,  train_loss=0.000124978,  val_loss=0.886490583,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:17:06,508 > Epoch: 232,  train_loss=0.000124686,  val_loss=0.890685081,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:18:14,346 > Epoch: 233,  train_loss=0.000124346,  val_loss=0.895503640,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:19:22,160 > Epoch: 234,  train_loss=0.000124142,  val_loss=0.902739406,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:20:29,956 > Epoch: 235,  train_loss=0.000123727,  val_loss=0.900388956,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:21:37,767 > Epoch: 236,  train_loss=0.000123420,  val_loss=0.906109512,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:22:45,574 > Epoch: 237,  train_loss=0.000122940,  val_loss=0.918798387,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:23:53,433 > Epoch: 238,  train_loss=0.000122627,  val_loss=0.936300755,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-19 22:25:01,279 > Epoch: 239,  train_loss=0.000122319,  val_loss=0.971171677,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 22:26:09,094 > Epoch: 240,  train_loss=0.000122185,  val_loss=1.019063950,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 22:27:16,924 > Epoch: 241,  train_loss=0.000121390,  val_loss=1.018964410,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 22:28:24,740 > Epoch: 242,  train_loss=0.000099745,  val_loss=1.059584260,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 22:29:32,698 > Epoch: 243,  train_loss=0.000227946,  val_loss=0.981834352,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 22:30:40,544 > Epoch: 244,  train_loss=0.000160093,  val_loss=0.958800435,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 22:31:48,418 > Epoch: 245,  train_loss=0.000127217,  val_loss=0.970071018,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 22:32:56,254 > Epoch: 246,  train_loss=0.000130876,  val_loss=1.010979176,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 22:34:04,115 > Epoch: 247,  train_loss=0.000141177,  val_loss=0.990668058,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 22:35:11,948 > Epoch: 248,  train_loss=0.000130214,  val_loss=0.979394138,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 22:36:19,815 > Epoch: 249,  train_loss=0.000126349,  val_loss=0.970835090,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-19 22:37:27,697 > Epoch: 250,  train_loss=0.000128101,  val_loss=0.959137082,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:38:35,531 > Epoch: 251,  train_loss=0.000123675,  val_loss=0.965151370,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-19 22:39:43,426 > Epoch: 252,  train_loss=0.000122317,  val_loss=0.996157765,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-19 22:40:51,299 > Epoch: 253,  train_loss=0.000105537,  val_loss=1.997778416,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-19 22:41:59,172 > Epoch: 254,  train_loss=0.041877435,  val_loss=12.298275948,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-19 22:43:07,054 > Epoch: 255,  train_loss=0.001797527,  val_loss=4.949618340,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-19 22:44:14,957 > Epoch: 256,  train_loss=0.000136737,  val_loss=5.114867210,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-19 22:45:22,773 > Epoch: 257,  train_loss=0.000121037,  val_loss=5.094699860,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-19 22:46:30,631 > Epoch: 258,  train_loss=0.000117827,  val_loss=5.073999882,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-19 22:47:38,534 > Epoch: 259,  train_loss=0.000117358,  val_loss=5.049552441,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-19 22:48:46,457 > Epoch: 260,  train_loss=0.000117812,  val_loss=5.052489281,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-19 22:49:54,335 > Epoch: 261,  train_loss=0.000115590,  val_loss=5.009598732,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-19 22:51:02,206 > Epoch: 262,  train_loss=0.000146363,  val_loss=4.998879433,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-19 22:52:10,091 > Epoch: 263,  train_loss=0.000116034,  val_loss=5.001337051,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-19 22:53:17,895 > Epoch: 264,  train_loss=0.000107648,  val_loss=4.881037712,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 22:54:25,769 > Epoch: 265,  train_loss=0.000127558,  val_loss=4.777403355,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 22:55:33,656 > Epoch: 266,  train_loss=0.000123904,  val_loss=4.774795055,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 22:56:41,528 > Epoch: 267,  train_loss=0.000124531,  val_loss=4.735419750,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 22:57:49,369 > Epoch: 268,  train_loss=0.000125316,  val_loss=4.694932938,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 22:58:57,160 > Epoch: 269,  train_loss=0.000125849,  val_loss=4.660133839,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:00:04,999 > Epoch: 270,  train_loss=0.000126514,  val_loss=4.630723953,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:01:12,893 > Epoch: 271,  train_loss=0.000126970,  val_loss=4.590028286,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:02:20,730 > Epoch: 272,  train_loss=0.000127415,  val_loss=4.550589561,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:03:28,606 > Epoch: 273,  train_loss=0.000127528,  val_loss=4.504486084,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:04:36,432 > Epoch: 274,  train_loss=0.000127664,  val_loss=4.481209755,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:05:44,276 > Epoch: 275,  train_loss=0.000127625,  val_loss=4.436561108,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-19 23:06:52,129 > Epoch: 276,  train_loss=0.000127487,  val_loss=4.396484852,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-19 23:07:59,971 > Epoch: 277,  train_loss=0.000126840,  val_loss=4.365023613,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-19 23:09:07,828 > Epoch: 278,  train_loss=0.000125852,  val_loss=4.292307854,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-19 23:10:15,681 > Epoch: 279,  train_loss=0.000125686,  val_loss=4.245902538,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-19 23:11:23,538 > Epoch: 280,  train_loss=0.000125042,  val_loss=4.226983070,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-19 23:12:31,374 > Epoch: 281,  train_loss=0.000122916,  val_loss=4.183236599,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-19 23:13:39,251 > Epoch: 282,  train_loss=0.000147631,  val_loss=4.196238518,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-19 23:14:47,079 > Epoch: 283,  train_loss=0.000102331,  val_loss=4.241586685,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-19 23:15:54,957 > Epoch: 284,  train_loss=0.000262274,  val_loss=3.904910564,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-19 23:17:02,769 > Epoch: 285,  train_loss=0.000133496,  val_loss=3.858016014,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-19 23:18:10,624 > Epoch: 286,  train_loss=0.000140478,  val_loss=3.720723629,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-19 23:19:18,466 > Epoch: 287,  train_loss=0.000133464,  val_loss=3.550334454,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-19 23:20:26,334 > Epoch: 288,  train_loss=0.000128997,  val_loss=3.564589977,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-19 23:21:34,165 > Epoch: 289,  train_loss=0.000127282,  val_loss=3.527638912,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-19 23:22:42,009 > Epoch: 290,  train_loss=0.000126089,  val_loss=3.452373505,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-19 23:23:49,811 > Epoch: 291,  train_loss=0.000124864,  val_loss=3.459492922,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-19 23:24:57,686 > Epoch: 292,  train_loss=0.000124342,  val_loss=3.449593544,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-19 23:26:05,531 > Epoch: 293,  train_loss=0.000123670,  val_loss=3.378873348,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-19 23:27:13,368 > Epoch: 294,  train_loss=0.000123128,  val_loss=3.382336378,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-19 23:28:21,205 > Epoch: 295,  train_loss=0.000122509,  val_loss=3.409059048,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-19 23:29:28,996 > Epoch: 296,  train_loss=0.000122492,  val_loss=3.267703533,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-19 23:30:36,865 > Epoch: 297,  train_loss=0.000110540,  val_loss=2.777083874,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-19 23:31:44,734 > Epoch: 298,  train_loss=0.000188548,  val_loss=4.682938576,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-19 23:32:52,588 > Epoch: 299,  train_loss=0.000234745,  val_loss=6.749253273,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-19 23:34:00,454 > Epoch: 300,  train_loss=0.000244921,  val_loss=9.434998512,  accuracy=0.500000000
[INFO|asb_main.py:613] 2018-10-19 23:35:08,354 > Epoch: 301,  train_loss=0.000177631,  val_loss=11.375539780,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-19 23:36:16,223 > Epoch: 302,  train_loss=0.000154001,  val_loss=12.257395744,  accuracy=0.359375000
[INFO|asb_main.py:613] 2018-10-19 23:37:24,112 > Epoch: 303,  train_loss=0.000114613,  val_loss=13.394268036,  accuracy=0.359375000
[INFO|asb_main.py:613] 2018-10-19 23:38:31,976 > Epoch: 304,  train_loss=0.000211247,  val_loss=15.260499001,  accuracy=0.351562500
[INFO|asb_main.py:613] 2018-10-19 23:39:39,822 > Epoch: 305,  train_loss=0.000385455,  val_loss=6.418966770,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-19 23:40:47,718 > Epoch: 306,  train_loss=0.007156525,  val_loss=27.071521759,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-19 23:41:55,589 > Epoch: 307,  train_loss=0.000379041,  val_loss=29.605194092,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-19 23:43:03,529 > Epoch: 308,  train_loss=0.000369967,  val_loss=27.249149323,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-19 23:44:11,373 > Epoch: 309,  train_loss=0.000229897,  val_loss=26.600177765,  accuracy=0.656250000
[INFO|asb_main.py:613] 2018-10-19 23:45:19,266 > Epoch: 310,  train_loss=0.000193446,  val_loss=25.789699554,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-19 23:46:27,078 > Epoch: 311,  train_loss=0.000185875,  val_loss=25.289318085,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-19 23:47:34,932 > Epoch: 312,  train_loss=0.000178964,  val_loss=24.811716080,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-19 23:48:42,754 > Epoch: 313,  train_loss=0.000177770,  val_loss=24.251743317,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-19 23:49:50,592 > Epoch: 314,  train_loss=0.000176379,  val_loss=23.562654495,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-19 23:50:58,499 > Epoch: 315,  train_loss=0.000175602,  val_loss=22.715509415,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-19 23:52:06,329 > Epoch: 316,  train_loss=0.000174692,  val_loss=22.305519104,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:53:14,168 > Epoch: 317,  train_loss=0.000163939,  val_loss=21.560026169,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:54:21,994 > Epoch: 318,  train_loss=0.000147488,  val_loss=21.101650238,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:55:29,855 > Epoch: 319,  train_loss=0.000147476,  val_loss=20.742801666,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:56:37,704 > Epoch: 320,  train_loss=0.000146461,  val_loss=20.490776062,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:57:45,502 > Epoch: 321,  train_loss=0.000146142,  val_loss=20.187709808,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-19 23:58:53,382 > Epoch: 322,  train_loss=0.000145258,  val_loss=20.445945740,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 00:00:01,286 > Epoch: 323,  train_loss=0.000144587,  val_loss=20.333427429,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 00:01:09,148 > Epoch: 324,  train_loss=0.000144547,  val_loss=19.745595932,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 00:02:16,993 > Epoch: 325,  train_loss=0.000143021,  val_loss=19.483987808,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 00:03:24,815 > Epoch: 326,  train_loss=0.000142279,  val_loss=19.055736542,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 00:04:32,682 > Epoch: 327,  train_loss=0.000127625,  val_loss=18.594467163,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 00:05:40,520 > Epoch: 328,  train_loss=0.000118656,  val_loss=18.448083878,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 00:06:48,374 > Epoch: 329,  train_loss=0.000115924,  val_loss=17.989524841,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 00:07:56,225 > Epoch: 330,  train_loss=0.000115073,  val_loss=17.724548340,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 00:09:04,055 > Epoch: 331,  train_loss=0.000115803,  val_loss=17.315355301,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 00:10:11,881 > Epoch: 332,  train_loss=0.000115464,  val_loss=17.075609207,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 00:11:19,694 > Epoch: 333,  train_loss=0.000115397,  val_loss=16.593746185,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 00:12:27,541 > Epoch: 334,  train_loss=0.000115627,  val_loss=16.028434753,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 00:13:35,346 > Epoch: 335,  train_loss=0.000132648,  val_loss=15.031771660,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 00:14:43,192 > Epoch: 336,  train_loss=0.000125651,  val_loss=14.524714470,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 00:15:51,074 > Epoch: 337,  train_loss=0.000118842,  val_loss=13.998089790,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 00:16:58,929 > Epoch: 338,  train_loss=0.000112969,  val_loss=13.462800026,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 00:18:06,854 > Epoch: 339,  train_loss=0.000114316,  val_loss=13.259613037,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-20 00:19:14,813 > Epoch: 340,  train_loss=0.000113747,  val_loss=12.900631905,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-20 00:20:22,782 > Epoch: 341,  train_loss=0.000108002,  val_loss=13.215221405,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 00:21:30,753 > Epoch: 342,  train_loss=0.000185390,  val_loss=15.345713615,  accuracy=0.484375000
[INFO|asb_main.py:613] 2018-10-20 00:22:38,792 > Epoch: 343,  train_loss=0.000258942,  val_loss=12.711118698,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 00:23:46,785 > Epoch: 344,  train_loss=0.008233288,  val_loss=24.758932114,  accuracy=0.460937500
[INFO|asb_main.py:613] 2018-10-20 00:24:54,763 > Epoch: 345,  train_loss=0.014180534,  val_loss=35.965179443,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 00:26:02,807 > Epoch: 346,  train_loss=0.017436607,  val_loss=11.891069412,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-20 00:27:10,749 > Epoch: 347,  train_loss=0.000180934,  val_loss=11.209728241,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-20 00:28:18,730 > Epoch: 348,  train_loss=0.000125502,  val_loss=11.249381065,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-20 00:29:26,764 > Epoch: 349,  train_loss=0.000114161,  val_loss=11.278004646,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-20 00:30:34,704 > Epoch: 350,  train_loss=0.000111171,  val_loss=11.315469742,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 00:31:42,688 > Epoch: 351,  train_loss=0.000109952,  val_loss=11.354051590,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 00:32:50,699 > Epoch: 352,  train_loss=0.000109413,  val_loss=11.394849777,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 00:33:58,750 > Epoch: 353,  train_loss=0.000109154,  val_loss=11.430047989,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 00:35:06,776 > Epoch: 354,  train_loss=0.000109036,  val_loss=11.465666771,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 00:36:14,745 > Epoch: 355,  train_loss=0.000108989,  val_loss=11.508728027,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 00:37:22,710 > Epoch: 356,  train_loss=0.000108987,  val_loss=11.552943230,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 00:38:30,667 > Epoch: 357,  train_loss=0.000109006,  val_loss=11.602922440,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 00:39:38,691 > Epoch: 358,  train_loss=0.000109049,  val_loss=11.656761169,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 00:40:46,684 > Epoch: 359,  train_loss=0.000109109,  val_loss=11.710456848,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-20 00:41:54,649 > Epoch: 360,  train_loss=0.000109164,  val_loss=11.755747795,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-20 00:43:02,672 > Epoch: 361,  train_loss=0.000109261,  val_loss=11.815169334,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-20 00:44:10,619 > Epoch: 362,  train_loss=0.000109328,  val_loss=11.871631622,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-20 00:45:18,573 > Epoch: 363,  train_loss=0.000109433,  val_loss=11.940940857,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-20 00:46:26,571 > Epoch: 364,  train_loss=0.000109565,  val_loss=12.036387444,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 00:47:34,589 > Epoch: 365,  train_loss=0.000109733,  val_loss=12.113752365,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-20 00:48:42,553 > Epoch: 366,  train_loss=0.000109862,  val_loss=12.207191467,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-20 00:49:50,505 > Epoch: 367,  train_loss=0.000110061,  val_loss=12.305412292,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-20 00:50:58,511 > Epoch: 368,  train_loss=0.000110171,  val_loss=12.439491272,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-20 00:52:06,492 > Epoch: 369,  train_loss=0.000110439,  val_loss=12.552765846,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-20 00:53:14,449 > Epoch: 370,  train_loss=0.000110627,  val_loss=12.733066559,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-20 00:54:22,531 > Epoch: 371,  train_loss=0.000110909,  val_loss=12.872867584,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 00:55:30,527 > Epoch: 372,  train_loss=0.000111032,  val_loss=13.081363678,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 00:56:38,541 > Epoch: 373,  train_loss=0.000111277,  val_loss=13.228860855,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-20 00:57:46,477 > Epoch: 374,  train_loss=0.000111462,  val_loss=13.475929260,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 00:58:54,461 > Epoch: 375,  train_loss=0.000111588,  val_loss=13.642168045,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 01:00:02,455 > Epoch: 376,  train_loss=0.000111752,  val_loss=13.837925911,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 01:01:10,443 > Epoch: 377,  train_loss=0.000111891,  val_loss=14.174033165,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 01:02:18,395 > Epoch: 378,  train_loss=0.000111986,  val_loss=14.429917336,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 01:03:26,456 > Epoch: 379,  train_loss=0.000112140,  val_loss=14.658021927,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 01:04:34,488 > Epoch: 380,  train_loss=0.000112177,  val_loss=14.973161697,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 01:05:42,477 > Epoch: 381,  train_loss=0.000112173,  val_loss=15.307289124,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 01:06:50,438 > Epoch: 382,  train_loss=0.000112332,  val_loss=15.696312904,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 01:07:58,380 > Epoch: 383,  train_loss=0.000112290,  val_loss=16.104724884,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-20 01:09:06,324 > Epoch: 384,  train_loss=0.000112255,  val_loss=16.513231277,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 01:10:14,308 > Epoch: 385,  train_loss=0.000112178,  val_loss=16.863946915,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 01:11:22,295 > Epoch: 386,  train_loss=0.000113353,  val_loss=17.454229355,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-20 01:12:30,256 > Epoch: 387,  train_loss=0.000101735,  val_loss=19.561773300,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 01:13:38,212 > Epoch: 388,  train_loss=0.000178763,  val_loss=18.652940750,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 01:14:46,180 > Epoch: 389,  train_loss=0.000163164,  val_loss=49.248767853,  accuracy=0.289062500
[INFO|asb_main.py:613] 2018-10-20 01:15:54,154 > Epoch: 390,  train_loss=0.000125074,  val_loss=48.894615173,  accuracy=0.289062500
[INFO|asb_main.py:613] 2018-10-20 01:17:02,195 > Epoch: 391,  train_loss=0.000120442,  val_loss=48.661682129,  accuracy=0.296875000
[INFO|asb_main.py:613] 2018-10-20 01:18:10,209 > Epoch: 392,  train_loss=0.000117993,  val_loss=49.063087463,  accuracy=0.296875000
[INFO|asb_main.py:613] 2018-10-20 01:19:18,220 > Epoch: 393,  train_loss=0.000116704,  val_loss=48.180278778,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 01:20:26,266 > Epoch: 394,  train_loss=0.000115586,  val_loss=48.139163971,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 01:21:34,270 > Epoch: 395,  train_loss=0.000115103,  val_loss=47.202411652,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:22:42,227 > Epoch: 396,  train_loss=0.000114278,  val_loss=46.552505493,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:23:50,247 > Epoch: 397,  train_loss=0.000113978,  val_loss=45.523342133,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 01:24:58,187 > Epoch: 398,  train_loss=0.000113383,  val_loss=45.714920044,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 01:26:06,208 > Epoch: 399,  train_loss=0.000113472,  val_loss=44.963485718,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 01:27:14,188 > Epoch: 400,  train_loss=0.000116340,  val_loss=43.733428955,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 01:28:22,166 > Epoch: 401,  train_loss=0.000112430,  val_loss=44.300613403,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 01:29:30,135 > Epoch: 402,  train_loss=0.000112188,  val_loss=42.544113159,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 01:30:38,160 > Epoch: 403,  train_loss=0.000113463,  val_loss=42.894870758,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 01:31:46,155 > Epoch: 404,  train_loss=0.000111107,  val_loss=40.121429443,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:32:54,182 > Epoch: 405,  train_loss=0.000113742,  val_loss=42.276237488,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:34:02,224 > Epoch: 406,  train_loss=0.000110772,  val_loss=39.412757874,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:35:10,167 > Epoch: 407,  train_loss=0.000113269,  val_loss=39.269756317,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:36:18,150 > Epoch: 408,  train_loss=0.000111834,  val_loss=41.750560760,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:37:26,085 > Epoch: 409,  train_loss=0.000111802,  val_loss=42.636230469,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 01:38:34,068 > Epoch: 410,  train_loss=0.000109849,  val_loss=39.576713562,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:39:42,033 > Epoch: 411,  train_loss=0.000112344,  val_loss=41.391044617,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 01:40:49,994 > Epoch: 412,  train_loss=0.000094096,  val_loss=75.034729004,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 01:41:58,015 > Epoch: 413,  train_loss=0.010758153,  val_loss=27.934612274,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 01:43:06,034 > Epoch: 414,  train_loss=0.000331191,  val_loss=26.411094666,  accuracy=0.429687500
[INFO|asb_main.py:613] 2018-10-20 01:44:14,010 > Epoch: 415,  train_loss=0.000231405,  val_loss=27.047906876,  accuracy=0.421875000
[INFO|asb_main.py:613] 2018-10-20 01:45:22,051 > Epoch: 416,  train_loss=0.000232812,  val_loss=29.010250092,  accuracy=0.421875000
[INFO|asb_main.py:613] 2018-10-20 01:46:30,014 > Epoch: 417,  train_loss=0.000209852,  val_loss=28.951986313,  accuracy=0.421875000
[INFO|asb_main.py:613] 2018-10-20 01:47:37,967 > Epoch: 418,  train_loss=0.000154069,  val_loss=29.133560181,  accuracy=0.421875000
[INFO|asb_main.py:613] 2018-10-20 01:48:45,956 > Epoch: 419,  train_loss=0.000137862,  val_loss=29.017896652,  accuracy=0.414062500
[INFO|asb_main.py:613] 2018-10-20 01:49:53,957 > Epoch: 420,  train_loss=0.000128166,  val_loss=29.251327515,  accuracy=0.414062500
[INFO|asb_main.py:613] 2018-10-20 01:51:01,915 > Epoch: 421,  train_loss=0.000126013,  val_loss=29.256923676,  accuracy=0.421875000
[INFO|asb_main.py:613] 2018-10-20 01:52:09,879 > Epoch: 422,  train_loss=0.000107142,  val_loss=29.009723663,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:53:17,866 > Epoch: 423,  train_loss=0.000210398,  val_loss=31.215259552,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 01:54:25,816 > Epoch: 424,  train_loss=0.000164832,  val_loss=32.041790009,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 01:55:33,768 > Epoch: 425,  train_loss=0.000148257,  val_loss=32.309783936,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 01:56:41,797 > Epoch: 426,  train_loss=0.000140102,  val_loss=32.294975281,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 01:57:49,799 > Epoch: 427,  train_loss=0.000134139,  val_loss=32.993240356,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 01:58:57,794 > Epoch: 428,  train_loss=0.000132697,  val_loss=33.483871460,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 02:00:05,768 > Epoch: 429,  train_loss=0.000129835,  val_loss=33.640007019,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 02:01:13,691 > Epoch: 430,  train_loss=0.000126074,  val_loss=34.600135803,  accuracy=0.296875000
[INFO|asb_main.py:613] 2018-10-20 02:02:21,717 > Epoch: 431,  train_loss=0.000180465,  val_loss=34.968078613,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 02:03:29,659 > Epoch: 432,  train_loss=0.000142133,  val_loss=36.215526581,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 02:04:37,629 > Epoch: 433,  train_loss=0.000184997,  val_loss=39.822223663,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 02:05:45,636 > Epoch: 434,  train_loss=0.000141294,  val_loss=41.571250916,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 02:06:53,669 > Epoch: 435,  train_loss=0.000145806,  val_loss=44.352230072,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 02:08:01,696 > Epoch: 436,  train_loss=0.000147853,  val_loss=48.243015289,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:09:09,718 > Epoch: 437,  train_loss=0.000133633,  val_loss=50.529449463,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:10:17,699 > Epoch: 438,  train_loss=0.000127688,  val_loss=53.361530304,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:11:25,668 > Epoch: 439,  train_loss=0.000188899,  val_loss=57.982429504,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:12:33,645 > Epoch: 440,  train_loss=0.000172717,  val_loss=57.246681213,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:13:41,642 > Epoch: 441,  train_loss=0.000196266,  val_loss=59.253261566,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:14:49,619 > Epoch: 442,  train_loss=0.000163341,  val_loss=61.419589996,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:15:57,677 > Epoch: 443,  train_loss=0.000144787,  val_loss=64.895286560,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:17:05,701 > Epoch: 444,  train_loss=0.000188040,  val_loss=73.522674561,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:18:13,721 > Epoch: 445,  train_loss=0.000164974,  val_loss=75.833648682,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:19:21,729 > Epoch: 446,  train_loss=0.000198771,  val_loss=67.749153137,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:20:29,690 > Epoch: 447,  train_loss=0.000171896,  val_loss=74.048843384,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:21:37,650 > Epoch: 448,  train_loss=0.000153124,  val_loss=79.683433533,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:22:45,637 > Epoch: 449,  train_loss=0.000152292,  val_loss=85.062011719,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:23:53,663 > Epoch: 450,  train_loss=0.000225714,  val_loss=78.048225403,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:25:01,651 > Epoch: 451,  train_loss=0.000144201,  val_loss=81.204551697,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:26:09,644 > Epoch: 452,  train_loss=0.000141109,  val_loss=87.797569275,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:27:17,629 > Epoch: 453,  train_loss=0.000124904,  val_loss=92.591545105,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:28:25,675 > Epoch: 454,  train_loss=0.000124568,  val_loss=94.742507935,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:29:33,738 > Epoch: 455,  train_loss=0.000123920,  val_loss=95.326782227,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 02:30:41,772 > Epoch: 456,  train_loss=0.020454853,  val_loss=57.626071930,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 02:31:49,748 > Epoch: 457,  train_loss=0.005069930,  val_loss=31.952953339,  accuracy=0.656250000
[INFO|asb_main.py:613] 2018-10-20 02:32:57,708 > Epoch: 458,  train_loss=0.000715971,  val_loss=44.492111206,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 02:34:05,693 > Epoch: 459,  train_loss=0.000243538,  val_loss=45.209667206,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-20 02:35:13,700 > Epoch: 460,  train_loss=0.000198119,  val_loss=46.198638916,  accuracy=0.539062500
[INFO|asb_main.py:613] 2018-10-20 02:36:21,706 > Epoch: 461,  train_loss=0.000188018,  val_loss=46.909053802,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 02:37:29,662 > Epoch: 462,  train_loss=0.000178549,  val_loss=47.327716827,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 02:38:37,676 > Epoch: 463,  train_loss=0.000175715,  val_loss=47.809299469,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 02:39:45,668 > Epoch: 464,  train_loss=0.000174008,  val_loss=47.631004333,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 02:40:53,673 > Epoch: 465,  train_loss=0.000173837,  val_loss=47.973609924,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 02:42:01,642 > Epoch: 466,  train_loss=0.000173511,  val_loss=48.690135956,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 02:43:09,621 > Epoch: 467,  train_loss=0.000173676,  val_loss=49.776443481,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 02:44:17,617 > Epoch: 468,  train_loss=0.000169286,  val_loss=50.803031921,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 02:45:25,615 > Epoch: 469,  train_loss=0.000168919,  val_loss=51.562019348,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 02:46:33,615 > Epoch: 470,  train_loss=0.000166419,  val_loss=51.972877502,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 02:47:41,606 > Epoch: 471,  train_loss=0.000165441,  val_loss=53.042491913,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 02:48:49,614 > Epoch: 472,  train_loss=0.000165618,  val_loss=53.758739471,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 02:49:57,617 > Epoch: 473,  train_loss=0.000164702,  val_loss=55.010177612,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 02:51:05,592 > Epoch: 474,  train_loss=0.000158360,  val_loss=55.663116455,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 02:52:13,531 > Epoch: 475,  train_loss=0.000179895,  val_loss=58.264888763,  accuracy=0.460937500
[INFO|asb_main.py:613] 2018-10-20 02:53:21,500 > Epoch: 476,  train_loss=0.000169309,  val_loss=59.563224792,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 02:54:29,492 > Epoch: 477,  train_loss=0.000166434,  val_loss=60.405082703,  accuracy=0.445312500
[INFO|asb_main.py:613] 2018-10-20 02:55:37,496 > Epoch: 478,  train_loss=0.000161097,  val_loss=62.436965942,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 02:56:45,505 > Epoch: 479,  train_loss=0.000161917,  val_loss=63.177986145,  accuracy=0.445312500
[INFO|asb_main.py:613] 2018-10-20 02:57:53,479 > Epoch: 480,  train_loss=0.000156854,  val_loss=62.309646606,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 02:59:01,433 > Epoch: 481,  train_loss=0.000156500,  val_loss=63.734592438,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 03:00:09,455 > Epoch: 482,  train_loss=0.000152638,  val_loss=64.090560913,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 03:01:17,469 > Epoch: 483,  train_loss=0.000155327,  val_loss=65.075462341,  accuracy=0.437500000
[INFO|asb_main.py:613] 2018-10-20 03:02:25,450 > Epoch: 484,  train_loss=0.000139140,  val_loss=68.589050293,  accuracy=0.421875000
[INFO|asb_main.py:613] 2018-10-20 03:03:33,554 > Epoch: 485,  train_loss=0.000200785,  val_loss=68.651794434,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 03:04:41,513 > Epoch: 486,  train_loss=0.000187883,  val_loss=70.058105469,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 03:05:49,473 > Epoch: 487,  train_loss=0.000157090,  val_loss=70.649108887,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 03:06:57,448 > Epoch: 488,  train_loss=0.000146669,  val_loss=73.060379028,  accuracy=0.312500000
[INFO|asb_main.py:613] 2018-10-20 03:08:05,439 > Epoch: 489,  train_loss=0.000141360,  val_loss=77.597648621,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 03:09:13,390 > Epoch: 490,  train_loss=0.000140094,  val_loss=79.364204407,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 03:10:21,409 > Epoch: 491,  train_loss=0.000137586,  val_loss=84.133468628,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 03:11:29,384 > Epoch: 492,  train_loss=0.000139234,  val_loss=86.733810425,  accuracy=0.296875000
[INFO|asb_main.py:613] 2018-10-20 03:12:37,346 > Epoch: 493,  train_loss=0.000131456,  val_loss=87.865203857,  accuracy=0.289062500
[INFO|asb_main.py:613] 2018-10-20 03:13:45,352 > Epoch: 494,  train_loss=0.000154953,  val_loss=93.023475647,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 03:14:53,325 > Epoch: 495,  train_loss=0.000132675,  val_loss=96.657897949,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 03:16:01,296 > Epoch: 496,  train_loss=0.000117788,  val_loss=98.021728516,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 03:17:09,293 > Epoch: 497,  train_loss=0.000167284,  val_loss=99.689682007,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 03:18:17,318 > Epoch: 498,  train_loss=0.000129948,  val_loss=102.539276123,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 03:19:25,307 > Epoch: 499,  train_loss=0.000183008,  val_loss=97.603324890,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 03:20:33,308 > Epoch: 500,  train_loss=0.000145742,  val_loss=102.396652222,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 03:21:41,281 > Epoch: 501,  train_loss=0.000111917,  val_loss=105.048858643,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 03:22:49,297 > Epoch: 502,  train_loss=0.000129709,  val_loss=101.664222717,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 03:23:57,335 > Epoch: 503,  train_loss=0.000110939,  val_loss=102.026626587,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 03:25:05,341 > Epoch: 504,  train_loss=0.000122644,  val_loss=98.996330261,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 03:26:13,345 > Epoch: 505,  train_loss=0.000108724,  val_loss=100.371017456,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 03:27:21,339 > Epoch: 506,  train_loss=0.000109617,  val_loss=102.196151733,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 03:28:29,310 > Epoch: 507,  train_loss=0.000105844,  val_loss=98.387832642,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 03:29:37,295 > Epoch: 508,  train_loss=0.000130477,  val_loss=99.669311523,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 03:30:45,273 > Epoch: 509,  train_loss=0.000196722,  val_loss=57.308979034,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 03:31:53,248 > Epoch: 510,  train_loss=0.007640392,  val_loss=22.479360580,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 03:33:01,227 > Epoch: 511,  train_loss=0.000508922,  val_loss=26.695711136,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 03:34:09,182 > Epoch: 512,  train_loss=0.000197803,  val_loss=27.437774658,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 03:35:17,192 > Epoch: 513,  train_loss=0.000187622,  val_loss=28.661989212,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 03:36:25,130 > Epoch: 514,  train_loss=0.000180924,  val_loss=29.710958481,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 03:37:33,103 > Epoch: 515,  train_loss=0.000177958,  val_loss=30.861913681,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 03:38:41,064 > Epoch: 516,  train_loss=0.000174925,  val_loss=32.099231720,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 03:39:49,021 > Epoch: 517,  train_loss=0.000158078,  val_loss=33.933361053,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-20 03:40:57,021 > Epoch: 518,  train_loss=0.000230942,  val_loss=36.729721069,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-20 03:42:04,989 > Epoch: 519,  train_loss=0.000202811,  val_loss=38.188327789,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-20 03:43:12,962 > Epoch: 520,  train_loss=0.000191771,  val_loss=39.276428223,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 03:44:20,947 > Epoch: 521,  train_loss=0.000180989,  val_loss=41.264472961,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 03:45:28,944 > Epoch: 522,  train_loss=0.000169953,  val_loss=43.588783264,  accuracy=0.539062500
[INFO|asb_main.py:613] 2018-10-20 03:46:36,945 > Epoch: 523,  train_loss=0.000162619,  val_loss=45.989318848,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 03:47:44,946 > Epoch: 524,  train_loss=0.000164213,  val_loss=45.995094299,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 03:48:52,942 > Epoch: 525,  train_loss=0.000155710,  val_loss=46.622093201,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 03:50:00,900 > Epoch: 526,  train_loss=0.000146828,  val_loss=47.283729553,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 03:51:08,878 > Epoch: 527,  train_loss=0.000145901,  val_loss=46.273921967,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 03:52:16,839 > Epoch: 528,  train_loss=0.000140675,  val_loss=49.814567566,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 03:53:24,788 > Epoch: 529,  train_loss=0.000134572,  val_loss=51.060932159,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 03:54:32,812 > Epoch: 530,  train_loss=0.000132599,  val_loss=51.308052063,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 03:55:40,822 > Epoch: 531,  train_loss=0.000130198,  val_loss=52.968322754,  accuracy=0.484375000
[INFO|asb_main.py:613] 2018-10-20 03:56:48,822 > Epoch: 532,  train_loss=0.000127600,  val_loss=54.411296844,  accuracy=0.484375000
[INFO|asb_main.py:613] 2018-10-20 03:57:56,803 > Epoch: 533,  train_loss=0.000125246,  val_loss=55.975990295,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 03:59:04,795 > Epoch: 534,  train_loss=0.000123212,  val_loss=56.364746094,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 04:00:12,750 > Epoch: 535,  train_loss=0.000123461,  val_loss=57.017379761,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 04:01:20,700 > Epoch: 536,  train_loss=0.000134167,  val_loss=58.579231262,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 04:02:28,705 > Epoch: 537,  train_loss=0.000125738,  val_loss=58.893234253,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 04:03:36,695 > Epoch: 538,  train_loss=0.000114374,  val_loss=61.061477661,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 04:04:44,707 > Epoch: 539,  train_loss=0.000110556,  val_loss=60.830154419,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 04:05:52,683 > Epoch: 540,  train_loss=0.000189048,  val_loss=65.365928650,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 04:07:00,688 > Epoch: 541,  train_loss=0.000120789,  val_loss=65.439888000,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 04:08:08,684 > Epoch: 542,  train_loss=0.000111903,  val_loss=67.253517151,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 04:09:16,678 > Epoch: 543,  train_loss=0.000149282,  val_loss=70.184028625,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 04:10:24,685 > Epoch: 544,  train_loss=0.000112195,  val_loss=68.488571167,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 04:11:32,628 > Epoch: 545,  train_loss=0.000125098,  val_loss=68.916877747,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 04:12:40,681 > Epoch: 546,  train_loss=0.000105144,  val_loss=65.519783020,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 04:13:48,720 > Epoch: 547,  train_loss=0.000127215,  val_loss=65.287399292,  accuracy=0.437500000
[INFO|asb_main.py:613] 2018-10-20 04:14:56,719 > Epoch: 548,  train_loss=0.000099622,  val_loss=66.016487122,  accuracy=0.445312500
[INFO|asb_main.py:613] 2018-10-20 04:16:04,711 > Epoch: 549,  train_loss=0.000100699,  val_loss=66.480590820,  accuracy=0.421875000
[INFO|asb_main.py:613] 2018-10-20 04:17:12,684 > Epoch: 550,  train_loss=0.000097291,  val_loss=70.880630493,  accuracy=0.421875000
[INFO|asb_main.py:613] 2018-10-20 04:18:20,739 > Epoch: 551,  train_loss=0.000109864,  val_loss=71.130020142,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 04:19:28,747 > Epoch: 552,  train_loss=0.000096234,  val_loss=68.902099609,  accuracy=0.320312500
[INFO|asb_main.py:613] 2018-10-20 04:20:36,714 > Epoch: 553,  train_loss=0.000094214,  val_loss=75.790847778,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 04:21:44,692 > Epoch: 554,  train_loss=0.000096036,  val_loss=72.723327637,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 04:22:52,655 > Epoch: 555,  train_loss=0.000100271,  val_loss=75.054916382,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 04:24:00,627 > Epoch: 556,  train_loss=0.000105635,  val_loss=71.468147278,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 04:25:08,592 > Epoch: 557,  train_loss=0.000097800,  val_loss=78.773010254,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 04:26:16,521 > Epoch: 558,  train_loss=0.000116481,  val_loss=80.404151917,  accuracy=0.234375000
[INFO|asb_main.py:613] 2018-10-20 04:27:24,572 > Epoch: 559,  train_loss=0.000085327,  val_loss=76.208412170,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 04:28:32,558 > Epoch: 560,  train_loss=0.000082554,  val_loss=78.430656433,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 04:29:40,552 > Epoch: 561,  train_loss=0.000077215,  val_loss=79.691619873,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 04:30:48,546 > Epoch: 562,  train_loss=0.000087056,  val_loss=84.491531372,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 04:31:56,514 > Epoch: 563,  train_loss=0.000084888,  val_loss=141.956390381,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 04:33:04,459 > Epoch: 564,  train_loss=0.000527725,  val_loss=217.106903076,  accuracy=0.335937500
[INFO|asb_main.py:613] 2018-10-20 04:34:12,438 > Epoch: 565,  train_loss=0.036261449,  val_loss=322.367767334,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 04:35:20,404 > Epoch: 566,  train_loss=0.025676515,  val_loss=357.639343262,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 04:36:28,358 > Epoch: 567,  train_loss=0.000853192,  val_loss=129.016082764,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-20 04:37:36,356 > Epoch: 568,  train_loss=0.000215021,  val_loss=132.874053955,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-20 04:38:44,376 > Epoch: 569,  train_loss=0.000197187,  val_loss=129.854202271,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-20 04:39:52,301 > Epoch: 570,  train_loss=0.000192727,  val_loss=126.873176575,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-20 04:41:00,262 > Epoch: 571,  train_loss=0.000186151,  val_loss=123.952667236,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 04:42:08,201 > Epoch: 572,  train_loss=0.000182647,  val_loss=120.838401794,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-20 04:43:16,165 > Epoch: 573,  train_loss=0.000179098,  val_loss=117.609527588,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-20 04:44:24,127 > Epoch: 574,  train_loss=0.000176151,  val_loss=114.928817749,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-20 04:45:32,121 > Epoch: 575,  train_loss=0.000173709,  val_loss=111.252189636,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 04:46:40,091 > Epoch: 576,  train_loss=0.000171447,  val_loss=108.396011353,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 04:47:48,075 > Epoch: 577,  train_loss=0.000168666,  val_loss=105.695686340,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-20 04:48:56,034 > Epoch: 578,  train_loss=0.000166725,  val_loss=102.379951477,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-20 04:50:03,983 > Epoch: 579,  train_loss=0.000165741,  val_loss=99.548446655,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 04:51:11,932 > Epoch: 580,  train_loss=0.000163665,  val_loss=96.305084229,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-20 04:52:19,896 > Epoch: 581,  train_loss=0.000162509,  val_loss=93.727699280,  accuracy=0.539062500
[INFO|asb_main.py:613] 2018-10-20 04:53:27,902 > Epoch: 582,  train_loss=0.000160599,  val_loss=89.328445435,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 04:54:35,857 > Epoch: 583,  train_loss=0.000160050,  val_loss=86.310745239,  accuracy=0.515625000
[INFO|asb_main.py:613] 2018-10-20 04:55:43,844 > Epoch: 584,  train_loss=0.000157370,  val_loss=83.240379333,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-20 04:56:51,776 > Epoch: 585,  train_loss=0.000156760,  val_loss=80.975959778,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 04:57:59,736 > Epoch: 586,  train_loss=0.000155884,  val_loss=78.054718018,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 04:59:07,695 > Epoch: 587,  train_loss=0.000153606,  val_loss=75.986206055,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-20 05:00:15,641 > Epoch: 588,  train_loss=0.000152151,  val_loss=74.619041443,  accuracy=0.484375000
[INFO|asb_main.py:613] 2018-10-20 05:01:23,581 > Epoch: 589,  train_loss=0.000150504,  val_loss=73.325332642,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:02:31,566 > Epoch: 590,  train_loss=0.000147069,  val_loss=76.289649963,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 05:03:39,510 > Epoch: 591,  train_loss=0.000146820,  val_loss=75.098869324,  accuracy=0.484375000
[INFO|asb_main.py:613] 2018-10-20 05:04:47,459 > Epoch: 592,  train_loss=0.000144615,  val_loss=74.758926392,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 05:05:55,419 > Epoch: 593,  train_loss=0.000140909,  val_loss=74.239593506,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 05:07:03,431 > Epoch: 594,  train_loss=0.000139938,  val_loss=74.461677551,  accuracy=0.460937500
[INFO|asb_main.py:613] 2018-10-20 05:08:11,419 > Epoch: 595,  train_loss=0.000137874,  val_loss=78.530014038,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 05:09:19,398 > Epoch: 596,  train_loss=0.000134467,  val_loss=90.849761963,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-20 05:10:27,366 > Epoch: 597,  train_loss=0.000132867,  val_loss=98.630714417,  accuracy=0.539062500
[INFO|asb_main.py:613] 2018-10-20 05:11:35,325 > Epoch: 598,  train_loss=0.000109926,  val_loss=93.474395752,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-20 05:12:43,308 > Epoch: 599,  train_loss=0.000167986,  val_loss=105.906219482,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 05:13:51,285 > Epoch: 600,  train_loss=0.000133570,  val_loss=112.864135742,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 05:14:59,217 > Epoch: 601,  train_loss=0.000126293,  val_loss=113.985504150,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 05:16:07,172 > Epoch: 602,  train_loss=0.000123502,  val_loss=120.098320007,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 05:17:15,218 > Epoch: 603,  train_loss=0.000123578,  val_loss=123.019927979,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 05:18:23,165 > Epoch: 604,  train_loss=0.000121865,  val_loss=122.650161743,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 05:19:31,139 > Epoch: 605,  train_loss=0.000120399,  val_loss=124.284912109,  accuracy=0.484375000
[INFO|asb_main.py:613] 2018-10-20 05:20:39,108 > Epoch: 606,  train_loss=0.000116975,  val_loss=128.568069458,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:21:47,085 > Epoch: 607,  train_loss=0.000120645,  val_loss=132.282302856,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:22:55,028 > Epoch: 608,  train_loss=0.000118475,  val_loss=134.272171021,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:24:03,027 > Epoch: 609,  train_loss=0.000117578,  val_loss=140.502166748,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:25:11,030 > Epoch: 610,  train_loss=0.000117036,  val_loss=147.400314331,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:26:18,997 > Epoch: 611,  train_loss=0.000117145,  val_loss=158.350585938,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:27:26,944 > Epoch: 612,  train_loss=0.000113329,  val_loss=162.083099365,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:28:34,936 > Epoch: 613,  train_loss=0.000114043,  val_loss=162.584777832,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:29:42,929 > Epoch: 614,  train_loss=0.000113350,  val_loss=158.210815430,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:30:50,899 > Epoch: 615,  train_loss=0.000109498,  val_loss=169.748687744,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-20 05:31:58,920 > Epoch: 616,  train_loss=0.000147172,  val_loss=174.143188477,  accuracy=0.460937500
[INFO|asb_main.py:613] 2018-10-20 05:33:06,877 > Epoch: 617,  train_loss=0.000102742,  val_loss=147.363189697,  accuracy=0.460937500
[INFO|asb_main.py:613] 2018-10-20 05:34:14,920 > Epoch: 618,  train_loss=0.000175617,  val_loss=169.557067871,  accuracy=0.234375000
[INFO|asb_main.py:613] 2018-10-20 05:35:22,864 > Epoch: 619,  train_loss=0.000135004,  val_loss=175.642791748,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 05:36:30,803 > Epoch: 620,  train_loss=0.000136191,  val_loss=174.642669678,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 05:37:38,796 > Epoch: 621,  train_loss=0.000113284,  val_loss=185.513763428,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 05:38:46,755 > Epoch: 622,  train_loss=0.000118565,  val_loss=186.766036987,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 05:39:54,725 > Epoch: 623,  train_loss=0.000103296,  val_loss=178.826141357,  accuracy=0.187500000
[INFO|asb_main.py:613] 2018-10-20 05:41:02,740 > Epoch: 624,  train_loss=0.000243381,  val_loss=218.395141602,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 05:42:10,701 > Epoch: 625,  train_loss=0.000194121,  val_loss=216.997940063,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 05:43:18,708 > Epoch: 626,  train_loss=0.000150123,  val_loss=200.292495728,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 05:44:26,686 > Epoch: 627,  train_loss=0.000095020,  val_loss=214.069427490,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 05:45:34,661 > Epoch: 628,  train_loss=0.000092022,  val_loss=219.986511230,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 05:46:42,640 > Epoch: 629,  train_loss=0.000093468,  val_loss=217.645492554,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 05:47:50,586 > Epoch: 630,  train_loss=0.000089074,  val_loss=226.589736938,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 05:48:58,580 > Epoch: 631,  train_loss=0.000082750,  val_loss=232.127502441,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 05:50:06,563 > Epoch: 632,  train_loss=0.000085418,  val_loss=222.331436157,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 05:51:14,504 > Epoch: 633,  train_loss=0.000085906,  val_loss=248.225067139,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 05:52:22,454 > Epoch: 634,  train_loss=0.000076069,  val_loss=255.095397949,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 05:53:30,446 > Epoch: 635,  train_loss=0.000154054,  val_loss=225.797973633,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 05:54:38,441 > Epoch: 636,  train_loss=0.000099624,  val_loss=231.403686523,  accuracy=0.187500000
[INFO|asb_main.py:613] 2018-10-20 05:55:46,441 > Epoch: 637,  train_loss=0.000079603,  val_loss=237.609893799,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 05:56:54,484 > Epoch: 638,  train_loss=0.000080508,  val_loss=232.712265015,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 05:58:02,486 > Epoch: 639,  train_loss=0.000085465,  val_loss=242.834136963,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 05:59:10,403 > Epoch: 640,  train_loss=0.000076653,  val_loss=333.175231934,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 06:00:18,378 > Epoch: 641,  train_loss=0.000418310,  val_loss=151.913360596,  accuracy=0.296875000
[INFO|asb_main.py:613] 2018-10-20 06:01:26,334 > Epoch: 642,  train_loss=0.039234214,  val_loss=95.752166748,  accuracy=0.359375000
[INFO|asb_main.py:613] 2018-10-20 06:02:34,298 > Epoch: 643,  train_loss=0.000313028,  val_loss=98.484458923,  accuracy=0.351562500
[INFO|asb_main.py:613] 2018-10-20 06:03:42,274 > Epoch: 644,  train_loss=0.000181086,  val_loss=96.727645874,  accuracy=0.359375000
[INFO|asb_main.py:613] 2018-10-20 06:04:50,206 > Epoch: 645,  train_loss=0.000129627,  val_loss=99.030349731,  accuracy=0.335937500
[INFO|asb_main.py:613] 2018-10-20 06:05:58,176 > Epoch: 646,  train_loss=0.000122921,  val_loss=108.218276978,  accuracy=0.328125000
[INFO|asb_main.py:613] 2018-10-20 06:07:06,120 > Epoch: 647,  train_loss=0.000118440,  val_loss=96.220184326,  accuracy=0.359375000
[INFO|asb_main.py:613] 2018-10-20 06:08:14,081 > Epoch: 648,  train_loss=0.000139448,  val_loss=95.093948364,  accuracy=0.375000000
[INFO|asb_main.py:613] 2018-10-20 06:09:22,006 > Epoch: 649,  train_loss=0.000164334,  val_loss=101.934890747,  accuracy=0.335937500
[INFO|asb_main.py:613] 2018-10-20 06:10:29,965 > Epoch: 650,  train_loss=0.000150098,  val_loss=95.466934204,  accuracy=0.351562500
[INFO|asb_main.py:613] 2018-10-20 06:11:37,916 > Epoch: 651,  train_loss=0.000145248,  val_loss=101.458412170,  accuracy=0.320312500
[INFO|asb_main.py:613] 2018-10-20 06:12:45,879 > Epoch: 652,  train_loss=0.000141545,  val_loss=114.173118591,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 06:13:53,830 > Epoch: 653,  train_loss=0.000150043,  val_loss=104.519699097,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 06:15:01,781 > Epoch: 654,  train_loss=0.000133224,  val_loss=120.316436768,  accuracy=0.289062500
[INFO|asb_main.py:613] 2018-10-20 06:16:09,709 > Epoch: 655,  train_loss=0.000215124,  val_loss=140.843887329,  accuracy=0.289062500
[INFO|asb_main.py:613] 2018-10-20 06:17:17,672 > Epoch: 656,  train_loss=0.000124142,  val_loss=116.150054932,  accuracy=0.289062500
[INFO|asb_main.py:613] 2018-10-20 06:18:25,613 > Epoch: 657,  train_loss=0.000124863,  val_loss=142.002349854,  accuracy=0.289062500
[INFO|asb_main.py:613] 2018-10-20 06:19:33,589 > Epoch: 658,  train_loss=0.000103086,  val_loss=139.403472900,  accuracy=0.281250000
[INFO|asb_main.py:613] 2018-10-20 06:20:41,556 > Epoch: 659,  train_loss=0.000187861,  val_loss=149.837585449,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 06:21:49,550 > Epoch: 660,  train_loss=0.000113520,  val_loss=153.043670654,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 06:22:57,497 > Epoch: 661,  train_loss=0.000122125,  val_loss=150.070922852,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 06:24:05,533 > Epoch: 662,  train_loss=0.000149274,  val_loss=166.794097900,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 06:25:13,466 > Epoch: 663,  train_loss=0.000092942,  val_loss=153.081329346,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 06:26:21,394 > Epoch: 664,  train_loss=0.000151065,  val_loss=158.829406738,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 06:27:29,339 > Epoch: 665,  train_loss=0.000131467,  val_loss=159.995590210,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 06:28:37,318 > Epoch: 666,  train_loss=0.000131288,  val_loss=157.008834839,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 06:29:45,289 > Epoch: 667,  train_loss=0.000128682,  val_loss=154.599090576,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 06:30:53,262 > Epoch: 668,  train_loss=0.000122640,  val_loss=169.565765381,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 06:32:01,214 > Epoch: 669,  train_loss=0.000151360,  val_loss=186.310211182,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 06:33:09,177 > Epoch: 670,  train_loss=0.000147332,  val_loss=201.632553101,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 06:34:17,131 > Epoch: 671,  train_loss=0.000181582,  val_loss=139.144866943,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 06:35:25,106 > Epoch: 672,  train_loss=0.000154946,  val_loss=218.076324463,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 06:36:33,082 > Epoch: 673,  train_loss=0.000193037,  val_loss=237.650833130,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 06:37:41,072 > Epoch: 674,  train_loss=0.000099506,  val_loss=238.464920044,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 06:38:49,005 > Epoch: 675,  train_loss=0.000141288,  val_loss=242.508605957,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 06:39:56,972 > Epoch: 676,  train_loss=0.000077215,  val_loss=242.789886475,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 06:41:04,942 > Epoch: 677,  train_loss=0.000186152,  val_loss=250.144470215,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 06:42:12,897 > Epoch: 678,  train_loss=0.000092409,  val_loss=252.550537109,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 06:43:20,896 > Epoch: 679,  train_loss=0.000114763,  val_loss=251.045806885,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 06:44:28,869 > Epoch: 680,  train_loss=0.000114690,  val_loss=254.376281738,  accuracy=0.187500000
[INFO|asb_main.py:613] 2018-10-20 06:45:36,873 > Epoch: 681,  train_loss=0.000143374,  val_loss=254.453765869,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 06:46:44,825 > Epoch: 682,  train_loss=0.000084523,  val_loss=252.710052490,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 06:47:52,796 > Epoch: 683,  train_loss=0.000114917,  val_loss=246.298828125,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 06:49:00,748 > Epoch: 684,  train_loss=0.000069621,  val_loss=246.328399658,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 06:50:08,720 > Epoch: 685,  train_loss=0.000140458,  val_loss=238.662933350,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 06:51:16,715 > Epoch: 686,  train_loss=0.000059501,  val_loss=233.522308350,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 06:52:24,672 > Epoch: 687,  train_loss=0.000217322,  val_loss=252.126876831,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 06:53:32,690 > Epoch: 688,  train_loss=0.000143855,  val_loss=263.445068359,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 06:54:40,705 > Epoch: 689,  train_loss=0.000088776,  val_loss=256.623687744,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 06:55:48,683 > Epoch: 690,  train_loss=0.000081821,  val_loss=255.738418579,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 06:56:56,670 > Epoch: 691,  train_loss=0.000079823,  val_loss=258.364227295,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 06:58:04,626 > Epoch: 692,  train_loss=0.000085794,  val_loss=247.306121826,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 06:59:12,600 > Epoch: 693,  train_loss=0.000080797,  val_loss=236.648849487,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:00:20,643 > Epoch: 694,  train_loss=0.000095309,  val_loss=222.227233887,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:01:28,675 > Epoch: 695,  train_loss=0.000098377,  val_loss=214.354553223,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:02:36,682 > Epoch: 696,  train_loss=0.000106050,  val_loss=200.490081787,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:03:44,642 > Epoch: 697,  train_loss=0.000071663,  val_loss=198.559326172,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:04:52,578 > Epoch: 698,  train_loss=0.000103315,  val_loss=174.602111816,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:06:00,606 > Epoch: 699,  train_loss=0.000105426,  val_loss=164.617660522,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:07:08,582 > Epoch: 700,  train_loss=0.000097196,  val_loss=183.602416992,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:08:16,595 > Epoch: 701,  train_loss=0.000080124,  val_loss=183.024719238,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:09:24,599 > Epoch: 702,  train_loss=0.000081219,  val_loss=174.625167847,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:10:32,572 > Epoch: 703,  train_loss=0.000075180,  val_loss=173.303909302,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:11:40,519 > Epoch: 704,  train_loss=0.000079288,  val_loss=161.261993408,  accuracy=0.156250000
[INFO|asb_main.py:613] 2018-10-20 07:12:48,467 > Epoch: 705,  train_loss=0.000083518,  val_loss=157.785369873,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:13:56,439 > Epoch: 706,  train_loss=0.000090648,  val_loss=190.222686768,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:15:04,490 > Epoch: 707,  train_loss=0.000100216,  val_loss=177.770462036,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:16:12,488 > Epoch: 708,  train_loss=0.000131848,  val_loss=227.219100952,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:17:20,474 > Epoch: 709,  train_loss=0.000217477,  val_loss=194.795227051,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:18:28,483 > Epoch: 710,  train_loss=0.000100936,  val_loss=220.257629395,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:19:36,500 > Epoch: 711,  train_loss=0.000073519,  val_loss=220.267333984,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:20:44,459 > Epoch: 712,  train_loss=0.000072333,  val_loss=218.018371582,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 07:21:52,483 > Epoch: 713,  train_loss=0.000075517,  val_loss=215.459396362,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 07:23:00,499 > Epoch: 714,  train_loss=0.000073321,  val_loss=214.417922974,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 07:24:08,468 > Epoch: 715,  train_loss=0.000072761,  val_loss=213.151733398,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 07:25:16,457 > Epoch: 716,  train_loss=0.000072512,  val_loss=212.118988037,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 07:26:24,437 > Epoch: 717,  train_loss=0.000072460,  val_loss=209.012924194,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 07:27:32,425 > Epoch: 718,  train_loss=0.000072195,  val_loss=207.301574707,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 07:28:40,411 > Epoch: 719,  train_loss=0.000072085,  val_loss=205.615036011,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 07:29:48,387 > Epoch: 720,  train_loss=0.000072791,  val_loss=191.075515747,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 07:30:56,401 > Epoch: 721,  train_loss=0.000081437,  val_loss=184.256744385,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 07:32:04,406 > Epoch: 722,  train_loss=0.000071747,  val_loss=187.805633545,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 07:33:12,405 > Epoch: 723,  train_loss=0.000071446,  val_loss=190.797042847,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 07:34:20,470 > Epoch: 724,  train_loss=0.000071395,  val_loss=193.075500488,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 07:35:28,433 > Epoch: 725,  train_loss=0.000071439,  val_loss=195.210876465,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 07:36:36,459 > Epoch: 726,  train_loss=0.000071234,  val_loss=189.950408936,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 07:37:44,506 > Epoch: 727,  train_loss=0.000071668,  val_loss=176.690933228,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 07:38:52,523 > Epoch: 728,  train_loss=0.000085344,  val_loss=195.903900146,  accuracy=0.187500000
[INFO|asb_main.py:613] 2018-10-20 07:40:00,555 > Epoch: 729,  train_loss=0.000439403,  val_loss=157.952758789,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 07:41:08,507 > Epoch: 730,  train_loss=0.013604357,  val_loss=80.730880737,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 07:42:16,487 > Epoch: 731,  train_loss=0.001049288,  val_loss=57.578468323,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 07:43:24,462 > Epoch: 732,  train_loss=0.000177194,  val_loss=57.369712830,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 07:44:32,508 > Epoch: 733,  train_loss=0.000151444,  val_loss=58.798683167,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 07:45:40,503 > Epoch: 734,  train_loss=0.000168201,  val_loss=60.118354797,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 07:46:48,487 > Epoch: 735,  train_loss=0.000127073,  val_loss=60.880584717,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 07:47:56,524 > Epoch: 736,  train_loss=0.000123481,  val_loss=62.510753632,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 07:49:04,546 > Epoch: 737,  train_loss=0.000117855,  val_loss=64.161941528,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 07:50:12,675 > Epoch: 738,  train_loss=0.000114099,  val_loss=66.237876892,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-20 07:51:20,689 > Epoch: 739,  train_loss=0.000113582,  val_loss=66.424476624,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-20 07:52:28,695 > Epoch: 740,  train_loss=0.000110012,  val_loss=69.238082886,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 07:53:36,645 > Epoch: 741,  train_loss=0.000102071,  val_loss=71.807121277,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 07:54:44,675 > Epoch: 742,  train_loss=0.000150665,  val_loss=76.069976807,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 07:55:52,645 > Epoch: 743,  train_loss=0.000079258,  val_loss=80.671791077,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 07:57:00,674 > Epoch: 744,  train_loss=0.000131534,  val_loss=80.763259888,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 07:58:08,659 > Epoch: 745,  train_loss=0.000076553,  val_loss=85.282363892,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 07:59:16,668 > Epoch: 746,  train_loss=0.000123886,  val_loss=88.377929688,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 08:00:24,631 > Epoch: 747,  train_loss=0.000113718,  val_loss=91.785827637,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 08:01:32,644 > Epoch: 748,  train_loss=0.000111876,  val_loss=100.015731812,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 08:02:40,703 > Epoch: 749,  train_loss=0.000109457,  val_loss=108.665115356,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-20 08:03:48,696 > Epoch: 750,  train_loss=0.000109772,  val_loss=117.757713318,  accuracy=0.515625000
[INFO|asb_main.py:613] 2018-10-20 08:04:56,671 > Epoch: 751,  train_loss=0.000104948,  val_loss=132.061920166,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 08:06:04,710 > Epoch: 752,  train_loss=0.000104468,  val_loss=134.705566406,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 08:07:12,731 > Epoch: 753,  train_loss=0.000107711,  val_loss=117.423866272,  accuracy=0.515625000
[INFO|asb_main.py:613] 2018-10-20 08:08:20,693 > Epoch: 754,  train_loss=0.000103468,  val_loss=127.239067078,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 08:09:28,661 > Epoch: 755,  train_loss=0.000106068,  val_loss=130.196411133,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 08:10:36,691 > Epoch: 756,  train_loss=0.000111492,  val_loss=127.105087280,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 08:11:44,681 > Epoch: 757,  train_loss=0.000102452,  val_loss=130.881134033,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 08:12:52,654 > Epoch: 758,  train_loss=0.000096757,  val_loss=135.392974854,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 08:14:00,645 > Epoch: 759,  train_loss=0.000132644,  val_loss=137.271484375,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-20 08:15:08,744 > Epoch: 760,  train_loss=0.000243674,  val_loss=122.059463501,  accuracy=0.484375000
[INFO|asb_main.py:613] 2018-10-20 08:16:16,746 > Epoch: 761,  train_loss=0.000071589,  val_loss=123.849929810,  accuracy=0.500000000
[INFO|asb_main.py:613] 2018-10-20 08:17:24,711 > Epoch: 762,  train_loss=0.000191300,  val_loss=124.217491150,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 08:18:32,746 > Epoch: 763,  train_loss=0.000111118,  val_loss=122.082298279,  accuracy=0.421875000
[INFO|asb_main.py:613] 2018-10-20 08:19:40,751 > Epoch: 764,  train_loss=0.000155140,  val_loss=120.196243286,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 08:20:48,725 > Epoch: 765,  train_loss=0.000083314,  val_loss=121.497665405,  accuracy=0.296875000
[INFO|asb_main.py:613] 2018-10-20 08:21:56,688 > Epoch: 766,  train_loss=0.000068201,  val_loss=120.175979614,  accuracy=0.296875000
[INFO|asb_main.py:613] 2018-10-20 08:23:04,680 > Epoch: 767,  train_loss=0.000139241,  val_loss=113.842758179,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 08:24:12,647 > Epoch: 768,  train_loss=0.000102285,  val_loss=114.626846313,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 08:25:20,661 > Epoch: 769,  train_loss=0.000076733,  val_loss=113.835388184,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 08:26:28,639 > Epoch: 770,  train_loss=0.000083376,  val_loss=109.782653809,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 08:27:36,668 > Epoch: 771,  train_loss=0.000082991,  val_loss=106.789031982,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 08:28:44,683 > Epoch: 772,  train_loss=0.000083393,  val_loss=103.970909119,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 08:29:52,659 > Epoch: 773,  train_loss=0.000077241,  val_loss=102.299346924,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 08:31:00,670 > Epoch: 774,  train_loss=0.000075855,  val_loss=101.431312561,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 08:32:08,676 > Epoch: 775,  train_loss=0.000076672,  val_loss=86.991432190,  accuracy=0.312500000
[INFO|asb_main.py:613] 2018-10-20 08:33:16,650 > Epoch: 776,  train_loss=0.000089590,  val_loss=85.244766235,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 08:34:24,612 > Epoch: 777,  train_loss=0.000076773,  val_loss=92.855628967,  accuracy=0.312500000
[INFO|asb_main.py:613] 2018-10-20 08:35:32,664 > Epoch: 778,  train_loss=0.000080404,  val_loss=95.828460693,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 08:36:40,701 > Epoch: 779,  train_loss=0.000079125,  val_loss=92.485877991,  accuracy=0.312500000
[INFO|asb_main.py:613] 2018-10-20 08:37:48,662 > Epoch: 780,  train_loss=0.000076482,  val_loss=92.384109497,  accuracy=0.328125000
[INFO|asb_main.py:613] 2018-10-20 08:38:56,625 > Epoch: 781,  train_loss=0.000074164,  val_loss=89.823890686,  accuracy=0.335937500
[INFO|asb_main.py:613] 2018-10-20 08:40:04,560 > Epoch: 782,  train_loss=0.000080725,  val_loss=86.538536072,  accuracy=0.335937500
[INFO|asb_main.py:613] 2018-10-20 08:41:12,522 > Epoch: 783,  train_loss=0.000074722,  val_loss=86.349304199,  accuracy=0.335937500
[INFO|asb_main.py:613] 2018-10-20 08:42:20,504 > Epoch: 784,  train_loss=0.000078500,  val_loss=84.012947083,  accuracy=0.343750000
[INFO|asb_main.py:613] 2018-10-20 08:43:28,503 > Epoch: 785,  train_loss=0.000073238,  val_loss=84.966354370,  accuracy=0.335937500
[INFO|asb_main.py:613] 2018-10-20 08:44:36,556 > Epoch: 786,  train_loss=0.000071637,  val_loss=84.888549805,  accuracy=0.343750000
[INFO|asb_main.py:613] 2018-10-20 08:45:44,554 > Epoch: 787,  train_loss=0.000076383,  val_loss=79.443748474,  accuracy=0.328125000
[INFO|asb_main.py:613] 2018-10-20 08:46:52,556 > Epoch: 788,  train_loss=0.000074041,  val_loss=88.094833374,  accuracy=0.335937500
[INFO|asb_main.py:613] 2018-10-20 08:48:00,522 > Epoch: 789,  train_loss=0.000089356,  val_loss=88.587440491,  accuracy=0.281250000
[INFO|asb_main.py:613] 2018-10-20 08:49:08,494 > Epoch: 790,  train_loss=0.000075368,  val_loss=97.620025635,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 08:50:16,473 > Epoch: 791,  train_loss=0.000070657,  val_loss=102.006828308,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 08:51:24,432 > Epoch: 792,  train_loss=0.000075861,  val_loss=173.042633057,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 08:52:32,429 > Epoch: 793,  train_loss=0.000281493,  val_loss=88.556571960,  accuracy=0.351562500
[INFO|asb_main.py:613] 2018-10-20 08:53:40,393 > Epoch: 794,  train_loss=0.000078583,  val_loss=120.354789734,  accuracy=0.281250000
[INFO|asb_main.py:613] 2018-10-20 08:54:48,338 > Epoch: 795,  train_loss=0.000073169,  val_loss=127.627273560,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 08:55:56,348 > Epoch: 796,  train_loss=0.000070469,  val_loss=129.004272461,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 08:57:04,305 > Epoch: 797,  train_loss=0.000069717,  val_loss=130.400161743,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 08:58:12,276 > Epoch: 798,  train_loss=0.000069414,  val_loss=131.711853027,  accuracy=0.226562500
[INFO|asb_main.py:613] 2018-10-20 08:59:20,282 > Epoch: 799,  train_loss=0.000069157,  val_loss=133.776947021,  accuracy=0.226562500
[INFO|asb_main.py:613] 2018-10-20 09:00:28,279 > Epoch: 800,  train_loss=0.000068924,  val_loss=135.656448364,  accuracy=0.226562500
[INFO|asb_main.py:613] 2018-10-20 09:01:36,248 > Epoch: 801,  train_loss=0.000068679,  val_loss=138.579833984,  accuracy=0.234375000
[INFO|asb_main.py:613] 2018-10-20 09:02:44,192 > Epoch: 802,  train_loss=0.000074264,  val_loss=150.977798462,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 09:03:52,187 > Epoch: 803,  train_loss=0.010872646,  val_loss=594.896118164,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 09:05:00,136 > Epoch: 804,  train_loss=0.001496018,  val_loss=116.838027954,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 09:06:08,099 > Epoch: 805,  train_loss=0.000351449,  val_loss=105.228355408,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 09:07:16,066 > Epoch: 806,  train_loss=0.000175011,  val_loss=102.953002930,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 09:08:24,019 > Epoch: 807,  train_loss=0.000130593,  val_loss=111.274276733,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 09:09:31,996 > Epoch: 808,  train_loss=0.000175106,  val_loss=104.552917480,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-20 09:10:39,933 > Epoch: 809,  train_loss=0.000162532,  val_loss=135.085388184,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 09:11:47,869 > Epoch: 810,  train_loss=0.000151245,  val_loss=156.686630249,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-20 09:12:55,845 > Epoch: 811,  train_loss=0.000133755,  val_loss=159.350906372,  accuracy=0.656250000
[INFO|asb_main.py:613] 2018-10-20 09:14:03,803 > Epoch: 812,  train_loss=0.000148082,  val_loss=158.943267822,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-20 09:15:11,779 > Epoch: 813,  train_loss=0.000121922,  val_loss=161.784866333,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-20 09:16:19,739 > Epoch: 814,  train_loss=0.000119364,  val_loss=164.241333008,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-20 09:17:27,708 > Epoch: 815,  train_loss=0.000113770,  val_loss=166.135192871,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-20 09:18:35,685 > Epoch: 816,  train_loss=0.000135079,  val_loss=167.563964844,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 09:19:43,638 > Epoch: 817,  train_loss=0.000139365,  val_loss=170.392181396,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 09:20:51,595 > Epoch: 818,  train_loss=0.000113006,  val_loss=165.852447510,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 09:21:59,577 > Epoch: 819,  train_loss=0.000102704,  val_loss=170.863739014,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-20 09:23:07,617 > Epoch: 820,  train_loss=0.000127085,  val_loss=169.847122192,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 09:24:15,589 > Epoch: 821,  train_loss=0.000104756,  val_loss=173.684448242,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-20 09:25:23,575 > Epoch: 822,  train_loss=0.000104698,  val_loss=173.136627197,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-20 09:26:31,545 > Epoch: 823,  train_loss=0.000097812,  val_loss=177.278137207,  accuracy=0.445312500
[INFO|asb_main.py:613] 2018-10-20 09:27:39,490 > Epoch: 824,  train_loss=0.000095859,  val_loss=181.314529419,  accuracy=0.414062500
[INFO|asb_main.py:613] 2018-10-20 09:28:47,428 > Epoch: 825,  train_loss=0.000093405,  val_loss=181.730667114,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 09:29:55,384 > Epoch: 826,  train_loss=0.000128265,  val_loss=185.475036621,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 09:31:03,391 > Epoch: 827,  train_loss=0.000094804,  val_loss=189.912506104,  accuracy=0.406250000
[INFO|asb_main.py:613] 2018-10-20 09:32:11,346 > Epoch: 828,  train_loss=0.000093650,  val_loss=193.003112793,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 09:33:19,276 > Epoch: 829,  train_loss=0.000082341,  val_loss=193.451965332,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 09:34:27,266 > Epoch: 830,  train_loss=0.000113237,  val_loss=189.189682007,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 09:35:35,232 > Epoch: 831,  train_loss=0.000120282,  val_loss=196.395629883,  accuracy=0.351562500
[INFO|asb_main.py:613] 2018-10-20 09:36:43,186 > Epoch: 832,  train_loss=0.000087605,  val_loss=198.192840576,  accuracy=0.343750000
[INFO|asb_main.py:613] 2018-10-20 09:37:51,158 > Epoch: 833,  train_loss=0.000087944,  val_loss=199.753570557,  accuracy=0.343750000
[INFO|asb_main.py:613] 2018-10-20 09:38:59,188 > Epoch: 834,  train_loss=0.000093563,  val_loss=205.907333374,  accuracy=0.328125000
[INFO|asb_main.py:613] 2018-10-20 09:40:07,153 > Epoch: 835,  train_loss=0.000084246,  val_loss=206.644042969,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 09:41:15,150 > Epoch: 836,  train_loss=0.000088352,  val_loss=207.966293335,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 09:42:23,115 > Epoch: 837,  train_loss=0.000085567,  val_loss=203.518341064,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 09:43:31,135 > Epoch: 838,  train_loss=0.000115465,  val_loss=192.638366699,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 09:44:39,103 > Epoch: 839,  train_loss=0.000077808,  val_loss=194.586135864,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 09:45:47,114 > Epoch: 840,  train_loss=0.000079038,  val_loss=194.956039429,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 09:46:55,084 > Epoch: 841,  train_loss=0.000080743,  val_loss=194.186370850,  accuracy=0.281250000
[INFO|asb_main.py:613] 2018-10-20 09:48:03,083 > Epoch: 842,  train_loss=0.000076718,  val_loss=195.229919434,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 09:49:11,074 > Epoch: 843,  train_loss=0.000076002,  val_loss=195.411010742,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 09:50:19,055 > Epoch: 844,  train_loss=0.000075024,  val_loss=194.776702881,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 09:51:27,015 > Epoch: 845,  train_loss=0.000074602,  val_loss=197.835021973,  accuracy=0.273437500
[INFO|asb_main.py:613] 2018-10-20 09:52:34,986 > Epoch: 846,  train_loss=0.000076028,  val_loss=200.306243896,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 09:53:43,010 > Epoch: 847,  train_loss=0.000073246,  val_loss=201.600570679,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 09:54:50,990 > Epoch: 848,  train_loss=0.000073473,  val_loss=202.803634644,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 09:55:58,952 > Epoch: 849,  train_loss=0.000080432,  val_loss=204.302886963,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 09:57:06,883 > Epoch: 850,  train_loss=0.000077352,  val_loss=190.648223877,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 09:58:14,836 > Epoch: 851,  train_loss=0.000094592,  val_loss=243.626800537,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 09:59:22,829 > Epoch: 852,  train_loss=0.004267915,  val_loss=259.196838379,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 10:00:30,791 > Epoch: 853,  train_loss=0.003340777,  val_loss=236.921585083,  accuracy=0.335937500
[INFO|asb_main.py:613] 2018-10-20 10:01:38,738 > Epoch: 854,  train_loss=0.000283712,  val_loss=246.552520752,  accuracy=0.281250000
[INFO|asb_main.py:613] 2018-10-20 10:02:46,675 > Epoch: 855,  train_loss=0.000103433,  val_loss=247.237731934,  accuracy=0.289062500
[INFO|asb_main.py:613] 2018-10-20 10:03:54,640 > Epoch: 856,  train_loss=0.000113716,  val_loss=250.247451782,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 10:05:02,574 > Epoch: 857,  train_loss=0.000112761,  val_loss=253.270477295,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 10:06:10,516 > Epoch: 858,  train_loss=0.000093612,  val_loss=254.940597534,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 10:07:18,477 > Epoch: 859,  train_loss=0.000090509,  val_loss=255.660568237,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 10:08:26,423 > Epoch: 860,  train_loss=0.000088739,  val_loss=257.134155273,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:09:34,382 > Epoch: 861,  train_loss=0.000091835,  val_loss=251.054016113,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:10:42,328 > Epoch: 862,  train_loss=0.000088123,  val_loss=256.133911133,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:11:50,286 > Epoch: 863,  train_loss=0.000108879,  val_loss=253.198013306,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 10:12:58,211 > Epoch: 864,  train_loss=0.000085135,  val_loss=256.870208740,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:14:06,156 > Epoch: 865,  train_loss=0.000087046,  val_loss=258.201965332,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 10:15:14,107 > Epoch: 866,  train_loss=0.000081195,  val_loss=251.198852539,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 10:16:22,094 > Epoch: 867,  train_loss=0.000259761,  val_loss=229.370315552,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:17:30,073 > Epoch: 868,  train_loss=0.000073847,  val_loss=247.284301758,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 10:18:38,042 > Epoch: 869,  train_loss=0.000132313,  val_loss=224.477615356,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:19:46,007 > Epoch: 870,  train_loss=0.000091451,  val_loss=242.278900146,  accuracy=0.226562500
[INFO|asb_main.py:613] 2018-10-20 10:20:53,969 > Epoch: 871,  train_loss=0.000137469,  val_loss=239.030532837,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 10:22:01,891 > Epoch: 872,  train_loss=0.000079021,  val_loss=255.323669434,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 10:23:09,860 > Epoch: 873,  train_loss=0.000077962,  val_loss=271.137115479,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 10:24:17,826 > Epoch: 874,  train_loss=0.000131272,  val_loss=218.941299438,  accuracy=0.226562500
[INFO|asb_main.py:613] 2018-10-20 10:25:25,824 > Epoch: 875,  train_loss=0.000072349,  val_loss=243.804870605,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 10:26:33,812 > Epoch: 876,  train_loss=0.000107139,  val_loss=242.681121826,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 10:27:41,778 > Epoch: 877,  train_loss=0.000096779,  val_loss=248.068099976,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 10:28:49,777 > Epoch: 878,  train_loss=0.000093531,  val_loss=256.314300537,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 10:29:57,711 > Epoch: 879,  train_loss=0.000089881,  val_loss=262.848022461,  accuracy=0.250000000
[INFO|asb_main.py:613] 2018-10-20 10:31:05,673 > Epoch: 880,  train_loss=0.000089351,  val_loss=241.822799683,  accuracy=0.234375000
[INFO|asb_main.py:613] 2018-10-20 10:32:13,617 > Epoch: 881,  train_loss=0.000083678,  val_loss=249.551025391,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 10:33:21,709 > Epoch: 882,  train_loss=0.000085786,  val_loss=255.458618164,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 10:34:29,692 > Epoch: 883,  train_loss=0.000095264,  val_loss=254.097686768,  accuracy=0.257812500
[INFO|asb_main.py:613] 2018-10-20 10:35:37,656 > Epoch: 884,  train_loss=0.000080943,  val_loss=263.183685303,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 10:36:45,628 > Epoch: 885,  train_loss=0.000079899,  val_loss=269.650878906,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 10:37:53,597 > Epoch: 886,  train_loss=0.000079015,  val_loss=274.544982910,  accuracy=0.265625000
[INFO|asb_main.py:613] 2018-10-20 10:39:01,584 > Epoch: 887,  train_loss=0.000079456,  val_loss=266.106689453,  accuracy=0.242187500
[INFO|asb_main.py:613] 2018-10-20 10:40:09,556 > Epoch: 888,  train_loss=0.000077751,  val_loss=276.760314941,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 10:41:17,521 > Epoch: 889,  train_loss=0.000079200,  val_loss=294.060760498,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 10:42:25,495 > Epoch: 890,  train_loss=0.000089590,  val_loss=233.195037842,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:43:33,477 > Epoch: 891,  train_loss=0.000084708,  val_loss=253.484252930,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:44:41,474 > Epoch: 892,  train_loss=0.000079187,  val_loss=254.164489746,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:45:49,494 > Epoch: 893,  train_loss=0.000076713,  val_loss=285.063903809,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 10:46:57,436 > Epoch: 894,  train_loss=0.000083683,  val_loss=296.271881104,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:48:05,417 > Epoch: 895,  train_loss=0.000075798,  val_loss=315.506072998,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:49:13,390 > Epoch: 896,  train_loss=0.000073394,  val_loss=328.475189209,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:50:21,442 > Epoch: 897,  train_loss=0.000086844,  val_loss=332.082214355,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 10:51:29,455 > Epoch: 898,  train_loss=0.000094635,  val_loss=242.989074707,  accuracy=0.187500000
[INFO|asb_main.py:613] 2018-10-20 10:52:37,448 > Epoch: 899,  train_loss=0.000075738,  val_loss=287.615570068,  accuracy=0.187500000
[INFO|asb_main.py:613] 2018-10-20 10:53:45,441 > Epoch: 900,  train_loss=0.000078910,  val_loss=303.940917969,  accuracy=0.210937500
[INFO|asb_main.py:613] 2018-10-20 10:54:53,408 > Epoch: 901,  train_loss=0.000075572,  val_loss=312.077636719,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 10:56:01,396 > Epoch: 902,  train_loss=0.000075952,  val_loss=322.253082275,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 10:57:09,431 > Epoch: 903,  train_loss=0.000074043,  val_loss=328.818664551,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 10:58:17,449 > Epoch: 904,  train_loss=0.000077934,  val_loss=306.947235107,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 10:59:25,423 > Epoch: 905,  train_loss=0.000082028,  val_loss=324.488647461,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 11:00:33,429 > Epoch: 906,  train_loss=0.000070817,  val_loss=335.593536377,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 11:01:41,417 > Epoch: 907,  train_loss=0.000071255,  val_loss=269.501922607,  accuracy=0.187500000
[INFO|asb_main.py:613] 2018-10-20 11:02:49,414 > Epoch: 908,  train_loss=0.000071653,  val_loss=286.142486572,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 11:03:57,415 > Epoch: 909,  train_loss=0.000076877,  val_loss=300.624938965,  accuracy=0.203125000
[INFO|asb_main.py:613] 2018-10-20 11:05:05,383 > Epoch: 910,  train_loss=0.000071608,  val_loss=301.484619141,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 11:06:13,384 > Epoch: 911,  train_loss=0.000088301,  val_loss=298.175598145,  accuracy=0.195312500
[INFO|asb_main.py:613] 2018-10-20 11:07:21,390 > Epoch: 912,  train_loss=0.000076150,  val_loss=422.879394531,  accuracy=0.218750000
[INFO|asb_main.py:613] 2018-10-20 11:08:29,338 > Epoch: 913,  train_loss=0.000190662,  val_loss=316.846588135,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:09:37,316 > Epoch: 914,  train_loss=0.002452758,  val_loss=344.374938965,  accuracy=0.281250000
[INFO|asb_main.py:613] 2018-10-20 11:10:45,406 > Epoch: 915,  train_loss=0.032407186,  val_loss=290.395599365,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:11:53,362 > Epoch: 916,  train_loss=0.000079028,  val_loss=296.927825928,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:13:01,354 > Epoch: 917,  train_loss=0.000080598,  val_loss=300.361541748,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:14:09,319 > Epoch: 918,  train_loss=0.000078233,  val_loss=302.671203613,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:15:17,347 > Epoch: 919,  train_loss=0.000076623,  val_loss=305.203399658,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:16:25,329 > Epoch: 920,  train_loss=0.000075743,  val_loss=307.538299561,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:17:33,382 > Epoch: 921,  train_loss=0.000074709,  val_loss=310.729583740,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:18:41,408 > Epoch: 922,  train_loss=0.000080381,  val_loss=312.577606201,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:19:49,436 > Epoch: 923,  train_loss=0.000078268,  val_loss=314.562927246,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:20:57,397 > Epoch: 924,  train_loss=0.000077174,  val_loss=316.570922852,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:22:05,381 > Epoch: 925,  train_loss=0.000077922,  val_loss=315.014099121,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:23:13,381 > Epoch: 926,  train_loss=0.000076058,  val_loss=318.260223389,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:24:21,320 > Epoch: 927,  train_loss=0.000075829,  val_loss=321.218078613,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:25:29,294 > Epoch: 928,  train_loss=0.000075539,  val_loss=324.089050293,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:26:37,248 > Epoch: 929,  train_loss=0.000075242,  val_loss=326.859619141,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:27:45,274 > Epoch: 930,  train_loss=0.000076047,  val_loss=323.534301758,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:28:53,257 > Epoch: 931,  train_loss=0.000075179,  val_loss=328.669555664,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:30:01,227 > Epoch: 932,  train_loss=0.000075013,  val_loss=332.518493652,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:31:09,265 > Epoch: 933,  train_loss=0.000074768,  val_loss=335.690429688,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:32:17,245 > Epoch: 934,  train_loss=0.000074531,  val_loss=338.636871338,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:33:25,241 > Epoch: 935,  train_loss=0.000074307,  val_loss=341.991607666,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:34:33,217 > Epoch: 936,  train_loss=0.000074124,  val_loss=345.723754883,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:35:41,251 > Epoch: 937,  train_loss=0.000071886,  val_loss=354.799987793,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:36:49,304 > Epoch: 938,  train_loss=0.000097559,  val_loss=348.559570312,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:37:57,299 > Epoch: 939,  train_loss=0.000079845,  val_loss=350.808258057,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:39:05,268 > Epoch: 940,  train_loss=0.000077413,  val_loss=354.356201172,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:40:13,234 > Epoch: 941,  train_loss=0.000077618,  val_loss=345.031555176,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:41:21,232 > Epoch: 942,  train_loss=0.000087564,  val_loss=346.264038086,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:42:29,210 > Epoch: 943,  train_loss=0.000077506,  val_loss=352.933319092,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:43:37,219 > Epoch: 944,  train_loss=0.000072073,  val_loss=372.853454590,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 11:44:45,208 > Epoch: 945,  train_loss=0.000133338,  val_loss=417.697998047,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 11:45:53,215 > Epoch: 946,  train_loss=0.000109916,  val_loss=369.599365234,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:47:01,200 > Epoch: 947,  train_loss=0.000077550,  val_loss=374.306915283,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:48:09,134 > Epoch: 948,  train_loss=0.000075929,  val_loss=386.324768066,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:49:17,143 > Epoch: 949,  train_loss=0.000079304,  val_loss=378.375976562,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:50:25,188 > Epoch: 950,  train_loss=0.000072528,  val_loss=409.961395264,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 11:51:33,125 > Epoch: 951,  train_loss=0.000115680,  val_loss=325.049468994,  accuracy=0.179687500
[INFO|asb_main.py:613] 2018-10-20 11:52:41,109 > Epoch: 952,  train_loss=0.000072476,  val_loss=341.685028076,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:53:49,082 > Epoch: 953,  train_loss=0.000071507,  val_loss=354.706542969,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:54:57,150 > Epoch: 954,  train_loss=0.000071718,  val_loss=364.171661377,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:56:05,101 > Epoch: 955,  train_loss=0.000071880,  val_loss=372.192504883,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:57:13,088 > Epoch: 956,  train_loss=0.000072005,  val_loss=366.620361328,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:58:21,057 > Epoch: 957,  train_loss=0.000071937,  val_loss=373.779357910,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 11:59:29,034 > Epoch: 958,  train_loss=0.000071965,  val_loss=379.604858398,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:00:37,024 > Epoch: 959,  train_loss=0.000071926,  val_loss=386.416748047,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:01:45,045 > Epoch: 960,  train_loss=0.000071827,  val_loss=393.333984375,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:02:53,049 > Epoch: 961,  train_loss=0.000071760,  val_loss=380.491088867,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:04:01,050 > Epoch: 962,  train_loss=0.000072709,  val_loss=387.481201172,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:05:09,031 > Epoch: 963,  train_loss=0.000071444,  val_loss=394.161468506,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:06:17,021 > Epoch: 964,  train_loss=0.000071468,  val_loss=399.603576660,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:07:24,990 > Epoch: 965,  train_loss=0.000071414,  val_loss=407.236999512,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:08:33,049 > Epoch: 966,  train_loss=0.000071289,  val_loss=415.295288086,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:09:41,029 > Epoch: 967,  train_loss=0.000071245,  val_loss=422.024475098,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:10:49,089 > Epoch: 968,  train_loss=0.000071162,  val_loss=428.347778320,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:11:57,084 > Epoch: 969,  train_loss=0.000071217,  val_loss=399.387695312,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:13:05,111 > Epoch: 970,  train_loss=0.000071100,  val_loss=410.762634277,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:14:13,112 > Epoch: 971,  train_loss=0.000071064,  val_loss=422.373199463,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:15:21,198 > Epoch: 972,  train_loss=0.000070988,  val_loss=407.972717285,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:16:29,169 > Epoch: 973,  train_loss=0.000070587,  val_loss=427.132965088,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:17:37,130 > Epoch: 974,  train_loss=0.000080663,  val_loss=334.095642090,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:18:45,104 > Epoch: 975,  train_loss=0.000070623,  val_loss=390.510192871,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:19:53,144 > Epoch: 976,  train_loss=0.000071833,  val_loss=422.472137451,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:21:01,173 > Epoch: 977,  train_loss=0.000067381,  val_loss=415.963165283,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:22:09,160 > Epoch: 978,  train_loss=0.000078898,  val_loss=444.622985840,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:23:17,149 > Epoch: 979,  train_loss=0.000218941,  val_loss=966.809631348,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:24:25,120 > Epoch: 980,  train_loss=0.000280444,  val_loss=596.524353027,  accuracy=0.171875000
[INFO|asb_main.py:613] 2018-10-20 12:25:33,132 > Epoch: 981,  train_loss=0.008485535,  val_loss=1999.052368164,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 12:26:41,096 > Epoch: 982,  train_loss=0.014352021,  val_loss=2208.794921875,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:27:49,067 > Epoch: 983,  train_loss=0.000235454,  val_loss=2222.273437500,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:28:57,084 > Epoch: 984,  train_loss=0.000118258,  val_loss=2218.859130859,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:30:05,120 > Epoch: 985,  train_loss=0.000105362,  val_loss=2209.286376953,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:31:13,112 > Epoch: 986,  train_loss=0.000095960,  val_loss=2189.922851562,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:32:21,096 > Epoch: 987,  train_loss=0.000108133,  val_loss=2197.305664062,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:33:29,140 > Epoch: 988,  train_loss=0.000092123,  val_loss=2204.720703125,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:34:37,164 > Epoch: 989,  train_loss=0.000164210,  val_loss=2255.753417969,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:35:45,147 > Epoch: 990,  train_loss=0.000094587,  val_loss=2250.972167969,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:36:53,115 > Epoch: 991,  train_loss=0.000086535,  val_loss=2241.980468750,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:38:01,135 > Epoch: 992,  train_loss=0.000105305,  val_loss=2249.412841797,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:39:09,104 > Epoch: 993,  train_loss=0.000084363,  val_loss=2249.812744141,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:40:17,099 > Epoch: 994,  train_loss=0.000082486,  val_loss=2293.957031250,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:41:25,103 > Epoch: 995,  train_loss=0.000080236,  val_loss=2290.299560547,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:42:33,079 > Epoch: 996,  train_loss=0.000079582,  val_loss=2289.271240234,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:43:41,085 > Epoch: 997,  train_loss=0.000079134,  val_loss=2286.125732422,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:44:49,120 > Epoch: 998,  train_loss=0.000078146,  val_loss=2275.476074219,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:45:57,129 > Epoch: 999,  train_loss=0.000076946,  val_loss=2273.774658203,  accuracy=0.164062500
[INFO|asb_main.py:613] 2018-10-20 12:47:05,161 > Epoch:1000,  train_loss=0.000076274,  val_loss=2273.002441406,  accuracy=0.164062500
[INFO|asb_main.py:624] 2018-10-20 12:47:05,453 > ### Learning Finished!
[INFO|asb_main.py:870] 2018-10-20 12:47:05,458 > Model_4 Program end [ Total time : 18 Hour 52 Minute 16 Second ]
[INFO|asb_main.py:540] 2018-10-20 12:47:05,459 > ### Learning Start!
2018-10-20 12:47:05.460148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-10-20 12:47:05.460174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-20 12:47:05.460179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-10-20 12:47:05.460183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-10-20 12:47:05.460280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10347 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
[INFO|asb_main.py:74] 2018-10-20 12:47:06,680 > Writing to /home/sdsra/Documents/Char-level-tf/runs/1540064826_model_5
[INFO|asb_main.py:47] 2018-10-20 12:47:06,713 > ### Parameters ----------------------------------
[INFO|asb_main.py:53] 2018-10-20 12:47:06,714 > 	epochs : 1000
[INFO|asb_main.py:53] 2018-10-20 12:47:06,714 > 	batch_size : 128
[INFO|asb_main.py:53] 2018-10-20 12:47:06,714 > 	learning_rate : 0.001
[INFO|asb_main.py:53] 2018-10-20 12:47:06,714 > 	dropout_rate : 0.5
[INFO|asb_main.py:53] 2018-10-20 12:47:06,714 > 	alphabet : abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'"/\|_@#$%^&*~`+-=<>()[]{}

[INFO|asb_main.py:53] 2018-10-20 12:47:06,714 > 	char_max_length : 500
[INFO|asb_main.py:53] 2018-10-20 12:47:06,714 > 	num_of_classes : 3
[INFO|asb_main.py:53] 2018-10-20 12:47:06,714 > 	input_num_of_rows : 3
[INFO|asb_main.py:57] 2018-10-20 12:47:06,714 > ### Values of Graph  ----------------------------------
[INFO|asb_main.py:61] 2018-10-20 12:47:06,714 > 	<tf.Variable 'Layer1/conv2d/kernel:0' shape=(3, 3, 1, 32) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,714 > 	<tf.Variable 'Layer1/conv2d/bias:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,714 > 	<tf.Variable 'Layer1/batch_normalization/gamma:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,714 > 	<tf.Variable 'Layer1/batch_normalization/beta:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,714 > 	<tf.Variable 'Layer1/batch_normalization/moving_mean:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,714 > 	<tf.Variable 'Layer1/batch_normalization/moving_variance:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,714 > 	<tf.Variable 'Layer2/conv2d/kernel:0' shape=(3, 3, 32, 32) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,714 > 	<tf.Variable 'Layer2/conv2d/bias:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,714 > 	<tf.Variable 'Layer2/batch_normalization/gamma:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer2/batch_normalization/beta:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer2/batch_normalization/moving_mean:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer2/batch_normalization/moving_variance:0' shape=(32,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer3/conv2d/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer3/conv2d/bias:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer3/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer3/batch_normalization/beta:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer3/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer3/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer4/conv2d/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer4/conv2d/bias:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer4/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer4/batch_normalization/beta:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer4/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'Layer4/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'FC/dense/kernel:0' shape=(28672, 1024) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'FC/dense/bias:0' shape=(1024,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'dense/kernel:0' shape=(1024, 3) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-20 12:47:06,715 > 	<tf.Variable 'dense/bias:0' shape=(3,) dtype=float32_ref>
[INFO|asb_main.py:65] 2018-10-20 12:47:06,715 > --------------------------------------------------------------------
[INFO|asb_main.py:613] 2018-10-20 12:48:14,625 > Epoch:   1,  train_loss=1.228405966,  val_loss=1.002096057,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 12:49:22,346 > Epoch:   2,  train_loss=0.113262859,  val_loss=1.030222297,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 12:50:30,127 > Epoch:   3,  train_loss=0.058122568,  val_loss=1.023725033,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 12:51:37,862 > Epoch:   4,  train_loss=0.027675461,  val_loss=1.036783814,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-20 12:52:45,684 > Epoch:   5,  train_loss=0.011059201,  val_loss=1.031993508,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 12:53:53,478 > Epoch:   6,  train_loss=0.005927292,  val_loss=1.030208349,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 12:55:01,333 > Epoch:   7,  train_loss=0.003475731,  val_loss=1.032554507,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 12:56:09,158 > Epoch:   8,  train_loss=0.001966137,  val_loss=1.028950453,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 12:57:16,897 > Epoch:   9,  train_loss=0.001589300,  val_loss=1.031713963,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 12:58:24,652 > Epoch:  10,  train_loss=0.000793292,  val_loss=1.028446913,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 12:59:32,419 > Epoch:  11,  train_loss=0.000707436,  val_loss=1.027011991,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 13:00:40,156 > Epoch:  12,  train_loss=0.000616579,  val_loss=1.028350472,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-20 13:01:48,040 > Epoch:  13,  train_loss=0.032880436,  val_loss=0.978661656,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-20 13:02:55,837 > Epoch:  14,  train_loss=0.198746084,  val_loss=0.814419150,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 13:04:03,705 > Epoch:  15,  train_loss=0.025296661,  val_loss=0.935213566,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-20 13:05:11,502 > Epoch:  16,  train_loss=0.009246498,  val_loss=0.955943167,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 13:06:19,350 > Epoch:  17,  train_loss=0.003702920,  val_loss=0.966323853,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 13:07:27,209 > Epoch:  18,  train_loss=0.001620256,  val_loss=0.971722126,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 13:08:34,974 > Epoch:  19,  train_loss=0.001567155,  val_loss=0.973841667,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-20 13:09:42,774 > Epoch:  20,  train_loss=0.000681011,  val_loss=0.968251288,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-20 13:10:50,658 > Epoch:  21,  train_loss=0.000617684,  val_loss=0.967388690,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-20 13:11:58,468 > Epoch:  22,  train_loss=0.000438073,  val_loss=0.969305098,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 13:13:06,239 > Epoch:  23,  train_loss=0.000390193,  val_loss=0.968887687,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 13:14:14,057 > Epoch:  24,  train_loss=0.000348941,  val_loss=0.966895342,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 13:15:21,804 > Epoch:  25,  train_loss=0.000363785,  val_loss=0.962306857,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-20 13:16:29,587 > Epoch:  26,  train_loss=0.000363975,  val_loss=0.964035332,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 13:17:37,398 > Epoch:  27,  train_loss=0.000364241,  val_loss=0.961658239,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-20 13:18:45,165 > Epoch:  28,  train_loss=0.000353559,  val_loss=0.960845351,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-20 13:19:52,984 > Epoch:  29,  train_loss=0.000456691,  val_loss=0.959740818,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-20 13:21:00,799 > Epoch:  30,  train_loss=0.000388710,  val_loss=0.959427893,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-20 13:22:08,619 > Epoch:  31,  train_loss=0.000415017,  val_loss=0.962186992,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 13:23:16,528 > Epoch:  32,  train_loss=0.000396594,  val_loss=0.960024416,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 13:24:24,276 > Epoch:  33,  train_loss=0.000375619,  val_loss=0.956115723,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 13:25:32,095 > Epoch:  34,  train_loss=0.000409002,  val_loss=0.956480086,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 13:26:39,863 > Epoch:  35,  train_loss=0.000376721,  val_loss=0.956564903,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 13:27:47,614 > Epoch:  36,  train_loss=0.000341310,  val_loss=0.952708721,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 13:28:55,462 > Epoch:  37,  train_loss=0.000381956,  val_loss=0.956696630,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 13:30:03,277 > Epoch:  38,  train_loss=0.000379097,  val_loss=0.955124795,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 13:31:11,090 > Epoch:  39,  train_loss=0.000387525,  val_loss=0.954316020,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 13:32:18,894 > Epoch:  40,  train_loss=0.000383859,  val_loss=0.945017159,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-20 13:33:26,650 > Epoch:  41,  train_loss=0.000353705,  val_loss=0.946326733,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-20 13:34:34,456 > Epoch:  42,  train_loss=0.000542304,  val_loss=0.959758461,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 13:35:42,239 > Epoch:  43,  train_loss=0.110786699,  val_loss=0.755411386,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 13:36:49,980 > Epoch:  44,  train_loss=0.102834635,  val_loss=0.647786260,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-20 13:37:57,809 > Epoch:  45,  train_loss=0.011028079,  val_loss=0.604557455,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-20 13:39:05,663 > Epoch:  46,  train_loss=0.004127792,  val_loss=0.610815883,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-20 13:40:13,474 > Epoch:  47,  train_loss=0.002260123,  val_loss=0.628504038,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-20 13:41:21,293 > Epoch:  48,  train_loss=0.001769357,  val_loss=0.641970277,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-20 13:42:29,138 > Epoch:  49,  train_loss=0.000904682,  val_loss=0.654794812,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 13:43:36,895 > Epoch:  50,  train_loss=0.000453064,  val_loss=0.652381837,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 13:44:44,668 > Epoch:  51,  train_loss=0.000402843,  val_loss=0.651671112,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 13:45:52,429 > Epoch:  52,  train_loss=0.000358596,  val_loss=0.649750233,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 13:47:00,230 > Epoch:  53,  train_loss=0.000341640,  val_loss=0.647684515,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 13:48:08,052 > Epoch:  54,  train_loss=0.000327510,  val_loss=0.647053123,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 13:49:15,911 > Epoch:  55,  train_loss=0.000320639,  val_loss=0.647945166,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 13:50:23,829 > Epoch:  56,  train_loss=0.000315160,  val_loss=0.649709821,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 13:51:31,584 > Epoch:  57,  train_loss=0.000312211,  val_loss=0.649484217,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 13:52:39,429 > Epoch:  58,  train_loss=0.000310399,  val_loss=0.647600532,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 13:53:47,173 > Epoch:  59,  train_loss=0.000333563,  val_loss=0.649316549,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 13:54:54,982 > Epoch:  60,  train_loss=0.000301505,  val_loss=0.666273832,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 13:56:02,848 > Epoch:  61,  train_loss=0.000302030,  val_loss=0.672705173,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 13:57:10,711 > Epoch:  62,  train_loss=0.000303684,  val_loss=0.668707848,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 13:58:18,538 > Epoch:  63,  train_loss=0.000304110,  val_loss=0.657806039,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 13:59:26,349 > Epoch:  64,  train_loss=0.000304332,  val_loss=0.661918402,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 14:00:34,160 > Epoch:  65,  train_loss=0.000324071,  val_loss=0.657294393,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 14:01:41,891 > Epoch:  66,  train_loss=0.000308132,  val_loss=0.656823039,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 14:02:49,679 > Epoch:  67,  train_loss=0.000310241,  val_loss=0.660902143,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 14:03:57,422 > Epoch:  68,  train_loss=0.000306016,  val_loss=0.654528677,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 14:05:05,296 > Epoch:  69,  train_loss=0.000298957,  val_loss=0.666520178,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 14:06:13,112 > Epoch:  70,  train_loss=0.045457781,  val_loss=0.559278011,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 14:07:20,938 > Epoch:  71,  train_loss=0.040384799,  val_loss=0.789830089,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-20 14:08:28,741 > Epoch:  72,  train_loss=0.001319906,  val_loss=0.676925063,  accuracy=0.671875000
[INFO|asb_main.py:613] 2018-10-20 14:09:36,518 > Epoch:  73,  train_loss=0.000430385,  val_loss=0.636545897,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 14:10:44,248 > Epoch:  74,  train_loss=0.000290318,  val_loss=0.627772093,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 14:11:52,030 > Epoch:  75,  train_loss=0.000270707,  val_loss=0.621957183,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 14:12:59,748 > Epoch:  76,  train_loss=0.000264984,  val_loss=0.618634701,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 14:14:07,587 > Epoch:  77,  train_loss=0.000266309,  val_loss=0.613563359,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 14:15:15,483 > Epoch:  78,  train_loss=0.000259558,  val_loss=0.610403299,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 14:16:23,254 > Epoch:  79,  train_loss=0.000256672,  val_loss=0.613819838,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 14:17:31,094 > Epoch:  80,  train_loss=0.000256112,  val_loss=0.607656121,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 14:18:38,838 > Epoch:  81,  train_loss=0.000263317,  val_loss=0.608528733,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 14:19:46,635 > Epoch:  82,  train_loss=0.000257445,  val_loss=0.606332183,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 14:20:54,407 > Epoch:  83,  train_loss=0.000258290,  val_loss=0.613303661,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 14:22:02,239 > Epoch:  84,  train_loss=0.000259803,  val_loss=0.615066051,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 14:23:10,084 > Epoch:  85,  train_loss=0.000259132,  val_loss=0.614173055,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 14:24:17,935 > Epoch:  86,  train_loss=0.000260933,  val_loss=0.598904788,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 14:25:25,779 > Epoch:  87,  train_loss=0.000265295,  val_loss=0.593715429,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 14:26:33,563 > Epoch:  88,  train_loss=0.000265362,  val_loss=0.602949858,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 14:27:41,334 > Epoch:  89,  train_loss=0.000274645,  val_loss=0.595723510,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 14:28:49,118 > Epoch:  90,  train_loss=0.000261830,  val_loss=0.615860581,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 14:29:56,858 > Epoch:  91,  train_loss=0.000261809,  val_loss=0.628248870,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 14:31:04,671 > Epoch:  92,  train_loss=0.000262834,  val_loss=0.632370830,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 14:32:12,456 > Epoch:  93,  train_loss=0.000260268,  val_loss=0.639892399,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 14:33:20,282 > Epoch:  94,  train_loss=0.000262310,  val_loss=0.662567735,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 14:34:28,065 > Epoch:  95,  train_loss=0.000306380,  val_loss=0.690110981,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 14:35:35,930 > Epoch:  96,  train_loss=0.000267667,  val_loss=0.653974295,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 14:36:43,692 > Epoch:  97,  train_loss=0.000333079,  val_loss=0.660164595,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 14:37:51,461 > Epoch:  98,  train_loss=0.000347813,  val_loss=0.648204505,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 14:38:59,205 > Epoch:  99,  train_loss=0.000315656,  val_loss=0.644247413,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 14:40:06,959 > Epoch: 100,  train_loss=0.000374226,  val_loss=0.662579536,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 14:41:14,763 > Epoch: 101,  train_loss=0.000298177,  val_loss=0.652968526,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 14:42:22,559 > Epoch: 102,  train_loss=0.000297231,  val_loss=0.649132431,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 14:43:30,433 > Epoch: 103,  train_loss=0.000328809,  val_loss=0.649668813,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 14:44:38,263 > Epoch: 104,  train_loss=0.000272208,  val_loss=0.653672159,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 14:45:46,085 > Epoch: 105,  train_loss=0.000227825,  val_loss=0.657207608,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 14:46:53,835 > Epoch: 106,  train_loss=0.000214273,  val_loss=0.658366680,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 14:48:01,638 > Epoch: 107,  train_loss=0.000209876,  val_loss=0.658050954,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 14:49:09,424 > Epoch: 108,  train_loss=0.000312205,  val_loss=0.685020089,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 14:50:17,309 > Epoch: 109,  train_loss=0.047981848,  val_loss=0.635468841,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 14:51:25,163 > Epoch: 110,  train_loss=0.006121032,  val_loss=0.383066833,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 14:52:33,035 > Epoch: 111,  train_loss=0.000498346,  val_loss=0.360338986,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 14:53:40,870 > Epoch: 112,  train_loss=0.000284946,  val_loss=0.359694362,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 14:54:48,654 > Epoch: 113,  train_loss=0.000245553,  val_loss=0.358024150,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 14:55:56,424 > Epoch: 114,  train_loss=0.000233529,  val_loss=0.353501916,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 14:57:04,191 > Epoch: 115,  train_loss=0.000244616,  val_loss=0.349955082,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 14:58:12,018 > Epoch: 116,  train_loss=0.000230986,  val_loss=0.346510231,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 14:59:19,826 > Epoch: 117,  train_loss=0.000231139,  val_loss=0.345712483,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 15:00:27,632 > Epoch: 118,  train_loss=0.000229281,  val_loss=0.342997879,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 15:01:35,408 > Epoch: 119,  train_loss=0.000229589,  val_loss=0.340307474,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 15:02:43,241 > Epoch: 120,  train_loss=0.000229526,  val_loss=0.341512382,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:03:51,090 > Epoch: 121,  train_loss=0.000230529,  val_loss=0.342550755,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:04:58,915 > Epoch: 122,  train_loss=0.000230588,  val_loss=0.342014313,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:06:06,672 > Epoch: 123,  train_loss=0.000230955,  val_loss=0.340854853,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:07:14,424 > Epoch: 124,  train_loss=0.000232670,  val_loss=0.339437664,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:08:22,246 > Epoch: 125,  train_loss=0.000236291,  val_loss=0.340671808,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:09:30,030 > Epoch: 126,  train_loss=0.000234877,  val_loss=0.337840497,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:10:37,902 > Epoch: 127,  train_loss=0.000237069,  val_loss=0.339805216,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:11:45,734 > Epoch: 128,  train_loss=0.000236996,  val_loss=0.336490959,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:12:53,510 > Epoch: 129,  train_loss=0.000241758,  val_loss=0.338137001,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:14:01,297 > Epoch: 130,  train_loss=0.000238685,  val_loss=0.337293863,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:15:09,083 > Epoch: 131,  train_loss=0.000237270,  val_loss=0.337143987,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:16:16,861 > Epoch: 132,  train_loss=0.000233592,  val_loss=0.337590843,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:17:24,663 > Epoch: 133,  train_loss=0.000222655,  val_loss=0.341049135,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:18:32,540 > Epoch: 134,  train_loss=0.000218282,  val_loss=0.342784762,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 15:19:40,421 > Epoch: 135,  train_loss=0.000215616,  val_loss=0.346513242,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 15:20:48,249 > Epoch: 136,  train_loss=0.000213856,  val_loss=0.345828533,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:21:56,033 > Epoch: 137,  train_loss=0.000223207,  val_loss=0.351443082,  accuracy=0.867187500
[INFO|asb_main.py:613] 2018-10-20 15:23:03,851 > Epoch: 138,  train_loss=0.000212205,  val_loss=0.357161492,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-20 15:24:11,627 > Epoch: 139,  train_loss=0.000222717,  val_loss=0.375634044,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 15:25:19,368 > Epoch: 140,  train_loss=0.000188773,  val_loss=0.382765442,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 15:26:27,198 > Epoch: 141,  train_loss=0.000217030,  val_loss=0.380140215,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 15:27:35,032 > Epoch: 142,  train_loss=0.000190950,  val_loss=0.380942583,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 15:28:42,847 > Epoch: 143,  train_loss=0.154021095,  val_loss=1.047411084,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-20 15:29:50,637 > Epoch: 144,  train_loss=0.002718812,  val_loss=0.389333725,  accuracy=0.898437500
[INFO|asb_main.py:613] 2018-10-20 15:30:58,379 > Epoch: 145,  train_loss=0.000607785,  val_loss=0.305974633,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:32:06,164 > Epoch: 146,  train_loss=0.000686931,  val_loss=0.302774131,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-20 15:33:13,940 > Epoch: 147,  train_loss=0.000219391,  val_loss=0.318268538,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-20 15:34:21,783 > Epoch: 148,  train_loss=0.000262617,  val_loss=0.321862042,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-20 15:35:29,641 > Epoch: 149,  train_loss=0.000195464,  val_loss=0.322749794,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-20 15:36:37,462 > Epoch: 150,  train_loss=0.000185323,  val_loss=0.322098732,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:37:45,263 > Epoch: 151,  train_loss=0.000180085,  val_loss=0.321848094,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:38:53,087 > Epoch: 152,  train_loss=0.000177681,  val_loss=0.321008772,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:40:00,900 > Epoch: 153,  train_loss=0.000174672,  val_loss=0.320404291,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:41:08,683 > Epoch: 154,  train_loss=0.000174485,  val_loss=0.325735748,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:42:16,479 > Epoch: 155,  train_loss=0.000170874,  val_loss=0.322955787,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:43:24,274 > Epoch: 156,  train_loss=0.000173530,  val_loss=0.323039204,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:44:32,113 > Epoch: 157,  train_loss=0.000174495,  val_loss=0.323851615,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:45:39,907 > Epoch: 158,  train_loss=0.000174738,  val_loss=0.322685778,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:46:47,739 > Epoch: 159,  train_loss=0.000176853,  val_loss=0.322814226,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:47:55,588 > Epoch: 160,  train_loss=0.000177157,  val_loss=0.322858304,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:49:03,366 > Epoch: 161,  train_loss=0.000179174,  val_loss=0.321946621,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:50:11,158 > Epoch: 162,  train_loss=0.000179557,  val_loss=0.320220619,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:51:18,896 > Epoch: 163,  train_loss=0.000181348,  val_loss=0.300080121,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:52:26,668 > Epoch: 164,  train_loss=0.000185136,  val_loss=0.299666226,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:53:34,484 > Epoch: 165,  train_loss=0.000188346,  val_loss=0.302274346,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:54:42,383 > Epoch: 166,  train_loss=0.000183990,  val_loss=0.297762156,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:55:50,217 > Epoch: 167,  train_loss=0.000168683,  val_loss=0.306265801,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:56:58,087 > Epoch: 168,  train_loss=0.000172457,  val_loss=0.310606122,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:58:05,895 > Epoch: 169,  train_loss=0.000166022,  val_loss=0.301232666,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 15:59:13,661 > Epoch: 170,  train_loss=0.000262351,  val_loss=0.299219966,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 16:00:21,438 > Epoch: 171,  train_loss=0.000213816,  val_loss=0.302796155,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 16:01:29,232 > Epoch: 172,  train_loss=0.000198750,  val_loss=0.306396723,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 16:02:37,007 > Epoch: 173,  train_loss=0.000232133,  val_loss=0.304705232,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 16:03:44,803 > Epoch: 174,  train_loss=0.000192805,  val_loss=0.298842967,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 16:04:52,643 > Epoch: 175,  train_loss=0.000266456,  val_loss=0.295590997,  accuracy=0.906250000
[INFO|asb_main.py:613] 2018-10-20 16:06:00,489 > Epoch: 176,  train_loss=0.000295236,  val_loss=0.299299598,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-20 16:07:08,273 > Epoch: 177,  train_loss=0.054151577,  val_loss=0.264180094,  accuracy=0.921875000
[INFO|asb_main.py:613] 2018-10-20 16:08:16,056 > Epoch: 178,  train_loss=0.002603074,  val_loss=0.634585023,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 16:09:23,856 > Epoch: 179,  train_loss=0.000253565,  val_loss=0.677759767,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 16:10:31,618 > Epoch: 180,  train_loss=0.000202540,  val_loss=0.672699153,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 16:11:39,430 > Epoch: 181,  train_loss=0.000191151,  val_loss=0.685211420,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 16:12:47,263 > Epoch: 182,  train_loss=0.000184357,  val_loss=0.686903477,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 16:13:55,112 > Epoch: 183,  train_loss=0.000181644,  val_loss=0.681136072,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 16:15:02,916 > Epoch: 184,  train_loss=0.000177363,  val_loss=0.684135556,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 16:16:10,710 > Epoch: 185,  train_loss=0.000174650,  val_loss=0.685978174,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 16:17:18,482 > Epoch: 186,  train_loss=0.000172955,  val_loss=0.695590258,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 16:18:26,300 > Epoch: 187,  train_loss=0.000170756,  val_loss=0.685040832,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 16:19:34,146 > Epoch: 188,  train_loss=0.000208726,  val_loss=0.697673678,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 16:20:42,002 > Epoch: 189,  train_loss=0.000186540,  val_loss=0.701125085,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 16:21:49,862 > Epoch: 190,  train_loss=0.000183514,  val_loss=0.699481785,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 16:22:57,708 > Epoch: 191,  train_loss=0.000179768,  val_loss=0.692850232,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 16:24:05,532 > Epoch: 192,  train_loss=0.000177906,  val_loss=0.698807836,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 16:25:13,342 > Epoch: 193,  train_loss=0.000174129,  val_loss=0.706982076,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 16:26:21,112 > Epoch: 194,  train_loss=0.000181668,  val_loss=0.710335553,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 16:27:28,902 > Epoch: 195,  train_loss=0.000184651,  val_loss=0.719973981,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 16:28:36,666 > Epoch: 196,  train_loss=0.000183053,  val_loss=0.746313870,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 16:29:44,504 > Epoch: 197,  train_loss=0.000179372,  val_loss=0.762969613,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 16:30:52,331 > Epoch: 198,  train_loss=0.000175614,  val_loss=0.771866918,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 16:32:00,131 > Epoch: 199,  train_loss=0.000288232,  val_loss=0.789116621,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 16:33:07,934 > Epoch: 200,  train_loss=0.000178191,  val_loss=0.827314556,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-20 16:34:15,733 > Epoch: 201,  train_loss=0.000173188,  val_loss=0.829118967,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-20 16:35:23,542 > Epoch: 202,  train_loss=0.000176104,  val_loss=0.833795190,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-20 16:36:31,353 > Epoch: 203,  train_loss=0.000175060,  val_loss=0.843964756,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-20 16:37:39,206 > Epoch: 204,  train_loss=0.000165343,  val_loss=0.834598482,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-20 16:38:47,077 > Epoch: 205,  train_loss=0.000161331,  val_loss=0.843202114,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 16:39:54,934 > Epoch: 206,  train_loss=0.000154952,  val_loss=0.839908779,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 16:41:02,768 > Epoch: 207,  train_loss=0.000150549,  val_loss=0.838112772,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 16:42:10,577 > Epoch: 208,  train_loss=0.000149152,  val_loss=0.824503720,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 16:43:18,374 > Epoch: 209,  train_loss=0.000160831,  val_loss=0.836908937,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-20 16:44:26,201 > Epoch: 210,  train_loss=0.000141531,  val_loss=0.799372494,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 16:45:34,033 > Epoch: 211,  train_loss=0.000138330,  val_loss=0.804017663,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-20 16:46:41,861 > Epoch: 212,  train_loss=0.000123753,  val_loss=0.747281194,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 16:47:49,670 > Epoch: 213,  train_loss=0.000244314,  val_loss=0.746200860,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-20 16:48:57,515 > Epoch: 214,  train_loss=0.004462469,  val_loss=0.763282061,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 16:50:05,303 > Epoch: 215,  train_loss=0.016119532,  val_loss=1.224413395,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-20 16:51:13,178 > Epoch: 216,  train_loss=0.006868510,  val_loss=11.574575424,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-20 16:52:20,986 > Epoch: 217,  train_loss=0.001435257,  val_loss=3.338068962,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 16:53:28,779 > Epoch: 218,  train_loss=0.000334708,  val_loss=2.735641956,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 16:54:36,546 > Epoch: 219,  train_loss=0.000394981,  val_loss=2.298426867,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-20 16:55:44,368 > Epoch: 220,  train_loss=0.000264796,  val_loss=2.145834446,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 16:56:52,219 > Epoch: 221,  train_loss=0.000223490,  val_loss=2.088075876,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 16:58:00,107 > Epoch: 222,  train_loss=0.000212841,  val_loss=2.094235897,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 16:59:07,953 > Epoch: 223,  train_loss=0.000208456,  val_loss=2.013195515,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 17:00:15,773 > Epoch: 224,  train_loss=0.000206350,  val_loss=2.003242731,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 17:01:23,550 > Epoch: 225,  train_loss=0.000204968,  val_loss=1.835801363,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-20 17:02:31,333 > Epoch: 226,  train_loss=0.000199929,  val_loss=2.220360279,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 17:03:39,135 > Epoch: 227,  train_loss=0.000203868,  val_loss=1.682348371,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 17:04:46,907 > Epoch: 228,  train_loss=0.000197403,  val_loss=2.189369678,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-20 17:05:54,782 > Epoch: 229,  train_loss=0.000201072,  val_loss=1.857463241,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 17:07:02,611 > Epoch: 230,  train_loss=0.000202123,  val_loss=1.675472975,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-20 17:08:10,430 > Epoch: 231,  train_loss=0.000187917,  val_loss=2.273878574,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:09:18,252 > Epoch: 232,  train_loss=0.000200610,  val_loss=1.714998364,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 17:10:26,067 > Epoch: 233,  train_loss=0.000179731,  val_loss=2.341085196,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 17:11:33,855 > Epoch: 234,  train_loss=0.000197658,  val_loss=1.575700998,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-20 17:12:41,679 > Epoch: 235,  train_loss=0.000189324,  val_loss=1.863270640,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 17:13:49,527 > Epoch: 236,  train_loss=0.000188420,  val_loss=1.448683739,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-20 17:14:57,405 > Epoch: 237,  train_loss=0.000189626,  val_loss=2.887822628,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:16:05,250 > Epoch: 238,  train_loss=0.000196145,  val_loss=1.483202696,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-20 17:17:13,086 > Epoch: 239,  train_loss=0.000172316,  val_loss=2.971833706,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 17:18:20,942 > Epoch: 240,  train_loss=0.000179882,  val_loss=1.828619480,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 17:19:28,811 > Epoch: 241,  train_loss=0.000181032,  val_loss=2.790467262,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 17:20:36,611 > Epoch: 242,  train_loss=0.000174302,  val_loss=1.491429448,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 17:21:44,383 > Epoch: 243,  train_loss=0.000152051,  val_loss=2.981430531,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 17:22:52,170 > Epoch: 244,  train_loss=0.000158097,  val_loss=1.843272448,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 17:23:59,981 > Epoch: 245,  train_loss=0.000136256,  val_loss=3.141673565,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 17:25:07,852 > Epoch: 246,  train_loss=0.000146014,  val_loss=1.750353575,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 17:26:15,726 > Epoch: 247,  train_loss=0.000130901,  val_loss=3.323481083,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 17:27:23,555 > Epoch: 248,  train_loss=0.000140290,  val_loss=1.740667105,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 17:28:31,331 > Epoch: 249,  train_loss=0.000133859,  val_loss=3.305974245,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 17:29:39,106 > Epoch: 250,  train_loss=0.000143608,  val_loss=2.473490715,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 17:30:46,877 > Epoch: 251,  train_loss=0.000124483,  val_loss=2.536564112,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 17:31:54,644 > Epoch: 252,  train_loss=0.000108723,  val_loss=3.169596434,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 17:33:02,443 > Epoch: 253,  train_loss=0.000866556,  val_loss=3.043787956,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-20 17:34:10,266 > Epoch: 254,  train_loss=0.092415145,  val_loss=3.975125313,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 17:35:18,139 > Epoch: 255,  train_loss=0.000664860,  val_loss=5.752439022,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 17:36:25,969 > Epoch: 256,  train_loss=0.000138541,  val_loss=5.599855423,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:37:33,748 > Epoch: 257,  train_loss=0.000369359,  val_loss=5.654934883,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:38:41,541 > Epoch: 258,  train_loss=0.000219979,  val_loss=5.713601589,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 17:39:49,381 > Epoch: 259,  train_loss=0.000189919,  val_loss=5.722120285,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 17:40:57,202 > Epoch: 260,  train_loss=0.000179685,  val_loss=5.748112202,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 17:42:05,037 > Epoch: 261,  train_loss=0.000173406,  val_loss=5.750081539,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 17:43:12,840 > Epoch: 262,  train_loss=0.000169247,  val_loss=5.742040634,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:44:20,682 > Epoch: 263,  train_loss=0.000166460,  val_loss=5.755383492,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:45:28,532 > Epoch: 264,  train_loss=0.000166179,  val_loss=5.775406361,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:46:36,332 > Epoch: 265,  train_loss=0.000165622,  val_loss=5.780690193,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:47:44,119 > Epoch: 266,  train_loss=0.000163171,  val_loss=5.801112652,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:48:51,879 > Epoch: 267,  train_loss=0.000165242,  val_loss=5.832369328,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:49:59,659 > Epoch: 268,  train_loss=0.000160466,  val_loss=5.833915710,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 17:51:07,502 > Epoch: 269,  train_loss=0.000167211,  val_loss=5.876250267,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 17:52:15,353 > Epoch: 270,  train_loss=0.000162751,  val_loss=5.897291183,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 17:53:23,224 > Epoch: 271,  train_loss=0.000167075,  val_loss=5.933156490,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 17:54:31,059 > Epoch: 272,  train_loss=0.000164470,  val_loss=5.942821503,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 17:55:38,850 > Epoch: 273,  train_loss=0.000167882,  val_loss=5.974443436,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 17:56:46,644 > Epoch: 274,  train_loss=0.000161903,  val_loss=5.982821941,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 17:57:54,404 > Epoch: 275,  train_loss=0.000167909,  val_loss=6.020959854,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 17:59:02,214 > Epoch: 276,  train_loss=0.000163130,  val_loss=6.033549309,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 18:00:10,027 > Epoch: 277,  train_loss=0.000166255,  val_loss=6.107654572,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 18:01:17,883 > Epoch: 278,  train_loss=0.000160432,  val_loss=6.134891987,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 18:02:25,802 > Epoch: 279,  train_loss=0.000145039,  val_loss=6.248825073,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 18:03:33,628 > Epoch: 280,  train_loss=0.000208733,  val_loss=6.225163460,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 18:04:41,436 > Epoch: 281,  train_loss=0.000189416,  val_loss=6.387331486,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 18:05:49,276 > Epoch: 282,  train_loss=0.000181139,  val_loss=6.416859150,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 18:06:57,076 > Epoch: 283,  train_loss=0.000173639,  val_loss=6.486765385,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 18:08:04,844 > Epoch: 284,  train_loss=0.000168887,  val_loss=6.559066772,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 18:09:12,713 > Epoch: 285,  train_loss=0.000159492,  val_loss=6.725914955,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 18:10:20,541 > Epoch: 286,  train_loss=0.000159460,  val_loss=6.855916977,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 18:11:28,398 > Epoch: 287,  train_loss=0.000154004,  val_loss=6.967921734,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-20 18:12:36,296 > Epoch: 288,  train_loss=0.000150948,  val_loss=6.908227921,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-20 18:13:44,057 > Epoch: 289,  train_loss=0.000290644,  val_loss=7.311766624,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 18:14:51,861 > Epoch: 290,  train_loss=0.000168369,  val_loss=7.377501488,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 18:15:59,709 > Epoch: 291,  train_loss=0.000155872,  val_loss=7.343207359,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 18:17:07,487 > Epoch: 292,  train_loss=0.000156034,  val_loss=7.542634487,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 18:18:15,358 > Epoch: 293,  train_loss=0.000141421,  val_loss=7.525583267,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 18:19:23,210 > Epoch: 294,  train_loss=0.000137952,  val_loss=7.602155209,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 18:20:31,060 > Epoch: 295,  train_loss=0.000132225,  val_loss=7.672244549,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 18:21:38,877 > Epoch: 296,  train_loss=0.000126884,  val_loss=7.922102928,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 18:22:46,702 > Epoch: 297,  train_loss=0.000127376,  val_loss=7.929035664,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 18:23:54,485 > Epoch: 298,  train_loss=0.000133981,  val_loss=8.118225098,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-20 18:25:02,272 > Epoch: 299,  train_loss=0.000126079,  val_loss=8.001733780,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 18:26:10,148 > Epoch: 300,  train_loss=0.000126481,  val_loss=8.270017624,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 18:27:17,997 > Epoch: 301,  train_loss=0.000123635,  val_loss=8.515140533,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 18:28:25,827 > Epoch: 302,  train_loss=0.000124839,  val_loss=8.481337547,  accuracy=0.671875000
[INFO|asb_main.py:613] 2018-10-20 18:29:33,703 > Epoch: 303,  train_loss=0.000122074,  val_loss=8.684744835,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-20 18:30:41,554 > Epoch: 304,  train_loss=0.000125570,  val_loss=8.787207603,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-20 18:31:49,373 > Epoch: 305,  train_loss=0.000121127,  val_loss=9.047874451,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-20 18:32:57,170 > Epoch: 306,  train_loss=0.000121433,  val_loss=9.256360054,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-20 18:34:04,942 > Epoch: 307,  train_loss=0.000124303,  val_loss=9.115391731,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 18:35:12,726 > Epoch: 308,  train_loss=0.000121178,  val_loss=8.810422897,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 18:36:20,572 > Epoch: 309,  train_loss=0.000125282,  val_loss=8.838431358,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-20 18:37:28,516 > Epoch: 310,  train_loss=0.000120931,  val_loss=9.102324486,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-20 18:38:36,366 > Epoch: 311,  train_loss=0.000121605,  val_loss=9.174276352,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-20 18:39:44,267 > Epoch: 312,  train_loss=0.000120555,  val_loss=9.484431267,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-20 18:40:52,092 > Epoch: 313,  train_loss=0.000119761,  val_loss=9.529147148,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-20 18:41:59,866 > Epoch: 314,  train_loss=0.000121563,  val_loss=9.633006096,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-20 18:43:07,668 > Epoch: 315,  train_loss=0.000121167,  val_loss=9.301374435,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 18:44:15,468 > Epoch: 316,  train_loss=0.000118906,  val_loss=9.900162697,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-20 18:45:23,316 > Epoch: 317,  train_loss=0.001416099,  val_loss=4.162623405,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 18:46:31,193 > Epoch: 318,  train_loss=0.007805226,  val_loss=15.095346451,  accuracy=0.304687500
[INFO|asb_main.py:613] 2018-10-20 18:47:39,122 > Epoch: 319,  train_loss=0.000593443,  val_loss=9.017302513,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-20 18:48:46,968 > Epoch: 320,  train_loss=0.000248155,  val_loss=10.466558456,  accuracy=0.460937500
[INFO|asb_main.py:613] 2018-10-20 18:49:54,776 > Epoch: 321,  train_loss=0.000169366,  val_loss=10.433410645,  accuracy=0.460937500
[INFO|asb_main.py:613] 2018-10-20 18:51:02,574 > Epoch: 322,  train_loss=0.000157752,  val_loss=10.448703766,  accuracy=0.460937500
[INFO|asb_main.py:613] 2018-10-20 18:52:10,419 > Epoch: 323,  train_loss=0.000149305,  val_loss=10.585529327,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 18:53:18,230 > Epoch: 324,  train_loss=0.000144767,  val_loss=10.544637680,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 18:54:26,070 > Epoch: 325,  train_loss=0.000141720,  val_loss=10.783674240,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 18:55:33,900 > Epoch: 326,  train_loss=0.000139658,  val_loss=10.841964722,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 18:56:41,767 > Epoch: 327,  train_loss=0.000137923,  val_loss=10.727621078,  accuracy=0.453125000
[INFO|asb_main.py:613] 2018-10-20 18:57:49,669 > Epoch: 328,  train_loss=0.000137060,  val_loss=10.765396118,  accuracy=0.460937500
[INFO|asb_main.py:613] 2018-10-20 18:58:57,479 > Epoch: 329,  train_loss=0.000123691,  val_loss=11.318798065,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-20 19:00:05,315 > Epoch: 330,  train_loss=0.000117617,  val_loss=11.591650963,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:01:13,088 > Epoch: 331,  train_loss=0.000115512,  val_loss=11.562574387,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:02:20,912 > Epoch: 332,  train_loss=0.000116201,  val_loss=11.623619080,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:03:28,763 > Epoch: 333,  train_loss=0.000116015,  val_loss=11.513555527,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:04:36,592 > Epoch: 334,  train_loss=0.000116429,  val_loss=11.506994247,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:05:44,425 > Epoch: 335,  train_loss=0.000116354,  val_loss=11.125864983,  accuracy=0.398437500
[INFO|asb_main.py:613] 2018-10-20 19:06:52,242 > Epoch: 336,  train_loss=0.000117765,  val_loss=11.396575928,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:08:00,044 > Epoch: 337,  train_loss=0.000117117,  val_loss=11.213716507,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 19:09:07,903 > Epoch: 338,  train_loss=0.000117748,  val_loss=11.639993668,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:10:15,677 > Epoch: 339,  train_loss=0.000117689,  val_loss=11.637885094,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:11:23,475 > Epoch: 340,  train_loss=0.000118240,  val_loss=11.498788834,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:12:31,334 > Epoch: 341,  train_loss=0.000118450,  val_loss=11.927345276,  accuracy=0.375000000
[INFO|asb_main.py:613] 2018-10-20 19:13:39,174 > Epoch: 342,  train_loss=0.000118115,  val_loss=11.618651390,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:14:46,988 > Epoch: 343,  train_loss=0.000118372,  val_loss=12.055354118,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 19:15:54,823 > Epoch: 344,  train_loss=0.000118918,  val_loss=12.006710052,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 19:17:02,656 > Epoch: 345,  train_loss=0.000118826,  val_loss=11.665269852,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 19:18:10,498 > Epoch: 346,  train_loss=0.000119267,  val_loss=12.456886292,  accuracy=0.367187500
[INFO|asb_main.py:613] 2018-10-20 19:19:18,279 > Epoch: 347,  train_loss=0.000118993,  val_loss=12.250757217,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 19:20:26,096 > Epoch: 348,  train_loss=0.000118577,  val_loss=12.345657349,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 19:21:33,964 > Epoch: 349,  train_loss=0.000120692,  val_loss=13.094411850,  accuracy=0.375000000
[INFO|asb_main.py:613] 2018-10-20 19:22:41,781 > Epoch: 350,  train_loss=0.000117850,  val_loss=12.285737991,  accuracy=0.390625000
[INFO|asb_main.py:613] 2018-10-20 19:23:49,615 > Epoch: 351,  train_loss=0.000121751,  val_loss=13.721340179,  accuracy=0.375000000
[INFO|asb_main.py:613] 2018-10-20 19:24:57,450 > Epoch: 352,  train_loss=0.000116853,  val_loss=12.576725006,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 19:26:05,267 > Epoch: 353,  train_loss=0.000120466,  val_loss=11.107477188,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 19:27:13,104 > Epoch: 354,  train_loss=0.000116902,  val_loss=12.055795670,  accuracy=0.375000000
[INFO|asb_main.py:613] 2018-10-20 19:28:20,945 > Epoch: 355,  train_loss=0.000119606,  val_loss=11.756002426,  accuracy=0.375000000
[INFO|asb_main.py:613] 2018-10-20 19:29:28,785 > Epoch: 356,  train_loss=0.000116055,  val_loss=11.423958778,  accuracy=0.382812500
[INFO|asb_main.py:613] 2018-10-20 19:30:36,620 > Epoch: 357,  train_loss=0.000109839,  val_loss=8.827409744,  accuracy=0.546875000
[INFO|asb_main.py:613] 2018-10-20 19:31:44,472 > Epoch: 358,  train_loss=0.023945939,  val_loss=5.959776402,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:32:52,305 > Epoch: 359,  train_loss=0.002186249,  val_loss=5.514569759,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:34:00,154 > Epoch: 360,  train_loss=0.000124711,  val_loss=5.498568058,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:35:08,017 > Epoch: 361,  train_loss=0.000114221,  val_loss=5.492743492,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:36:15,834 > Epoch: 362,  train_loss=0.000110346,  val_loss=5.490261078,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:37:23,711 > Epoch: 363,  train_loss=0.000108694,  val_loss=5.488428593,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:38:31,512 > Epoch: 364,  train_loss=0.000107505,  val_loss=5.488042831,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:39:39,377 > Epoch: 365,  train_loss=0.000105065,  val_loss=5.480553627,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:40:47,202 > Epoch: 366,  train_loss=0.000108798,  val_loss=5.490113735,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:41:55,018 > Epoch: 367,  train_loss=0.000109244,  val_loss=5.487977028,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:43:02,853 > Epoch: 368,  train_loss=0.000108718,  val_loss=5.496552467,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:44:10,664 > Epoch: 369,  train_loss=0.000109582,  val_loss=5.488582134,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:45:18,466 > Epoch: 370,  train_loss=0.000109154,  val_loss=5.489789009,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:46:26,287 > Epoch: 371,  train_loss=0.000110479,  val_loss=5.485234737,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:47:34,080 > Epoch: 372,  train_loss=0.000109855,  val_loss=5.495083332,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:48:41,927 > Epoch: 373,  train_loss=0.000111131,  val_loss=5.484607220,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:49:49,865 > Epoch: 374,  train_loss=0.000110546,  val_loss=5.488961697,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:50:57,776 > Epoch: 375,  train_loss=0.000112040,  val_loss=5.483642578,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:52:05,633 > Epoch: 376,  train_loss=0.000111242,  val_loss=5.492027283,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:53:13,442 > Epoch: 377,  train_loss=0.000112841,  val_loss=5.476972580,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:54:21,289 > Epoch: 378,  train_loss=0.000111856,  val_loss=5.473047733,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:55:29,107 > Epoch: 379,  train_loss=0.000113919,  val_loss=5.467501163,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:56:37,038 > Epoch: 380,  train_loss=0.000113198,  val_loss=5.465560436,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:57:44,903 > Epoch: 381,  train_loss=0.000113334,  val_loss=5.456525803,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 19:58:52,752 > Epoch: 382,  train_loss=0.000114250,  val_loss=5.434118271,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:00:00,652 > Epoch: 383,  train_loss=0.000113769,  val_loss=5.441509247,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:01:08,502 > Epoch: 384,  train_loss=0.000114697,  val_loss=5.423600674,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:02:16,318 > Epoch: 385,  train_loss=0.000113780,  val_loss=5.430325031,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:03:24,118 > Epoch: 386,  train_loss=0.000116281,  val_loss=5.403185368,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:04:31,931 > Epoch: 387,  train_loss=0.000114329,  val_loss=5.411825180,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:05:39,733 > Epoch: 388,  train_loss=0.000116292,  val_loss=5.385326385,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:06:47,620 > Epoch: 389,  train_loss=0.000115450,  val_loss=5.363856792,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:07:55,454 > Epoch: 390,  train_loss=0.000115700,  val_loss=5.365942955,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:09:03,372 > Epoch: 391,  train_loss=0.000116249,  val_loss=5.315111637,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:10:11,242 > Epoch: 392,  train_loss=0.000116542,  val_loss=5.295544624,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:11:19,114 > Epoch: 393,  train_loss=0.000115586,  val_loss=5.296434879,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:12:26,961 > Epoch: 394,  train_loss=0.000117889,  val_loss=5.238977432,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:13:34,775 > Epoch: 395,  train_loss=0.000115842,  val_loss=5.245856762,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:14:42,610 > Epoch: 396,  train_loss=0.000115952,  val_loss=5.239399433,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:15:50,493 > Epoch: 397,  train_loss=0.000116132,  val_loss=5.225746632,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:16:58,390 > Epoch: 398,  train_loss=0.000118047,  val_loss=5.187433720,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:18:06,227 > Epoch: 399,  train_loss=0.000114012,  val_loss=5.180561066,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:19:14,122 > Epoch: 400,  train_loss=0.000115827,  val_loss=5.150048256,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:20:21,976 > Epoch: 401,  train_loss=0.000113872,  val_loss=5.153008938,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:21:29,847 > Epoch: 402,  train_loss=0.000112528,  val_loss=5.097329617,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:22:37,677 > Epoch: 403,  train_loss=0.000112366,  val_loss=5.092801094,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:23:45,533 > Epoch: 404,  train_loss=0.000112942,  val_loss=5.074914455,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:24:53,400 > Epoch: 405,  train_loss=0.000091694,  val_loss=5.117279053,  accuracy=0.890625000
[INFO|asb_main.py:613] 2018-10-20 20:26:01,299 > Epoch: 406,  train_loss=0.001284655,  val_loss=5.051015854,  accuracy=0.882812500
[INFO|asb_main.py:613] 2018-10-20 20:27:09,176 > Epoch: 407,  train_loss=0.016428319,  val_loss=3.981794357,  accuracy=0.875000000
[INFO|asb_main.py:613] 2018-10-20 20:28:17,053 > Epoch: 408,  train_loss=0.001186963,  val_loss=3.075160742,  accuracy=0.914062500
[INFO|asb_main.py:613] 2018-10-20 20:29:24,879 > Epoch: 409,  train_loss=0.006810413,  val_loss=6.900842667,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 20:30:32,674 > Epoch: 410,  train_loss=0.000470399,  val_loss=6.461456776,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 20:31:40,484 > Epoch: 411,  train_loss=0.000447527,  val_loss=7.070410728,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 20:32:48,251 > Epoch: 412,  train_loss=0.000349558,  val_loss=6.991711617,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 20:33:56,095 > Epoch: 413,  train_loss=0.000240674,  val_loss=7.105003834,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 20:35:03,909 > Epoch: 414,  train_loss=0.000225434,  val_loss=7.137918949,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 20:36:11,757 > Epoch: 415,  train_loss=0.000217612,  val_loss=7.242002010,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 20:37:19,587 > Epoch: 416,  train_loss=0.000202679,  val_loss=7.246324062,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 20:38:27,377 > Epoch: 417,  train_loss=0.000311112,  val_loss=7.484917641,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 20:39:35,152 > Epoch: 418,  train_loss=0.000195338,  val_loss=7.672154427,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 20:40:42,977 > Epoch: 419,  train_loss=0.000182699,  val_loss=7.729099751,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 20:41:50,819 > Epoch: 420,  train_loss=0.000182804,  val_loss=7.785674095,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 20:42:58,639 > Epoch: 421,  train_loss=0.000178753,  val_loss=7.830361366,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 20:44:06,470 > Epoch: 422,  train_loss=0.000177774,  val_loss=7.893655777,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 20:45:14,337 > Epoch: 423,  train_loss=0.000176155,  val_loss=7.975604057,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-20 20:46:22,209 > Epoch: 424,  train_loss=0.000172580,  val_loss=8.022937775,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-20 20:47:29,988 > Epoch: 425,  train_loss=0.000172243,  val_loss=8.064505577,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-20 20:48:37,830 > Epoch: 426,  train_loss=0.000168761,  val_loss=8.152675629,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-20 20:49:45,632 > Epoch: 427,  train_loss=0.000169637,  val_loss=8.152463913,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 20:50:53,432 > Epoch: 428,  train_loss=0.000166696,  val_loss=8.242910385,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 20:52:01,267 > Epoch: 429,  train_loss=0.000166294,  val_loss=8.251862526,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 20:53:09,109 > Epoch: 430,  train_loss=0.000163538,  val_loss=8.372718811,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 20:54:16,926 > Epoch: 431,  train_loss=0.000164173,  val_loss=8.432422638,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 20:55:24,769 > Epoch: 432,  train_loss=0.000150396,  val_loss=8.360755920,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 20:56:32,557 > Epoch: 433,  train_loss=0.000130256,  val_loss=8.329104424,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 20:57:40,363 > Epoch: 434,  train_loss=0.000128067,  val_loss=8.355198860,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 20:58:48,150 > Epoch: 435,  train_loss=0.000128650,  val_loss=8.298835754,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 20:59:56,006 > Epoch: 436,  train_loss=0.000126977,  val_loss=8.268529892,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 21:01:03,831 > Epoch: 437,  train_loss=0.000127595,  val_loss=8.245895386,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 21:02:11,667 > Epoch: 438,  train_loss=0.000128295,  val_loss=8.223201752,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:03:19,493 > Epoch: 439,  train_loss=0.000127910,  val_loss=8.242269516,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:04:27,304 > Epoch: 440,  train_loss=0.000127482,  val_loss=8.222953796,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:05:35,088 > Epoch: 441,  train_loss=0.000128613,  val_loss=8.228598595,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:06:42,867 > Epoch: 442,  train_loss=0.000126718,  val_loss=8.237970352,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:07:50,664 > Epoch: 443,  train_loss=0.000127367,  val_loss=8.228339195,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:08:58,480 > Epoch: 444,  train_loss=0.000126936,  val_loss=8.166076660,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:10:06,289 > Epoch: 445,  train_loss=0.000126514,  val_loss=7.993814468,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:11:14,172 > Epoch: 446,  train_loss=0.000092595,  val_loss=8.097252846,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-20 21:12:22,048 > Epoch: 447,  train_loss=0.000439650,  val_loss=5.312205315,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 21:13:29,918 > Epoch: 448,  train_loss=0.000131901,  val_loss=5.393029690,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:14:37,740 > Epoch: 449,  train_loss=0.000111888,  val_loss=5.289595127,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 21:15:45,557 > Epoch: 450,  train_loss=0.000116556,  val_loss=5.499814510,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 21:16:53,337 > Epoch: 451,  train_loss=0.000117916,  val_loss=5.467021942,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 21:18:01,131 > Epoch: 452,  train_loss=0.000117729,  val_loss=5.405060768,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:19:08,974 > Epoch: 453,  train_loss=0.000116939,  val_loss=5.432826042,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:20:16,889 > Epoch: 454,  train_loss=0.000117432,  val_loss=5.409891605,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:21:24,756 > Epoch: 455,  train_loss=0.000116317,  val_loss=5.447573662,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:22:32,635 > Epoch: 456,  train_loss=0.000117317,  val_loss=5.370430470,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:23:40,426 > Epoch: 457,  train_loss=0.000115483,  val_loss=5.439774513,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:24:48,238 > Epoch: 458,  train_loss=0.000116974,  val_loss=5.360190392,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:25:56,019 > Epoch: 459,  train_loss=0.000116011,  val_loss=5.344287872,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:27:03,842 > Epoch: 460,  train_loss=0.000115157,  val_loss=5.450366020,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:28:11,681 > Epoch: 461,  train_loss=0.000115563,  val_loss=5.420820713,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:29:19,567 > Epoch: 462,  train_loss=0.000114953,  val_loss=5.399833679,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 21:30:27,400 > Epoch: 463,  train_loss=0.000113842,  val_loss=5.480668068,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-20 21:31:35,285 > Epoch: 464,  train_loss=0.000114301,  val_loss=5.601443291,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 21:32:43,121 > Epoch: 465,  train_loss=0.000116582,  val_loss=5.215255737,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 21:33:50,928 > Epoch: 466,  train_loss=0.006813859,  val_loss=18.055694580,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 21:34:58,730 > Epoch: 467,  train_loss=0.011928329,  val_loss=13.130045891,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 21:36:06,569 > Epoch: 468,  train_loss=0.000943850,  val_loss=20.837741852,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 21:37:14,511 > Epoch: 469,  train_loss=0.000190458,  val_loss=21.249603271,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 21:38:22,355 > Epoch: 470,  train_loss=0.000171912,  val_loss=21.351890564,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 21:39:30,272 > Epoch: 471,  train_loss=0.000168477,  val_loss=21.442104340,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 21:40:38,120 > Epoch: 472,  train_loss=0.000165886,  val_loss=21.524290085,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 21:41:45,956 > Epoch: 473,  train_loss=0.000164975,  val_loss=21.592941284,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 21:42:53,792 > Epoch: 474,  train_loss=0.000163700,  val_loss=21.676895142,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 21:44:01,625 > Epoch: 475,  train_loss=0.000163026,  val_loss=21.750583649,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 21:45:09,414 > Epoch: 476,  train_loss=0.000162333,  val_loss=21.959533691,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 21:46:17,258 > Epoch: 477,  train_loss=0.000157527,  val_loss=22.097114563,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 21:47:25,138 > Epoch: 478,  train_loss=0.000139032,  val_loss=22.293766022,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 21:48:33,052 > Epoch: 479,  train_loss=0.000241408,  val_loss=22.510147095,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 21:49:40,969 > Epoch: 480,  train_loss=0.000207226,  val_loss=22.901365280,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 21:50:48,802 > Epoch: 481,  train_loss=0.000194870,  val_loss=23.195064545,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 21:51:56,614 > Epoch: 482,  train_loss=0.000188201,  val_loss=23.144838333,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 21:53:04,429 > Epoch: 483,  train_loss=0.000183021,  val_loss=23.107254028,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 21:54:12,222 > Epoch: 484,  train_loss=0.000181128,  val_loss=23.411548615,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 21:55:20,065 > Epoch: 485,  train_loss=0.000175719,  val_loss=23.785186768,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 21:56:27,946 > Epoch: 486,  train_loss=0.000165627,  val_loss=24.314342499,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:57:35,819 > Epoch: 487,  train_loss=0.000150714,  val_loss=24.140979767,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 21:58:43,689 > Epoch: 488,  train_loss=0.000145462,  val_loss=24.096324921,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 21:59:51,502 > Epoch: 489,  train_loss=0.000146270,  val_loss=23.649375916,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 22:00:59,436 > Epoch: 490,  train_loss=0.000158396,  val_loss=23.549776077,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 22:02:07,280 > Epoch: 491,  train_loss=0.000147781,  val_loss=23.987249374,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 22:03:15,101 > Epoch: 492,  train_loss=0.000176705,  val_loss=24.004041672,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 22:04:22,953 > Epoch: 493,  train_loss=0.000161658,  val_loss=23.993768692,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 22:05:30,877 > Epoch: 494,  train_loss=0.000154295,  val_loss=23.573455811,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 22:06:38,769 > Epoch: 495,  train_loss=0.000148370,  val_loss=23.534145355,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-20 22:07:46,645 > Epoch: 496,  train_loss=0.000143738,  val_loss=22.902053833,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 22:08:54,474 > Epoch: 497,  train_loss=0.000142592,  val_loss=22.786540985,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 22:10:02,271 > Epoch: 498,  train_loss=0.000140496,  val_loss=22.552186966,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 22:11:10,090 > Epoch: 499,  train_loss=0.000137152,  val_loss=21.967651367,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 22:12:17,910 > Epoch: 500,  train_loss=0.000134576,  val_loss=21.937093735,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 22:13:25,749 > Epoch: 501,  train_loss=0.000134645,  val_loss=22.962192535,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-20 22:14:33,575 > Epoch: 502,  train_loss=0.000133166,  val_loss=22.422107697,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 22:15:41,473 > Epoch: 503,  train_loss=0.000132437,  val_loss=22.222019196,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 22:16:49,321 > Epoch: 504,  train_loss=0.000130585,  val_loss=22.086526871,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 22:17:57,205 > Epoch: 505,  train_loss=0.000127424,  val_loss=21.499397278,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 22:19:05,036 > Epoch: 506,  train_loss=0.000127228,  val_loss=20.700256348,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-20 22:20:12,885 > Epoch: 507,  train_loss=0.000127849,  val_loss=19.614467621,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 22:21:20,720 > Epoch: 508,  train_loss=0.000127812,  val_loss=20.047630310,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 22:22:28,631 > Epoch: 509,  train_loss=0.000129918,  val_loss=19.984897614,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 22:23:36,467 > Epoch: 510,  train_loss=0.000131432,  val_loss=18.713363647,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 22:24:44,346 > Epoch: 511,  train_loss=0.000123605,  val_loss=18.575866699,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 22:25:52,182 > Epoch: 512,  train_loss=0.000123765,  val_loss=18.056457520,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-20 22:27:00,047 > Epoch: 513,  train_loss=0.000123796,  val_loss=19.624641418,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-20 22:28:07,882 > Epoch: 514,  train_loss=0.000111271,  val_loss=17.300624847,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-20 22:29:15,706 > Epoch: 515,  train_loss=0.007696050,  val_loss=11.590547562,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 22:30:23,580 > Epoch: 516,  train_loss=0.005763159,  val_loss=13.908288956,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:31:31,406 > Epoch: 517,  train_loss=0.000225929,  val_loss=15.819350243,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:32:39,278 > Epoch: 518,  train_loss=0.000164271,  val_loss=15.775814056,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:33:47,144 > Epoch: 519,  train_loss=0.000139968,  val_loss=15.797029495,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:34:55,008 > Epoch: 520,  train_loss=0.000135413,  val_loss=15.790512085,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:36:02,793 > Epoch: 521,  train_loss=0.000132606,  val_loss=15.791669846,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:37:10,612 > Epoch: 522,  train_loss=0.000130509,  val_loss=15.853436470,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:38:18,453 > Epoch: 523,  train_loss=0.000129008,  val_loss=15.814451218,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:39:26,287 > Epoch: 524,  train_loss=0.000127138,  val_loss=15.831440926,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:40:34,155 > Epoch: 525,  train_loss=0.000125538,  val_loss=15.932469368,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:41:42,024 > Epoch: 526,  train_loss=0.000124417,  val_loss=15.860346794,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:42:49,889 > Epoch: 527,  train_loss=0.000121416,  val_loss=15.971720695,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:43:57,726 > Epoch: 528,  train_loss=0.000120708,  val_loss=15.883492470,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:45:05,532 > Epoch: 529,  train_loss=0.000117739,  val_loss=15.820001602,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 22:46:13,335 > Epoch: 530,  train_loss=0.000115752,  val_loss=15.766634941,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:47:21,142 > Epoch: 531,  train_loss=0.000126867,  val_loss=15.596279144,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:48:28,959 > Epoch: 532,  train_loss=0.000123729,  val_loss=15.692649841,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:49:36,837 > Epoch: 533,  train_loss=0.000115657,  val_loss=15.596275330,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:50:44,739 > Epoch: 534,  train_loss=0.000112379,  val_loss=15.449897766,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:51:52,575 > Epoch: 535,  train_loss=0.000111870,  val_loss=15.389353752,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:53:00,477 > Epoch: 536,  train_loss=0.000111720,  val_loss=15.302548409,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:54:08,277 > Epoch: 537,  train_loss=0.000110627,  val_loss=15.287405014,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:55:16,124 > Epoch: 538,  train_loss=0.000114362,  val_loss=14.850271225,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:56:23,941 > Epoch: 539,  train_loss=0.000109436,  val_loss=14.648671150,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:57:31,766 > Epoch: 540,  train_loss=0.000109448,  val_loss=14.640057564,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:58:39,606 > Epoch: 541,  train_loss=0.000108250,  val_loss=14.507075310,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 22:59:47,462 > Epoch: 542,  train_loss=0.000105634,  val_loss=14.401308060,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:00:55,340 > Epoch: 543,  train_loss=0.000100642,  val_loss=14.413175583,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:02:03,218 > Epoch: 544,  train_loss=0.000125398,  val_loss=14.441329002,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:03:11,042 > Epoch: 545,  train_loss=0.000106236,  val_loss=14.333503723,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:04:18,874 > Epoch: 546,  train_loss=0.000105537,  val_loss=13.981376648,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:05:26,710 > Epoch: 547,  train_loss=0.000103357,  val_loss=14.360951424,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:06:34,521 > Epoch: 548,  train_loss=0.000106741,  val_loss=13.891639709,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 23:07:42,361 > Epoch: 549,  train_loss=0.000103843,  val_loss=14.577583313,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:08:50,228 > Epoch: 550,  train_loss=0.000106657,  val_loss=13.567512512,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:09:58,145 > Epoch: 551,  train_loss=0.000106103,  val_loss=13.834484100,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:11:06,097 > Epoch: 552,  train_loss=0.000101881,  val_loss=13.402745247,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:12:14,009 > Epoch: 553,  train_loss=0.000101390,  val_loss=13.066407204,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:13:21,850 > Epoch: 554,  train_loss=0.000103460,  val_loss=12.969097137,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:14:29,730 > Epoch: 555,  train_loss=0.000100203,  val_loss=12.855172157,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:15:37,610 > Epoch: 556,  train_loss=0.000101341,  val_loss=11.980333328,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 23:16:45,491 > Epoch: 557,  train_loss=0.000102075,  val_loss=13.385597229,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 23:17:53,344 > Epoch: 558,  train_loss=0.000156081,  val_loss=9.590736389,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:19:01,198 > Epoch: 559,  train_loss=0.000092305,  val_loss=17.424505234,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:20:09,078 > Epoch: 560,  train_loss=0.000618699,  val_loss=12.635817528,  accuracy=0.851562500
[INFO|asb_main.py:613] 2018-10-20 23:21:17,030 > Epoch: 561,  train_loss=0.000357809,  val_loss=9.083250046,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:22:24,909 > Epoch: 562,  train_loss=0.000096044,  val_loss=8.916570663,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:23:32,750 > Epoch: 563,  train_loss=0.000096489,  val_loss=8.913829803,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:24:40,599 > Epoch: 564,  train_loss=0.000096902,  val_loss=8.918958664,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:25:48,460 > Epoch: 565,  train_loss=0.000097172,  val_loss=8.903944969,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:26:56,324 > Epoch: 566,  train_loss=0.000097186,  val_loss=8.871211052,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:28:04,126 > Epoch: 567,  train_loss=0.000096945,  val_loss=8.888597488,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:29:11,993 > Epoch: 568,  train_loss=0.000097153,  val_loss=8.907396317,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:30:19,904 > Epoch: 569,  train_loss=0.000097241,  val_loss=8.918875694,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:31:27,768 > Epoch: 570,  train_loss=0.000097328,  val_loss=8.871501923,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-20 23:32:35,689 > Epoch: 571,  train_loss=0.000097316,  val_loss=8.927426338,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 23:33:43,510 > Epoch: 572,  train_loss=0.000097365,  val_loss=8.917779922,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 23:34:51,331 > Epoch: 573,  train_loss=0.000097364,  val_loss=8.894861221,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 23:35:59,224 > Epoch: 574,  train_loss=0.000097430,  val_loss=8.985620499,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 23:37:07,039 > Epoch: 575,  train_loss=0.000097393,  val_loss=8.936342239,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 23:38:14,956 > Epoch: 576,  train_loss=0.000097441,  val_loss=8.887430191,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-20 23:39:22,834 > Epoch: 577,  train_loss=0.000096951,  val_loss=9.025015831,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-20 23:40:30,734 > Epoch: 578,  train_loss=0.000282643,  val_loss=6.729187965,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 23:41:38,608 > Epoch: 579,  train_loss=0.000238265,  val_loss=3.787751198,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:42:46,418 > Epoch: 580,  train_loss=0.000104694,  val_loss=3.876816750,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:43:54,285 > Epoch: 581,  train_loss=0.000108343,  val_loss=3.602886677,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:45:02,091 > Epoch: 582,  train_loss=0.000152396,  val_loss=4.383648872,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:46:09,905 > Epoch: 583,  train_loss=0.000118880,  val_loss=3.993870020,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-20 23:47:17,822 > Epoch: 584,  train_loss=0.000110203,  val_loss=3.579285860,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 23:48:25,670 > Epoch: 585,  train_loss=0.000108927,  val_loss=3.558814526,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:49:33,565 > Epoch: 586,  train_loss=0.000105768,  val_loss=3.537684917,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:50:41,485 > Epoch: 587,  train_loss=0.000099055,  val_loss=3.804071665,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-20 23:51:49,333 > Epoch: 588,  train_loss=0.000118601,  val_loss=3.066896677,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:52:57,216 > Epoch: 589,  train_loss=0.000107911,  val_loss=3.184868813,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 23:54:05,051 > Epoch: 590,  train_loss=0.000124839,  val_loss=3.156499386,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-20 23:55:12,898 > Epoch: 591,  train_loss=0.000163165,  val_loss=3.752119064,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 23:56:20,832 > Epoch: 592,  train_loss=0.000101160,  val_loss=1.913949370,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-20 23:57:28,709 > Epoch: 593,  train_loss=0.000103123,  val_loss=1.634331942,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-20 23:58:36,543 > Epoch: 594,  train_loss=0.000129330,  val_loss=2.361220360,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-20 23:59:44,434 > Epoch: 595,  train_loss=0.000109096,  val_loss=1.775503874,  accuracy=0.859375000
[INFO|asb_main.py:613] 2018-10-21 00:00:52,284 > Epoch: 596,  train_loss=0.011494066,  val_loss=33.940589905,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-21 00:02:00,079 > Epoch: 597,  train_loss=0.002760130,  val_loss=29.402648926,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-21 00:03:07,874 > Epoch: 598,  train_loss=0.000309736,  val_loss=32.881500244,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-21 00:04:15,679 > Epoch: 599,  train_loss=0.000227631,  val_loss=32.801239014,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-21 00:05:23,561 > Epoch: 600,  train_loss=0.000123793,  val_loss=33.085292816,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-21 00:06:31,449 > Epoch: 601,  train_loss=0.000115894,  val_loss=33.176246643,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-21 00:07:39,280 > Epoch: 602,  train_loss=0.000112431,  val_loss=33.044082642,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:08:47,172 > Epoch: 603,  train_loss=0.000110326,  val_loss=33.128509521,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:09:54,973 > Epoch: 604,  train_loss=0.000109015,  val_loss=33.224891663,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:11:02,787 > Epoch: 605,  train_loss=0.000107737,  val_loss=33.296371460,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:12:10,598 > Epoch: 606,  train_loss=0.000107028,  val_loss=33.396976471,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:13:18,404 > Epoch: 607,  train_loss=0.000106003,  val_loss=33.449729919,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:14:26,295 > Epoch: 608,  train_loss=0.000105577,  val_loss=32.891242981,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:15:34,183 > Epoch: 609,  train_loss=0.000104587,  val_loss=32.961380005,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:16:42,064 > Epoch: 610,  train_loss=0.000104154,  val_loss=33.046680450,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:17:49,897 > Epoch: 611,  train_loss=0.000103366,  val_loss=33.102081299,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:18:57,718 > Epoch: 612,  train_loss=0.000102714,  val_loss=33.240280151,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:20:05,561 > Epoch: 613,  train_loss=0.000103525,  val_loss=33.385807037,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:21:13,372 > Epoch: 614,  train_loss=0.000101486,  val_loss=33.365135193,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 00:22:21,206 > Epoch: 615,  train_loss=0.000101255,  val_loss=33.670661926,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-21 00:23:29,071 > Epoch: 616,  train_loss=0.000109339,  val_loss=34.180755615,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:24:36,925 > Epoch: 617,  train_loss=0.000101060,  val_loss=34.478797913,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:25:44,783 > Epoch: 618,  train_loss=0.000099232,  val_loss=34.473190308,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:26:52,614 > Epoch: 619,  train_loss=0.000101445,  val_loss=35.017856598,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:28:00,474 > Epoch: 620,  train_loss=0.000098447,  val_loss=35.080612183,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:29:08,312 > Epoch: 621,  train_loss=0.000098879,  val_loss=35.338623047,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:30:16,190 > Epoch: 622,  train_loss=0.000102466,  val_loss=35.325344086,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:31:23,983 > Epoch: 623,  train_loss=0.000097850,  val_loss=35.395439148,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:32:31,843 > Epoch: 624,  train_loss=0.000100104,  val_loss=36.233306885,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:33:39,744 > Epoch: 625,  train_loss=0.000097326,  val_loss=36.343795776,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:34:47,607 > Epoch: 626,  train_loss=0.000099260,  val_loss=37.261878967,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:35:55,467 > Epoch: 627,  train_loss=0.000099118,  val_loss=37.387748718,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:37:03,290 > Epoch: 628,  train_loss=0.000097001,  val_loss=37.595405579,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:38:11,117 > Epoch: 629,  train_loss=0.000097823,  val_loss=37.281227112,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:39:18,930 > Epoch: 630,  train_loss=0.000098168,  val_loss=37.146987915,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:40:26,781 > Epoch: 631,  train_loss=0.000098685,  val_loss=37.629295349,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-21 00:41:34,657 > Epoch: 632,  train_loss=0.000096306,  val_loss=37.597106934,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:42:42,577 > Epoch: 633,  train_loss=0.000099218,  val_loss=37.991111755,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-21 00:43:50,423 > Epoch: 634,  train_loss=0.000096151,  val_loss=38.717292786,  accuracy=0.570312500
[INFO|asb_main.py:613] 2018-10-21 00:44:58,289 > Epoch: 635,  train_loss=0.000097883,  val_loss=35.733230591,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-21 00:46:06,135 > Epoch: 636,  train_loss=0.000149728,  val_loss=33.589145660,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-21 00:47:13,945 > Epoch: 637,  train_loss=0.000107307,  val_loss=33.796394348,  accuracy=0.554687500
[INFO|asb_main.py:613] 2018-10-21 00:48:21,784 > Epoch: 638,  train_loss=0.000094881,  val_loss=33.134853363,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-21 00:49:29,601 > Epoch: 639,  train_loss=0.000103027,  val_loss=32.298751831,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-21 00:50:37,477 > Epoch: 640,  train_loss=0.000097257,  val_loss=32.055572510,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-21 00:51:45,350 > Epoch: 641,  train_loss=0.000094926,  val_loss=32.078151703,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-21 00:52:53,218 > Epoch: 642,  train_loss=0.000101189,  val_loss=30.308498383,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-21 00:54:01,044 > Epoch: 643,  train_loss=0.000094714,  val_loss=30.899436951,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-21 00:55:08,907 > Epoch: 644,  train_loss=0.000093915,  val_loss=52.452091217,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 00:56:16,706 > Epoch: 645,  train_loss=0.028853143,  val_loss=48.636196136,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 00:57:24,538 > Epoch: 646,  train_loss=0.000385120,  val_loss=48.548339844,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 00:58:32,356 > Epoch: 647,  train_loss=0.000258627,  val_loss=48.062927246,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 00:59:40,186 > Epoch: 648,  train_loss=0.000159225,  val_loss=47.712394714,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:00:48,025 > Epoch: 649,  train_loss=0.000163678,  val_loss=47.430038452,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:01:55,896 > Epoch: 650,  train_loss=0.000153609,  val_loss=47.271190643,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:03:03,763 > Epoch: 651,  train_loss=0.000144436,  val_loss=47.020278931,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:04:11,588 > Epoch: 652,  train_loss=0.000139508,  val_loss=46.922996521,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:05:19,430 > Epoch: 653,  train_loss=0.000135265,  val_loss=46.913581848,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:06:27,274 > Epoch: 654,  train_loss=0.000131891,  val_loss=46.589797974,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:07:35,132 > Epoch: 655,  train_loss=0.000129376,  val_loss=46.220134735,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:08:43,004 > Epoch: 656,  train_loss=0.000128230,  val_loss=45.826797485,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:09:50,883 > Epoch: 657,  train_loss=0.000126781,  val_loss=45.376312256,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:10:58,749 > Epoch: 658,  train_loss=0.000124859,  val_loss=44.489936829,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:12:06,573 > Epoch: 659,  train_loss=0.000123172,  val_loss=44.909614563,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:13:14,397 > Epoch: 660,  train_loss=0.000123054,  val_loss=44.307888031,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:14:22,217 > Epoch: 661,  train_loss=0.000121740,  val_loss=44.201309204,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:15:30,034 > Epoch: 662,  train_loss=0.000120096,  val_loss=43.983295441,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:16:37,852 > Epoch: 663,  train_loss=0.000131720,  val_loss=43.913185120,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:17:45,724 > Epoch: 664,  train_loss=0.000120818,  val_loss=43.560508728,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:18:53,590 > Epoch: 665,  train_loss=0.000128823,  val_loss=43.230171204,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:20:01,444 > Epoch: 666,  train_loss=0.000120729,  val_loss=42.664505005,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:21:09,292 > Epoch: 667,  train_loss=0.000130226,  val_loss=42.542671204,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:22:17,146 > Epoch: 668,  train_loss=0.000117220,  val_loss=41.599250793,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:23:24,988 > Epoch: 669,  train_loss=0.000126329,  val_loss=43.231391907,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:24:32,848 > Epoch: 670,  train_loss=0.000116710,  val_loss=41.253097534,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:25:40,657 > Epoch: 671,  train_loss=0.000119788,  val_loss=40.527374268,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:26:48,511 > Epoch: 672,  train_loss=0.000115657,  val_loss=40.378597260,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:27:56,350 > Epoch: 673,  train_loss=0.000116469,  val_loss=37.990524292,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:29:04,239 > Epoch: 674,  train_loss=0.000107259,  val_loss=37.362018585,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 01:30:12,112 > Epoch: 675,  train_loss=0.000119006,  val_loss=35.999984741,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:31:19,924 > Epoch: 676,  train_loss=0.000110879,  val_loss=35.061214447,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:32:27,723 > Epoch: 677,  train_loss=0.000133301,  val_loss=33.024566650,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:33:35,547 > Epoch: 678,  train_loss=0.000110596,  val_loss=32.420639038,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:34:43,373 > Epoch: 679,  train_loss=0.000113122,  val_loss=31.130760193,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:35:51,260 > Epoch: 680,  train_loss=0.000105368,  val_loss=30.662376404,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:36:59,163 > Epoch: 681,  train_loss=0.000107122,  val_loss=30.024761200,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:38:07,069 > Epoch: 682,  train_loss=0.000109374,  val_loss=29.664745331,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 01:39:14,929 > Epoch: 683,  train_loss=0.000103083,  val_loss=28.478366852,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 01:40:22,786 > Epoch: 684,  train_loss=0.000108956,  val_loss=28.354822159,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 01:41:30,611 > Epoch: 685,  train_loss=0.000102914,  val_loss=27.397533417,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 01:42:38,446 > Epoch: 686,  train_loss=0.000102508,  val_loss=26.778816223,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 01:43:46,303 > Epoch: 687,  train_loss=0.000100707,  val_loss=26.087903976,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:44:54,172 > Epoch: 688,  train_loss=0.000107671,  val_loss=26.112659454,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-21 01:46:02,055 > Epoch: 689,  train_loss=0.000099298,  val_loss=24.986215591,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:47:09,924 > Epoch: 690,  train_loss=0.000104504,  val_loss=25.761573792,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:48:17,836 > Epoch: 691,  train_loss=0.000099883,  val_loss=25.904264450,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 01:49:25,661 > Epoch: 692,  train_loss=0.000099825,  val_loss=24.287649155,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 01:50:33,523 > Epoch: 693,  train_loss=0.000094571,  val_loss=23.153812408,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:51:41,375 > Epoch: 694,  train_loss=0.000149354,  val_loss=26.815202713,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-21 01:52:49,187 > Epoch: 695,  train_loss=0.000140326,  val_loss=25.273202896,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 01:53:57,034 > Epoch: 696,  train_loss=0.002256482,  val_loss=40.013793945,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-21 01:55:04,911 > Epoch: 697,  train_loss=0.001187211,  val_loss=17.280363083,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-21 01:56:12,824 > Epoch: 698,  train_loss=0.000149750,  val_loss=17.503067017,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 01:57:20,680 > Epoch: 699,  train_loss=0.000103076,  val_loss=16.901996613,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 01:58:28,551 > Epoch: 700,  train_loss=0.000113543,  val_loss=16.703664780,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 01:59:36,336 > Epoch: 701,  train_loss=0.000103314,  val_loss=16.072296143,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 02:00:44,173 > Epoch: 702,  train_loss=0.000117110,  val_loss=16.023807526,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 02:01:52,021 > Epoch: 703,  train_loss=0.000105056,  val_loss=15.589627266,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 02:02:59,897 > Epoch: 704,  train_loss=0.000103800,  val_loss=15.194609642,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 02:04:07,778 > Epoch: 705,  train_loss=0.000103041,  val_loss=14.826141357,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 02:05:15,640 > Epoch: 706,  train_loss=0.000102410,  val_loss=14.433575630,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 02:06:23,529 > Epoch: 707,  train_loss=0.000102287,  val_loss=14.092890739,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-21 02:07:31,355 > Epoch: 708,  train_loss=0.000101625,  val_loss=13.733690262,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-21 02:08:39,183 > Epoch: 709,  train_loss=0.000099117,  val_loss=13.408018112,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-21 02:09:47,007 > Epoch: 710,  train_loss=0.000116851,  val_loss=13.676799774,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 02:10:54,841 > Epoch: 711,  train_loss=0.000100676,  val_loss=13.485912323,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 02:12:02,691 > Epoch: 712,  train_loss=0.000101031,  val_loss=13.487498283,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 02:13:10,536 > Epoch: 713,  train_loss=0.000099161,  val_loss=13.256194115,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-21 02:14:18,410 > Epoch: 714,  train_loss=0.000101752,  val_loss=13.182296753,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-21 02:15:26,314 > Epoch: 715,  train_loss=0.000098676,  val_loss=13.106599808,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 02:16:34,125 > Epoch: 716,  train_loss=0.000100271,  val_loss=12.970786095,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 02:17:41,980 > Epoch: 717,  train_loss=0.000097566,  val_loss=12.805479050,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 02:18:49,820 > Epoch: 718,  train_loss=0.000099949,  val_loss=12.605768204,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 02:19:57,638 > Epoch: 719,  train_loss=0.000095960,  val_loss=12.712383270,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 02:21:05,540 > Epoch: 720,  train_loss=0.000099271,  val_loss=12.521262169,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 02:22:13,426 > Epoch: 721,  train_loss=0.000091749,  val_loss=12.068733215,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 02:23:21,267 > Epoch: 722,  train_loss=0.000086255,  val_loss=11.151328087,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-21 02:24:29,140 > Epoch: 723,  train_loss=0.000138232,  val_loss=12.704066277,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 02:25:36,964 > Epoch: 724,  train_loss=0.000084064,  val_loss=11.771323204,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 02:26:44,825 > Epoch: 725,  train_loss=0.000109938,  val_loss=12.029178619,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 02:27:52,655 > Epoch: 726,  train_loss=0.000090525,  val_loss=11.302440643,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 02:29:00,483 > Epoch: 727,  train_loss=0.000097261,  val_loss=10.453020096,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-21 02:30:08,361 > Epoch: 728,  train_loss=0.000091288,  val_loss=9.576692581,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 02:31:16,212 > Epoch: 729,  train_loss=0.000122579,  val_loss=10.187973022,  accuracy=0.656250000
[INFO|asb_main.py:613] 2018-10-21 02:32:24,096 > Epoch: 730,  train_loss=0.000087879,  val_loss=8.995570183,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-21 02:33:31,985 > Epoch: 731,  train_loss=0.000088496,  val_loss=8.963296890,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-21 02:34:39,829 > Epoch: 732,  train_loss=0.000086682,  val_loss=8.277302742,  accuracy=0.718750000
[INFO|asb_main.py:613] 2018-10-21 02:35:47,664 > Epoch: 733,  train_loss=0.000087325,  val_loss=13.413521767,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-21 02:36:55,505 > Epoch: 734,  train_loss=0.005761948,  val_loss=48.848411560,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 02:38:03,304 > Epoch: 735,  train_loss=0.000163358,  val_loss=45.779472351,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-21 02:39:11,167 > Epoch: 736,  train_loss=0.000138138,  val_loss=45.600318909,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-21 02:40:19,039 > Epoch: 737,  train_loss=0.000128261,  val_loss=46.784694672,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 02:41:26,898 > Epoch: 738,  train_loss=0.000122436,  val_loss=47.250377655,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 02:42:34,738 > Epoch: 739,  train_loss=0.000121515,  val_loss=46.344165802,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 02:43:42,541 > Epoch: 740,  train_loss=0.000121399,  val_loss=45.107391357,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-21 02:44:50,378 > Epoch: 741,  train_loss=0.000120811,  val_loss=43.042942047,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-21 02:45:58,227 > Epoch: 742,  train_loss=0.000119779,  val_loss=42.243724823,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-21 02:47:06,051 > Epoch: 743,  train_loss=0.000118789,  val_loss=41.533470154,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-21 02:48:13,921 > Epoch: 744,  train_loss=0.000117915,  val_loss=40.871025085,  accuracy=0.804687500
[INFO|asb_main.py:613] 2018-10-21 02:49:21,779 > Epoch: 745,  train_loss=0.000117039,  val_loss=40.193290710,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-21 02:50:29,635 > Epoch: 746,  train_loss=0.000116320,  val_loss=39.516254425,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-21 02:51:37,485 > Epoch: 747,  train_loss=0.000115736,  val_loss=38.828887939,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-21 02:52:45,296 > Epoch: 748,  train_loss=0.000115085,  val_loss=38.171234131,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-21 02:53:53,112 > Epoch: 749,  train_loss=0.000114564,  val_loss=37.502204895,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-21 02:55:00,923 > Epoch: 750,  train_loss=0.000114190,  val_loss=37.081619263,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-21 02:56:08,734 > Epoch: 751,  train_loss=0.000113614,  val_loss=36.516407013,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-21 02:57:16,587 > Epoch: 752,  train_loss=0.000113182,  val_loss=35.893386841,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-21 02:58:24,453 > Epoch: 753,  train_loss=0.000112956,  val_loss=35.544044495,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-21 02:59:32,317 > Epoch: 754,  train_loss=0.000112355,  val_loss=34.867343903,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-21 03:00:40,150 > Epoch: 755,  train_loss=0.000111968,  val_loss=34.091533661,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-21 03:01:47,972 > Epoch: 756,  train_loss=0.000111944,  val_loss=33.777580261,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-21 03:02:55,777 > Epoch: 757,  train_loss=0.000111141,  val_loss=33.219131470,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-21 03:04:03,628 > Epoch: 758,  train_loss=0.000110978,  val_loss=32.614242554,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-21 03:05:11,438 > Epoch: 759,  train_loss=0.000110562,  val_loss=31.869310379,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-21 03:06:19,283 > Epoch: 760,  train_loss=0.000110068,  val_loss=31.188253403,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-21 03:07:27,152 > Epoch: 761,  train_loss=0.000110119,  val_loss=30.902172089,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-21 03:08:35,010 > Epoch: 762,  train_loss=0.000109276,  val_loss=30.336080551,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-21 03:09:42,890 > Epoch: 763,  train_loss=0.000108998,  val_loss=29.731111526,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-21 03:10:50,705 > Epoch: 764,  train_loss=0.000108942,  val_loss=29.505662918,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-21 03:11:58,543 > Epoch: 765,  train_loss=0.000108280,  val_loss=28.926717758,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-21 03:13:06,368 > Epoch: 766,  train_loss=0.000107882,  val_loss=28.203748703,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-21 03:14:14,199 > Epoch: 767,  train_loss=0.000108065,  val_loss=27.421426773,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-21 03:15:22,047 > Epoch: 768,  train_loss=0.000107657,  val_loss=26.700199127,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-21 03:16:29,892 > Epoch: 769,  train_loss=0.000106065,  val_loss=25.760290146,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-21 03:17:37,763 > Epoch: 770,  train_loss=0.000108325,  val_loss=24.725948334,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-21 03:18:45,642 > Epoch: 771,  train_loss=0.000105893,  val_loss=23.806745529,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-21 03:19:53,482 > Epoch: 772,  train_loss=0.000105132,  val_loss=22.970920563,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-21 03:21:01,304 > Epoch: 773,  train_loss=0.000105067,  val_loss=21.229557037,  accuracy=0.843750000
[INFO|asb_main.py:613] 2018-10-21 03:22:09,132 > Epoch: 774,  train_loss=0.000107073,  val_loss=20.064441681,  accuracy=0.835937500
[INFO|asb_main.py:613] 2018-10-21 03:23:16,963 > Epoch: 775,  train_loss=0.000103553,  val_loss=19.718194962,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-21 03:24:24,828 > Epoch: 776,  train_loss=0.000106263,  val_loss=19.018203735,  accuracy=0.820312500
[INFO|asb_main.py:613] 2018-10-21 03:25:32,683 > Epoch: 777,  train_loss=0.000108896,  val_loss=18.829849243,  accuracy=0.828125000
[INFO|asb_main.py:613] 2018-10-21 03:26:40,553 > Epoch: 778,  train_loss=0.000127234,  val_loss=16.175457001,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-21 03:27:48,467 > Epoch: 779,  train_loss=0.000098600,  val_loss=12.677240372,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-21 03:28:56,325 > Epoch: 780,  train_loss=0.000177126,  val_loss=12.372035027,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 03:30:04,146 > Epoch: 781,  train_loss=0.000092148,  val_loss=10.570128441,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-21 03:31:11,944 > Epoch: 782,  train_loss=0.000093476,  val_loss=10.329475403,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 03:32:19,763 > Epoch: 783,  train_loss=0.000121980,  val_loss=10.854948997,  accuracy=0.703125000
[INFO|asb_main.py:613] 2018-10-21 03:33:27,715 > Epoch: 784,  train_loss=0.000094579,  val_loss=10.665272713,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-21 03:34:35,568 > Epoch: 785,  train_loss=0.000191709,  val_loss=11.854516029,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-21 03:35:43,444 > Epoch: 786,  train_loss=0.000183163,  val_loss=3.778761864,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 03:36:51,303 > Epoch: 787,  train_loss=0.000211370,  val_loss=7.848763943,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 03:37:59,136 > Epoch: 788,  train_loss=0.000106281,  val_loss=7.268334866,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 03:39:07,011 > Epoch: 789,  train_loss=0.000130879,  val_loss=5.755315781,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-21 03:40:14,819 > Epoch: 790,  train_loss=0.000143741,  val_loss=4.927773476,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-21 03:41:22,641 > Epoch: 791,  train_loss=0.008867453,  val_loss=39.440940857,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 03:42:30,510 > Epoch: 792,  train_loss=0.004894255,  val_loss=28.161787033,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-21 03:43:38,348 > Epoch: 793,  train_loss=0.000319889,  val_loss=33.126609802,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-21 03:44:46,210 > Epoch: 794,  train_loss=0.000326625,  val_loss=29.626117706,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-21 03:45:54,078 > Epoch: 795,  train_loss=0.000209096,  val_loss=27.233215332,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 03:47:01,884 > Epoch: 796,  train_loss=0.000173243,  val_loss=26.848455429,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 03:48:09,693 > Epoch: 797,  train_loss=0.000164507,  val_loss=26.623331070,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 03:49:17,508 > Epoch: 798,  train_loss=0.000158843,  val_loss=26.567304611,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 03:50:25,341 > Epoch: 799,  train_loss=0.000168704,  val_loss=26.714849472,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 03:51:33,189 > Epoch: 800,  train_loss=0.000158927,  val_loss=25.407327652,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 03:52:41,056 > Epoch: 801,  train_loss=0.000155582,  val_loss=25.538942337,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 03:53:48,931 > Epoch: 802,  train_loss=0.000148686,  val_loss=25.839218140,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 03:54:56,812 > Epoch: 803,  train_loss=0.000149251,  val_loss=26.470127106,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 03:56:04,628 > Epoch: 804,  train_loss=0.000146511,  val_loss=26.932592392,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 03:57:12,435 > Epoch: 805,  train_loss=0.000143916,  val_loss=26.458980560,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 03:58:20,267 > Epoch: 806,  train_loss=0.000138382,  val_loss=26.979867935,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 03:59:28,081 > Epoch: 807,  train_loss=0.000135276,  val_loss=26.265857697,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 04:00:35,946 > Epoch: 808,  train_loss=0.000131679,  val_loss=26.436447144,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 04:01:43,810 > Epoch: 809,  train_loss=0.000125694,  val_loss=27.139091492,  accuracy=0.789062500
[INFO|asb_main.py:613] 2018-10-21 04:02:51,678 > Epoch: 810,  train_loss=0.000102396,  val_loss=26.601549149,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 04:03:59,587 > Epoch: 811,  train_loss=0.000201966,  val_loss=25.866746902,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 04:05:07,419 > Epoch: 812,  train_loss=0.000136652,  val_loss=26.329851151,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 04:06:15,262 > Epoch: 813,  train_loss=0.000113157,  val_loss=26.142824173,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 04:07:23,080 > Epoch: 814,  train_loss=0.000175434,  val_loss=27.191343307,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 04:08:30,903 > Epoch: 815,  train_loss=0.000179687,  val_loss=29.063964844,  accuracy=0.781250000
[INFO|asb_main.py:613] 2018-10-21 04:09:38,796 > Epoch: 816,  train_loss=0.000113773,  val_loss=21.745124817,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 04:10:46,689 > Epoch: 817,  train_loss=0.000117802,  val_loss=20.896717072,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 04:11:54,547 > Epoch: 818,  train_loss=0.000098512,  val_loss=21.771999359,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-21 04:13:02,383 > Epoch: 819,  train_loss=0.000112411,  val_loss=19.968147278,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-21 04:14:10,237 > Epoch: 820,  train_loss=0.000103175,  val_loss=21.587545395,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-21 04:15:18,082 > Epoch: 821,  train_loss=0.000103204,  val_loss=19.632905960,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-21 04:16:25,911 > Epoch: 822,  train_loss=0.000089236,  val_loss=21.120845795,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-21 04:17:33,760 > Epoch: 823,  train_loss=0.000102553,  val_loss=20.931003571,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 04:18:41,651 > Epoch: 824,  train_loss=0.000103090,  val_loss=20.191837311,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 04:19:49,525 > Epoch: 825,  train_loss=0.000092398,  val_loss=18.931535721,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 04:20:57,403 > Epoch: 826,  train_loss=0.000084404,  val_loss=21.575908661,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 04:22:05,290 > Epoch: 827,  train_loss=0.002342410,  val_loss=32.750320435,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-21 04:23:13,124 > Epoch: 828,  train_loss=0.009002164,  val_loss=28.221624374,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-21 04:24:20,953 > Epoch: 829,  train_loss=0.000137716,  val_loss=26.570716858,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:25:28,776 > Epoch: 830,  train_loss=0.000277986,  val_loss=27.315250397,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:26:36,601 > Epoch: 831,  train_loss=0.000135071,  val_loss=27.281044006,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:27:44,480 > Epoch: 832,  train_loss=0.000153133,  val_loss=27.586612701,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:28:52,362 > Epoch: 833,  train_loss=0.000134537,  val_loss=27.575576782,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:30:00,231 > Epoch: 834,  train_loss=0.000126459,  val_loss=27.501407623,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:31:08,101 > Epoch: 835,  train_loss=0.000126442,  val_loss=27.474697113,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:32:15,932 > Epoch: 836,  train_loss=0.000122654,  val_loss=27.452236176,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:33:23,758 > Epoch: 837,  train_loss=0.000119819,  val_loss=27.383684158,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:34:31,570 > Epoch: 838,  train_loss=0.000117992,  val_loss=27.338958740,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:35:39,410 > Epoch: 839,  train_loss=0.000116776,  val_loss=27.284355164,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:36:47,257 > Epoch: 840,  train_loss=0.000115616,  val_loss=27.211219788,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:37:55,136 > Epoch: 841,  train_loss=0.000113592,  val_loss=27.132766724,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:39:03,021 > Epoch: 842,  train_loss=0.000113279,  val_loss=27.082088470,  accuracy=0.695312500
[INFO|asb_main.py:613] 2018-10-21 04:40:10,878 > Epoch: 843,  train_loss=0.000111848,  val_loss=27.003282547,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 04:41:18,727 > Epoch: 844,  train_loss=0.000111082,  val_loss=26.822620392,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 04:42:26,548 > Epoch: 845,  train_loss=0.000110838,  val_loss=26.728546143,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 04:43:34,350 > Epoch: 846,  train_loss=0.000109136,  val_loss=26.584909439,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 04:44:42,159 > Epoch: 847,  train_loss=0.000108623,  val_loss=26.382930756,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-21 04:45:50,023 > Epoch: 848,  train_loss=0.000107080,  val_loss=26.032461166,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-21 04:46:57,874 > Epoch: 849,  train_loss=0.000110039,  val_loss=26.075016022,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-21 04:48:05,770 > Epoch: 850,  train_loss=0.000103621,  val_loss=26.095493317,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-21 04:49:13,628 > Epoch: 851,  train_loss=0.000101886,  val_loss=25.793708801,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-21 04:50:21,445 > Epoch: 852,  train_loss=0.000102758,  val_loss=25.599906921,  accuracy=0.679687500
[INFO|asb_main.py:613] 2018-10-21 04:51:29,281 > Epoch: 853,  train_loss=0.000106225,  val_loss=25.220556259,  accuracy=0.671875000
[INFO|asb_main.py:613] 2018-10-21 04:52:37,123 > Epoch: 854,  train_loss=0.000101495,  val_loss=24.794712067,  accuracy=0.664062500
[INFO|asb_main.py:613] 2018-10-21 04:53:44,947 > Epoch: 855,  train_loss=0.000097866,  val_loss=24.637748718,  accuracy=0.656250000
[INFO|asb_main.py:613] 2018-10-21 04:54:52,829 > Epoch: 856,  train_loss=0.000100702,  val_loss=24.938980103,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 04:56:00,725 > Epoch: 857,  train_loss=0.000094780,  val_loss=24.006645203,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 04:57:08,575 > Epoch: 858,  train_loss=0.000108684,  val_loss=23.688320160,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 04:58:16,451 > Epoch: 859,  train_loss=0.000091683,  val_loss=23.247730255,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 04:59:24,248 > Epoch: 860,  train_loss=0.000093151,  val_loss=24.042152405,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 05:00:32,066 > Epoch: 861,  train_loss=0.000088527,  val_loss=22.904148102,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 05:01:39,882 > Epoch: 862,  train_loss=0.000088399,  val_loss=23.082466125,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 05:02:47,728 > Epoch: 863,  train_loss=0.000097267,  val_loss=20.332088470,  accuracy=0.539062500
[INFO|asb_main.py:613] 2018-10-21 05:03:55,676 > Epoch: 864,  train_loss=0.000169455,  val_loss=20.915786743,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-21 05:05:03,593 > Epoch: 865,  train_loss=0.000081636,  val_loss=17.261732101,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-21 05:06:11,517 > Epoch: 866,  train_loss=0.000096270,  val_loss=15.797871590,  accuracy=0.515625000
[INFO|asb_main.py:613] 2018-10-21 05:07:19,484 > Epoch: 867,  train_loss=0.000111476,  val_loss=15.284566879,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 05:08:27,407 > Epoch: 868,  train_loss=0.000085560,  val_loss=18.506521225,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-21 05:09:35,286 > Epoch: 869,  train_loss=0.000091816,  val_loss=15.637266159,  accuracy=0.492187500
[INFO|asb_main.py:613] 2018-10-21 05:10:43,188 > Epoch: 870,  train_loss=0.000080540,  val_loss=15.525063515,  accuracy=0.476562500
[INFO|asb_main.py:613] 2018-10-21 05:11:51,091 > Epoch: 871,  train_loss=0.000080087,  val_loss=15.699580193,  accuracy=0.468750000
[INFO|asb_main.py:613] 2018-10-21 05:12:58,994 > Epoch: 872,  train_loss=0.000080072,  val_loss=17.478277206,  accuracy=0.562500000
[INFO|asb_main.py:613] 2018-10-21 05:14:06,924 > Epoch: 873,  train_loss=0.000092581,  val_loss=28.826618195,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 05:15:14,873 > Epoch: 874,  train_loss=0.000111455,  val_loss=163.406829834,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:16:22,819 > Epoch: 875,  train_loss=0.000102513,  val_loss=32.778942108,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 05:17:30,680 > Epoch: 876,  train_loss=0.000092690,  val_loss=21.840160370,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 05:18:38,570 > Epoch: 877,  train_loss=0.000075308,  val_loss=34.246219635,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 05:19:46,456 > Epoch: 878,  train_loss=0.000086825,  val_loss=29.327049255,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-21 05:20:54,388 > Epoch: 879,  train_loss=0.000068923,  val_loss=35.956939697,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 05:22:02,311 > Epoch: 880,  train_loss=0.000105379,  val_loss=26.545030594,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-21 05:23:10,264 > Epoch: 881,  train_loss=0.000073501,  val_loss=30.297279358,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-21 05:24:18,170 > Epoch: 882,  train_loss=0.000098725,  val_loss=22.050239563,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 05:25:26,147 > Epoch: 883,  train_loss=0.000081246,  val_loss=49.112770081,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-21 05:26:34,049 > Epoch: 884,  train_loss=0.000234546,  val_loss=17.847751617,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 05:27:41,956 > Epoch: 885,  train_loss=0.000215987,  val_loss=18.515962601,  accuracy=0.578125000
[INFO|asb_main.py:613] 2018-10-21 05:28:49,829 > Epoch: 886,  train_loss=0.000163724,  val_loss=16.515359879,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 05:29:57,760 > Epoch: 887,  train_loss=0.000084284,  val_loss=19.127126694,  accuracy=0.601562500
[INFO|asb_main.py:613] 2018-10-21 05:31:05,736 > Epoch: 888,  train_loss=0.000130289,  val_loss=16.717191696,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 05:32:13,719 > Epoch: 889,  train_loss=0.000087886,  val_loss=13.537962914,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-21 05:33:21,656 > Epoch: 890,  train_loss=0.000088798,  val_loss=17.751846313,  accuracy=0.593750000
[INFO|asb_main.py:613] 2018-10-21 05:34:29,618 > Epoch: 891,  train_loss=0.000082523,  val_loss=16.131423950,  accuracy=0.625000000
[INFO|asb_main.py:613] 2018-10-21 05:35:37,498 > Epoch: 892,  train_loss=0.001889275,  val_loss=41.781730652,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:36:45,402 > Epoch: 893,  train_loss=0.000997432,  val_loss=43.029701233,  accuracy=0.687500000
[INFO|asb_main.py:613] 2018-10-21 05:37:53,299 > Epoch: 894,  train_loss=0.029720463,  val_loss=29.182455063,  accuracy=0.640625000
[INFO|asb_main.py:613] 2018-10-21 05:39:01,234 > Epoch: 895,  train_loss=0.001692716,  val_loss=115.825378418,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:40:09,172 > Epoch: 896,  train_loss=0.000136622,  val_loss=105.565078735,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:41:17,137 > Epoch: 897,  train_loss=0.000152226,  val_loss=98.504287720,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:42:25,053 > Epoch: 898,  train_loss=0.000113671,  val_loss=96.793998718,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:43:33,025 > Epoch: 899,  train_loss=0.000112644,  val_loss=94.381935120,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:44:40,940 > Epoch: 900,  train_loss=0.000103870,  val_loss=93.584030151,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:45:48,866 > Epoch: 901,  train_loss=0.000106176,  val_loss=91.317047119,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:46:56,745 > Epoch: 902,  train_loss=0.000100675,  val_loss=90.277297974,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:48:04,679 > Epoch: 903,  train_loss=0.000099560,  val_loss=89.504058838,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:49:12,640 > Epoch: 904,  train_loss=0.000102154,  val_loss=86.988533020,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:50:20,608 > Epoch: 905,  train_loss=0.000097678,  val_loss=85.950622559,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:51:28,580 > Epoch: 906,  train_loss=0.000097142,  val_loss=84.823898315,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:52:36,536 > Epoch: 907,  train_loss=0.000096685,  val_loss=83.494476318,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:53:44,447 > Epoch: 908,  train_loss=0.000096340,  val_loss=82.692817688,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:54:52,388 > Epoch: 909,  train_loss=0.000095775,  val_loss=80.930526733,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:56:00,287 > Epoch: 910,  train_loss=0.000095497,  val_loss=80.516632080,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:57:08,282 > Epoch: 911,  train_loss=0.000101489,  val_loss=79.484695435,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:58:16,212 > Epoch: 912,  train_loss=0.000094837,  val_loss=79.155914307,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 05:59:24,224 > Epoch: 913,  train_loss=0.000094911,  val_loss=78.800827026,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:00:32,178 > Epoch: 914,  train_loss=0.000094922,  val_loss=79.493667603,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:01:40,115 > Epoch: 915,  train_loss=0.000094644,  val_loss=79.300964355,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:02:48,089 > Epoch: 916,  train_loss=0.000094636,  val_loss=78.990257263,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:03:55,955 > Epoch: 917,  train_loss=0.000094422,  val_loss=78.521072388,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:05:03,885 > Epoch: 918,  train_loss=0.000094385,  val_loss=77.781311035,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:06:11,797 > Epoch: 919,  train_loss=0.000094368,  val_loss=77.647293091,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:07:19,766 > Epoch: 920,  train_loss=0.000094017,  val_loss=77.649635315,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:08:27,705 > Epoch: 921,  train_loss=0.000094061,  val_loss=76.799934387,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:09:35,666 > Epoch: 922,  train_loss=0.000101599,  val_loss=82.154693604,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:10:43,596 > Epoch: 923,  train_loss=0.000102551,  val_loss=75.372154236,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:11:51,569 > Epoch: 924,  train_loss=0.000093488,  val_loss=78.846588135,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:12:59,526 > Epoch: 925,  train_loss=0.000094191,  val_loss=76.911834717,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:14:07,464 > Epoch: 926,  train_loss=0.000093743,  val_loss=80.773635864,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:15:15,362 > Epoch: 927,  train_loss=0.000094080,  val_loss=78.426872253,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:16:23,349 > Epoch: 928,  train_loss=0.000093921,  val_loss=83.160308838,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:17:31,313 > Epoch: 929,  train_loss=0.000094874,  val_loss=69.842163086,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:18:39,298 > Epoch: 930,  train_loss=0.000093545,  val_loss=69.218200684,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:19:47,226 > Epoch: 931,  train_loss=0.000094702,  val_loss=69.220344543,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:20:55,134 > Epoch: 932,  train_loss=0.000094113,  val_loss=68.719215393,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:22:03,072 > Epoch: 933,  train_loss=0.000094856,  val_loss=68.385299683,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:23:11,000 > Epoch: 934,  train_loss=0.000094144,  val_loss=68.290405273,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:24:18,875 > Epoch: 935,  train_loss=0.000094658,  val_loss=68.312561035,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:25:26,814 > Epoch: 936,  train_loss=0.000094042,  val_loss=68.160552979,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:26:34,767 > Epoch: 937,  train_loss=0.000094257,  val_loss=68.132148743,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:27:42,740 > Epoch: 938,  train_loss=0.000095000,  val_loss=68.801551819,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:28:50,680 > Epoch: 939,  train_loss=0.000093251,  val_loss=67.144920349,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:29:58,573 > Epoch: 940,  train_loss=0.000094111,  val_loss=68.124961853,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:31:06,512 > Epoch: 941,  train_loss=0.000093075,  val_loss=49.653980255,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 06:32:14,459 > Epoch: 942,  train_loss=0.000108638,  val_loss=50.671134949,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:33:22,337 > Epoch: 943,  train_loss=0.000095998,  val_loss=50.731754303,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:34:30,285 > Epoch: 944,  train_loss=0.000095181,  val_loss=49.010444641,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:35:38,358 > Epoch: 945,  train_loss=0.000091597,  val_loss=50.120349884,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:36:46,300 > Epoch: 946,  train_loss=0.000104335,  val_loss=46.420318604,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:37:54,253 > Epoch: 947,  train_loss=0.000090958,  val_loss=35.174518585,  accuracy=0.632812500
[INFO|asb_main.py:613] 2018-10-21 06:39:02,155 > Epoch: 948,  train_loss=0.003818088,  val_loss=87.822280884,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 06:40:10,071 > Epoch: 949,  train_loss=0.000758370,  val_loss=104.139228821,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 06:41:17,960 > Epoch: 950,  train_loss=0.000253615,  val_loss=96.182006836,  accuracy=0.515625000
[INFO|asb_main.py:613] 2018-10-21 06:42:25,854 > Epoch: 951,  train_loss=0.000124016,  val_loss=93.492111206,  accuracy=0.515625000
[INFO|asb_main.py:613] 2018-10-21 06:43:33,806 > Epoch: 952,  train_loss=0.000104885,  val_loss=91.883171082,  accuracy=0.515625000
[INFO|asb_main.py:613] 2018-10-21 06:44:41,777 > Epoch: 953,  train_loss=0.000100538,  val_loss=90.843666077,  accuracy=0.515625000
[INFO|asb_main.py:613] 2018-10-21 06:45:49,734 > Epoch: 954,  train_loss=0.000099028,  val_loss=89.727668762,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 06:46:57,729 > Epoch: 955,  train_loss=0.000098284,  val_loss=88.601806641,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 06:48:05,609 > Epoch: 956,  train_loss=0.000098025,  val_loss=87.235763550,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 06:49:13,496 > Epoch: 957,  train_loss=0.000097443,  val_loss=86.118637085,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 06:50:21,397 > Epoch: 958,  train_loss=0.000097311,  val_loss=84.390525818,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 06:51:29,333 > Epoch: 959,  train_loss=0.000098353,  val_loss=83.171188354,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 06:52:37,279 > Epoch: 960,  train_loss=0.000097363,  val_loss=81.817695618,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 06:53:45,257 > Epoch: 961,  train_loss=0.000097181,  val_loss=80.209762573,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 06:54:53,233 > Epoch: 962,  train_loss=0.000096814,  val_loss=79.416748047,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 06:56:01,239 > Epoch: 963,  train_loss=0.000096769,  val_loss=80.425773621,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 06:57:09,215 > Epoch: 964,  train_loss=0.000097422,  val_loss=75.257308960,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 06:58:17,133 > Epoch: 965,  train_loss=0.000097489,  val_loss=74.283859253,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 06:59:25,025 > Epoch: 966,  train_loss=0.000097250,  val_loss=74.305656433,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 07:00:32,890 > Epoch: 967,  train_loss=0.000095490,  val_loss=72.951644897,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 07:01:40,898 > Epoch: 968,  train_loss=0.000097588,  val_loss=73.226661682,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 07:02:48,909 > Epoch: 969,  train_loss=0.000096742,  val_loss=80.629722595,  accuracy=0.515625000
[INFO|asb_main.py:613] 2018-10-21 07:03:56,867 > Epoch: 970,  train_loss=0.000099226,  val_loss=71.603302002,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 07:05:04,860 > Epoch: 971,  train_loss=0.000098527,  val_loss=73.394676208,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 07:06:12,756 > Epoch: 972,  train_loss=0.000095946,  val_loss=73.139266968,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 07:07:20,669 > Epoch: 973,  train_loss=0.000096868,  val_loss=85.048896790,  accuracy=0.507812500
[INFO|asb_main.py:613] 2018-10-21 07:08:28,589 > Epoch: 974,  train_loss=0.000097978,  val_loss=53.966201782,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-21 07:09:36,500 > Epoch: 975,  train_loss=0.000097601,  val_loss=59.474685669,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 07:10:44,424 > Epoch: 976,  train_loss=0.000096376,  val_loss=68.321388245,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 07:11:52,349 > Epoch: 977,  train_loss=0.000096266,  val_loss=49.143898010,  accuracy=0.617187500
[INFO|asb_main.py:613] 2018-10-21 07:13:00,308 > Epoch: 978,  train_loss=0.000099608,  val_loss=51.312240601,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-21 07:14:08,338 > Epoch: 979,  train_loss=0.000095890,  val_loss=57.606334686,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 07:15:16,222 > Epoch: 980,  train_loss=0.000099008,  val_loss=69.359413147,  accuracy=0.523437500
[INFO|asb_main.py:613] 2018-10-21 07:16:24,151 > Epoch: 981,  train_loss=0.000096804,  val_loss=71.369934082,  accuracy=0.500000000
[INFO|asb_main.py:613] 2018-10-21 07:17:32,136 > Epoch: 982,  train_loss=0.000095681,  val_loss=54.385288239,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-21 07:18:40,015 > Epoch: 983,  train_loss=0.000096537,  val_loss=67.825111389,  accuracy=0.531250000
[INFO|asb_main.py:613] 2018-10-21 07:19:47,967 > Epoch: 984,  train_loss=0.000097345,  val_loss=41.431098938,  accuracy=0.648437500
[INFO|asb_main.py:613] 2018-10-21 07:20:55,934 > Epoch: 985,  train_loss=0.000098878,  val_loss=37.756462097,  accuracy=0.710937500
[INFO|asb_main.py:613] 2018-10-21 07:22:03,912 > Epoch: 986,  train_loss=0.000101226,  val_loss=35.028450012,  accuracy=0.726562500
[INFO|asb_main.py:613] 2018-10-21 07:23:11,876 > Epoch: 987,  train_loss=0.000096906,  val_loss=34.728298187,  accuracy=0.742187500
[INFO|asb_main.py:613] 2018-10-21 07:24:19,869 > Epoch: 988,  train_loss=0.000095310,  val_loss=33.581470490,  accuracy=0.750000000
[INFO|asb_main.py:613] 2018-10-21 07:25:27,765 > Epoch: 989,  train_loss=0.000095893,  val_loss=31.419292450,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 07:26:35,672 > Epoch: 990,  train_loss=0.000100264,  val_loss=30.118999481,  accuracy=0.757812500
[INFO|asb_main.py:613] 2018-10-21 07:27:43,580 > Epoch: 991,  train_loss=0.000092857,  val_loss=30.755907059,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 07:28:51,526 > Epoch: 992,  train_loss=0.000094750,  val_loss=29.530250549,  accuracy=0.765625000
[INFO|asb_main.py:613] 2018-10-21 07:29:59,475 > Epoch: 993,  train_loss=0.000095092,  val_loss=27.677299500,  accuracy=0.773437500
[INFO|asb_main.py:613] 2018-10-21 07:31:07,427 > Epoch: 994,  train_loss=0.000080908,  val_loss=29.037120819,  accuracy=0.734375000
[INFO|asb_main.py:613] 2018-10-21 07:32:15,394 > Epoch: 995,  train_loss=0.000269298,  val_loss=44.743377686,  accuracy=0.609375000
[INFO|asb_main.py:613] 2018-10-21 07:33:23,321 > Epoch: 996,  train_loss=0.000078421,  val_loss=48.782608032,  accuracy=0.585937500
[INFO|asb_main.py:613] 2018-10-21 07:34:31,224 > Epoch: 997,  train_loss=0.007218436,  val_loss=63.756763458,  accuracy=0.796875000
[INFO|asb_main.py:613] 2018-10-21 07:35:39,119 > Epoch: 998,  train_loss=0.003681600,  val_loss=110.503662109,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-21 07:36:47,045 > Epoch: 999,  train_loss=0.000161960,  val_loss=106.836555481,  accuracy=0.812500000
[INFO|asb_main.py:613] 2018-10-21 07:37:55,023 > Epoch:1000,  train_loss=0.000117731,  val_loss=106.160339355,  accuracy=0.812500000
[INFO|asb_main.py:624] 2018-10-21 07:37:55,324 > ### Learning Finished!
[INFO|asb_main.py:870] 2018-10-21 07:37:55,329 > Model_4 Program end [ Total time : 18 Hour 50 Minute 49 Second ]
/home/sdsra/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[INFO|asb_main.py:1080] 2018-10-23 16:36:11,379 > ### Loading data...
[INFO|preprocessing.py:27] 2018-10-23 16:36:12,703 > x_char_seq_ind=(21375, 500)
[INFO|preprocessing.py:28] 2018-10-23 16:36:12,703 > y shape=(21375,)
[INFO|preprocessing.py:27] 2018-10-23 16:36:12,715 > x_char_seq_ind=(147, 500)
[INFO|preprocessing.py:28] 2018-10-23 16:36:12,715 > y shape=(147,)
[INFO|asb_main.py:1091] 2018-10-23 16:36:12,715 > Train/Validate split: 21375/147
[INFO|asb_main.py:104] 2018-10-23 16:36:12,715 > ### model_100 Learning Start!
2018-10-23 16:36:12.716067: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-23 16:36:12.825664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-23 16:36:12.826175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 479.25MiB
2018-10-23 16:36:12.826195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-10-23 16:36:13.318147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-23 16:36:13.318176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-10-23 16:36:13.318181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-10-23 16:36:13.318298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 190 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/sdsra/Documents/Char-level-tf/model.py:136: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

[INFO|asb_main.py:74] 2018-10-23 16:36:14,137 > Writing to /home/sdsra/Documents/Char-level-tf/runs/model_100_1540337774
[INFO|asb_main.py:47] 2018-10-23 16:36:14,173 > ### Parameters ----------------------------------
[INFO|asb_main.py:53] 2018-10-23 16:36:14,174 > 	epochs : 300
[INFO|asb_main.py:53] 2018-10-23 16:36:14,174 > 	batch_size : 128
[INFO|asb_main.py:53] 2018-10-23 16:36:14,175 > 	learning_rate : 5e-06
[INFO|asb_main.py:53] 2018-10-23 16:36:14,175 > 	dropout_rate : 0.5
[INFO|asb_main.py:53] 2018-10-23 16:36:14,175 > 	alphabet : abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'"/\|_@#$%^&*~`+-=<>()[]{}

[INFO|asb_main.py:53] 2018-10-23 16:36:14,175 > 	char_max_length : 500
[INFO|asb_main.py:53] 2018-10-23 16:36:14,175 > 	num_of_classes : 3
[INFO|asb_main.py:53] 2018-10-23 16:36:14,175 > 	input_num_of_rows : 1
[INFO|asb_main.py:53] 2018-10-23 16:36:14,175 > 	filter_sizes : (7, 7, 3, 3, 3, 3)
[INFO|asb_main.py:53] 2018-10-23 16:36:14,175 > 	num_filters_per_size : 256
[INFO|asb_main.py:53] 2018-10-23 16:36:14,175 > 	num_quantized_chars : 70
[INFO|asb_main.py:57] 2018-10-23 16:36:14,175 > ### Values of Graph  ----------------------------------
[INFO|asb_main.py:61] 2018-10-23 16:36:14,175 > 	<tf.Variable 'conv-maxpool-1/conv2d/kernel:0' shape=(70, 7, 1, 256) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,175 > 	<tf.Variable 'conv-maxpool-1/conv2d/bias:0' shape=(256,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,175 > 	<tf.Variable 'conv-maxpool-2/conv2d/kernel:0' shape=(1, 7, 256, 256) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,175 > 	<tf.Variable 'conv-maxpool-2/conv2d/bias:0' shape=(256,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,175 > 	<tf.Variable 'conv-3/conv2d/kernel:0' shape=(1, 3, 256, 256) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,175 > 	<tf.Variable 'conv-3/conv2d/bias:0' shape=(256,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,175 > 	<tf.Variable 'conv-4/conv2d/kernel:0' shape=(1, 3, 256, 256) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,175 > 	<tf.Variable 'conv-4/conv2d/bias:0' shape=(256,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'conv-5/conv2d/kernel:0' shape=(1, 3, 256, 256) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'conv-5/conv2d/bias:0' shape=(256,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'conv-maxpool-6/conv2d/kernel:0' shape=(1, 3, 256, 256) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'conv-maxpool-6/conv2d/bias:0' shape=(256,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'fc-1/dense/kernel:0' shape=(3584, 1024) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'fc-1/dense/bias:0' shape=(1024,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'fc-2/dense/kernel:0' shape=(1024, 1024) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'fc-2/dense/bias:0' shape=(1024,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'fc-3/dense/kernel:0' shape=(1024, 1024) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'fc-3/dense/bias:0' shape=(1024,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'dense/kernel:0' shape=(1024, 3) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:36:14,176 > 	<tf.Variable 'dense/bias:0' shape=(3,) dtype=float32_ref>
[INFO|asb_main.py:65] 2018-10-23 16:36:14,176 > --------------------------------------------------------------------
2018-10-23 16:36:24.570510: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 490.0KiB.  Current allocation summary follows.
2018-10-23 16:36:24.570545: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (256): 	Total Chunks: 33, Chunks in use: 33. 8.2KiB allocated for chunks. 8.2KiB in use in bin. 164B client-requested in use in bin.
2018-10-23 16:36:24.570552: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570557: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1024): 	Total Chunks: 27, Chunks in use: 27. 27.8KiB allocated for chunks. 27.8KiB in use in bin. 27.5KiB client-requested in use in bin.
2018-10-23 16:36:24.570561: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570566: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4096): 	Total Chunks: 10, Chunks in use: 10. 40.0KiB allocated for chunks. 40.0KiB in use in bin. 40.0KiB client-requested in use in bin.
2018-10-23 16:36:24.570571: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8192): 	Total Chunks: 4, Chunks in use: 4. 48.0KiB allocated for chunks. 48.0KiB in use in bin. 48.0KiB client-requested in use in bin.
2018-10-23 16:36:24.570575: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570580: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570587: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570591: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570596: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (262144): 	Total Chunks: 4, Chunks in use: 4. 1.91MiB allocated for chunks. 1.91MiB in use in bin. 1.91MiB client-requested in use in bin.
2018-10-23 16:36:24.570600: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (524288): 	Total Chunks: 13, Chunks in use: 13. 9.75MiB allocated for chunks. 9.75MiB in use in bin. 9.75MiB client-requested in use in bin.
2018-10-23 16:36:24.570605: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (1048576): 	Total Chunks: 4, Chunks in use: 4. 7.00MiB allocated for chunks. 7.00MiB in use in bin. 7.00MiB client-requested in use in bin.
2018-10-23 16:36:24.570609: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570620: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (4194304): 	Total Chunks: 7, Chunks in use: 7. 28.00MiB allocated for chunks. 28.00MiB in use in bin. 28.00MiB client-requested in use in bin.
2018-10-23 16:36:24.570625: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (8388608): 	Total Chunks: 4, Chunks in use: 4. 56.00MiB allocated for chunks. 56.00MiB in use in bin. 56.00MiB client-requested in use in bin.
2018-10-23 16:36:24.570630: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (16777216): 	Total Chunks: 1, Chunks in use: 1. 17.09MiB allocated for chunks. 17.09MiB in use in bin. 17.09MiB client-requested in use in bin.
2018-10-23 16:36:24.570634: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570639: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (67108864): 	Total Chunks: 1, Chunks in use: 1. 70.38MiB allocated for chunks. 70.38MiB in use in bin. 61.75MiB client-requested in use in bin.
2018-10-23 16:36:24.570644: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570650: I tensorflow/core/common_runtime/bfc_allocator.cc:630] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2018-10-23 16:36:24.570656: I tensorflow/core/common_runtime/bfc_allocator.cc:646] Bin for 490.0KiB was 256.0KiB, Chunk State: 
2018-10-23 16:36:24.570661: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c000000 of size 1280
2018-10-23 16:36:24.570664: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c000500 of size 256
2018-10-23 16:36:24.570667: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c000600 of size 256
2018-10-23 16:36:24.570671: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c000700 of size 1024
2018-10-23 16:36:24.570674: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c000b00 of size 256
2018-10-23 16:36:24.570677: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c000c00 of size 1024
2018-10-23 16:36:24.570681: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c001000 of size 256
2018-10-23 16:36:24.570684: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c001100 of size 1024
2018-10-23 16:36:24.570687: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c001500 of size 256
2018-10-23 16:36:24.570690: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c001600 of size 1024
2018-10-23 16:36:24.570694: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c001a00 of size 501760
2018-10-23 16:36:24.570697: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c07c200 of size 256
2018-10-23 16:36:24.570700: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c07c300 of size 256
2018-10-23 16:36:24.570703: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c07c400 of size 1024
2018-10-23 16:36:24.570707: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c07c800 of size 1835008
2018-10-23 16:36:24.570711: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c23c800 of size 256
2018-10-23 16:36:24.570716: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c23c900 of size 256
2018-10-23 16:36:24.570720: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c23ca00 of size 1024
2018-10-23 16:36:24.570724: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c23ce00 of size 786432
2018-10-23 16:36:24.570727: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c2fce00 of size 256
2018-10-23 16:36:24.570732: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c2fcf00 of size 256
2018-10-23 16:36:24.570736: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c2fd000 of size 256
2018-10-23 16:36:24.570739: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c2fd100 of size 12288
2018-10-23 16:36:24.570742: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c300100 of size 256
2018-10-23 16:36:24.570746: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c300200 of size 256
2018-10-23 16:36:24.570749: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422c300300 of size 14680064
2018-10-23 16:36:24.570752: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d100300 of size 256
2018-10-23 16:36:24.570755: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d100400 of size 256
2018-10-23 16:36:24.570758: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d100500 of size 256
2018-10-23 16:36:24.570762: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d100600 of size 4096
2018-10-23 16:36:24.570765: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d101600 of size 4194304
2018-10-23 16:36:24.570769: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d501600 of size 256
2018-10-23 16:36:24.570772: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d501700 of size 256
2018-10-23 16:36:24.570775: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d501800 of size 501760
2018-10-23 16:36:24.570781: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d57c000 of size 1024
2018-10-23 16:36:24.570786: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d57c400 of size 1024
2018-10-23 16:36:24.570789: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d57c800 of size 1024
2018-10-23 16:36:24.570792: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d57cc00 of size 1835008
2018-10-23 16:36:24.570795: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d73cc00 of size 1835008
2018-10-23 16:36:24.570799: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422d8fcc00 of size 1835008
2018-10-23 16:36:24.570802: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dabcc00 of size 1024
2018-10-23 16:36:24.570805: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dabd000 of size 1024
2018-10-23 16:36:24.570808: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dabd400 of size 1024
2018-10-23 16:36:24.570812: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dabd800 of size 786432
2018-10-23 16:36:24.570815: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422db7d800 of size 786432
2018-10-23 16:36:24.570818: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dc3d800 of size 786432
2018-10-23 16:36:24.570821: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dcfd800 of size 256
2018-10-23 16:36:24.570824: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dcfd900 of size 256
2018-10-23 16:36:24.570828: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dcfda00 of size 256
2018-10-23 16:36:24.570831: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dcfdb00 of size 12288
2018-10-23 16:36:24.570834: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dd00b00 of size 12288
2018-10-23 16:36:24.570837: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dd03b00 of size 12288
2018-10-23 16:36:24.570841: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422dd06b00 of size 14680064
2018-10-23 16:36:24.570846: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422eb06b00 of size 14680064
2018-10-23 16:36:24.570853: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f422f906b00 of size 14680064
2018-10-23 16:36:24.570856: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230706b00 of size 4194304
2018-10-23 16:36:24.570860: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b06b00 of size 4096
2018-10-23 16:36:24.570863: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b07b00 of size 4096
2018-10-23 16:36:24.570866: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b08b00 of size 4096
2018-10-23 16:36:24.570869: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b09b00 of size 4096
2018-10-23 16:36:24.570872: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b0ab00 of size 4096
2018-10-23 16:36:24.570876: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b0bb00 of size 4096
2018-10-23 16:36:24.570879: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b0cb00 of size 4096
2018-10-23 16:36:24.570882: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b0db00 of size 4096
2018-10-23 16:36:24.570885: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b0eb00 of size 4096
2018-10-23 16:36:24.570888: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230b0fb00 of size 4194304
2018-10-23 16:36:24.570892: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4230f0fb00 of size 4194304
2018-10-23 16:36:24.570895: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f423130fb00 of size 4194304
2018-10-23 16:36:24.570898: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f423170fb00 of size 4194304
2018-10-23 16:36:24.570901: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4231b0fb00 of size 4194304
2018-10-23 16:36:24.570905: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4231f0fb00 of size 256
2018-10-23 16:36:24.570909: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4231f0fc00 of size 256
2018-10-23 16:36:24.570914: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4231f0fd00 of size 1024
2018-10-23 16:36:24.570918: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4231f10100 of size 1024
2018-10-23 16:36:24.570921: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4231f10500 of size 1024
2018-10-23 16:36:24.570925: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4231f10900 of size 786432
2018-10-23 16:36:24.570928: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4231fd0900 of size 786432
2018-10-23 16:36:24.570931: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232090900 of size 786432
2018-10-23 16:36:24.570934: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232150900 of size 1024
2018-10-23 16:36:24.570938: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232150d00 of size 1024
2018-10-23 16:36:24.570941: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232151100 of size 1024
2018-10-23 16:36:24.570944: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232151500 of size 786432
2018-10-23 16:36:24.570947: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232211500 of size 786432
2018-10-23 16:36:24.570951: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42322d1500 of size 786432
2018-10-23 16:36:24.570954: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232391500 of size 1024
2018-10-23 16:36:24.570957: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232391900 of size 1024
2018-10-23 16:36:24.570960: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232391d00 of size 1024
2018-10-23 16:36:24.570963: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232392100 of size 786432
2018-10-23 16:36:24.570968: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232452100 of size 786432
2018-10-23 16:36:24.570972: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f4232512100 of size 786432
2018-10-23 16:36:24.570977: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42325d2100 of size 1024
2018-10-23 16:36:24.570982: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42325d2500 of size 1024
2018-10-23 16:36:24.570986: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42325d2900 of size 1024
2018-10-23 16:36:24.570989: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42325d2d00 of size 501760
2018-10-23 16:36:24.570992: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f423264d500 of size 501760
2018-10-23 16:36:24.570995: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c7d00 of size 256
2018-10-23 16:36:24.570999: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c7e00 of size 256
2018-10-23 16:36:24.571002: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c7f00 of size 256
2018-10-23 16:36:24.571005: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c8000 of size 256
2018-10-23 16:36:24.571008: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c8100 of size 256
2018-10-23 16:36:24.571011: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c8200 of size 256
2018-10-23 16:36:24.571015: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c8300 of size 256
2018-10-23 16:36:24.571018: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c8400 of size 256
2018-10-23 16:36:24.571021: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c8500 of size 256
2018-10-23 16:36:24.571024: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42326c8600 of size 17920000
2018-10-23 16:36:24.571028: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42337df600 of size 1536
2018-10-23 16:36:24.571031: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42337dfc00 of size 1024
2018-10-23 16:36:24.571034: I tensorflow/core/common_runtime/bfc_allocator.cc:665] Chunk at 0x7f42337e0000 of size 73793536
2018-10-23 16:36:24.571038: I tensorflow/core/common_runtime/bfc_allocator.cc:671]      Summary of in-use Chunks by size: 
2018-10-23 16:36:24.571044: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 33 Chunks of size 256 totalling 8.2KiB
2018-10-23 16:36:24.571050: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 25 Chunks of size 1024 totalling 25.0KiB
2018-10-23 16:36:24.571054: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1280 totalling 1.2KiB
2018-10-23 16:36:24.571057: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 1536 totalling 1.5KiB
2018-10-23 16:36:24.571061: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 10 Chunks of size 4096 totalling 40.0KiB
2018-10-23 16:36:24.571065: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 12288 totalling 48.0KiB
2018-10-23 16:36:24.571068: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 501760 totalling 1.91MiB
2018-10-23 16:36:24.571072: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 13 Chunks of size 786432 totalling 9.75MiB
2018-10-23 16:36:24.571076: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 1835008 totalling 7.00MiB
2018-10-23 16:36:24.571079: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 7 Chunks of size 4194304 totalling 28.00MiB
2018-10-23 16:36:24.571083: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 4 Chunks of size 14680064 totalling 56.00MiB
2018-10-23 16:36:24.571087: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 17920000 totalling 17.09MiB
2018-10-23 16:36:24.571093: I tensorflow/core/common_runtime/bfc_allocator.cc:674] 1 Chunks of size 73793536 totalling 70.38MiB
2018-10-23 16:36:24.571097: I tensorflow/core/common_runtime/bfc_allocator.cc:678] Sum Total of in-use chunks: 190.25MiB
2018-10-23 16:36:24.571102: I tensorflow/core/common_runtime/bfc_allocator.cc:680] Stats: 
Limit:                   199491584
InUse:                   199491584
MaxInUse:                199491584
NumAllocs:                     108
MaxAllocSize:             73793536

2018-10-23 16:36:24.571113: W tensorflow/core/common_runtime/bfc_allocator.cc:279] ************************************************************************************************xxxx
2018-10-23 16:36:24.571131: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at conv_ops.cc:682 : Resource exhausted: OOM when allocating tensor with shape[256,1,70,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1278, in _do_call
    return fn(*args)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1263, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1350, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[256,1,70,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: conv-maxpool-1/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](gradients/conv-maxpool-1/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv-maxpool-1/conv2d/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "asb_main.py", line 1293, in <module>
    'model_221': False})
  File "asb_main.py", line 1110, in main
    num_quantized_chars=70)
  File "asb_main.py", line 149, in train_model_100
    c, _, train_loss_summary = model_.train(batch_xs_one_hot, batch_ys)
  File "/home/sdsra/Documents/Char-level-tf/model.py", line 164, in train
    self.X: x_data, self.Y: y_data, self.training: training})
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 877, in run
    run_metadata_ptr)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1100, in _run
    feed_dict_tensor, options, run_metadata)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1272, in _do_run
    run_metadata)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py", line 1291, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[256,1,70,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: conv-maxpool-1/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](gradients/conv-maxpool-1/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv-maxpool-1/conv2d/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


Caused by op 'conv-maxpool-1/conv2d/Conv2D', defined at:
  File "asb_main.py", line 1293, in <module>
    'model_221': False})
  File "asb_main.py", line 1110, in main
    num_quantized_chars=70)
  File "asb_main.py", line 116, in train_model_100
    num_quantized_chars)
  File "/home/sdsra/Documents/Char-level-tf/model.py", line 34, in build_net
    padding="VALID", activation=tf.nn.relu)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py", line 425, in conv2d
    return layer.apply(inputs)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py", line 805, in apply
    return self.__call__(inputs, *args, **kwargs)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py", line 362, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py", line 736, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py", line 186, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 868, in __call__
    return self.conv_op(inp, filter)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 520, in __call__
    return self.call(inp, filter)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py", line 204, in __call__
    name=self.name)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py", line 956, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py", line 454, in new_func
    return func(*args, **kwargs)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 3155, in create_op
    op_def=op_def)
  File "/home/sdsra/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py", line 1717, in __init__
    self._traceback = tf_stack.extract_stack()

ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,1,70,7] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[Node: conv-maxpool-1/conv2d/Conv2D = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device="/job:localhost/replica:0/task:0/device:GPU:0"](gradients/conv-maxpool-1/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv-maxpool-1/conv2d/kernel/read)]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.


/home/sdsra/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[INFO|asb_main.py:1080] 2018-10-23 16:40:13,202 > ### Loading data...
[INFO|preprocessing.py:27] 2018-10-23 16:40:14,514 > x_char_seq_ind=(21375, 500)
[INFO|preprocessing.py:28] 2018-10-23 16:40:14,514 > y shape=(21375,)
[INFO|preprocessing.py:27] 2018-10-23 16:40:14,527 > x_char_seq_ind=(147, 500)
[INFO|preprocessing.py:28] 2018-10-23 16:40:14,527 > y shape=(147,)
[INFO|asb_main.py:1091] 2018-10-23 16:40:14,527 > Train/Validate split: 21375/147
[INFO|asb_main.py:435] 2018-10-23 16:40:14,527 > ### model_201 Learning Start!
2018-10-23 16:40:14.527348: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-23 16:40:14.612748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-23 16:40:14.613281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.70GiB
2018-10-23 16:40:14.613293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-10-23 16:40:14.787547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-23 16:40:14.787579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-10-23 16:40:14.787585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-10-23 16:40:14.787732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10347 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/sdsra/Documents/Char-level-tf/model.py:507: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

[INFO|asb_main.py:74] 2018-10-23 16:40:17,238 > Writing to /home/sdsra/Documents/Char-level-tf/runs/model_201_1540338017
[INFO|asb_main.py:47] 2018-10-23 16:40:17,260 > ### Parameters ----------------------------------
[INFO|asb_main.py:53] 2018-10-23 16:40:17,261 > 	epochs : 1000
[INFO|asb_main.py:53] 2018-10-23 16:40:17,261 > 	batch_size : 128
[INFO|asb_main.py:53] 2018-10-23 16:40:17,261 > 	learning_rate : 0.001
[INFO|asb_main.py:53] 2018-10-23 16:40:17,261 > 	dropout_rate : 0.5
[INFO|asb_main.py:53] 2018-10-23 16:40:17,261 > 	alphabet : abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'"/\|_@#$%^&*~`+-=<>()[]{}

[INFO|asb_main.py:53] 2018-10-23 16:40:17,261 > 	char_max_length : 500
[INFO|asb_main.py:53] 2018-10-23 16:40:17,261 > 	num_of_classes : 3
[INFO|asb_main.py:53] 2018-10-23 16:40:17,261 > 	input_num_of_rows : 1
[INFO|asb_main.py:53] 2018-10-23 16:40:17,261 > 	first_decay_steps : 5000
[INFO|asb_main.py:57] 2018-10-23 16:40:17,261 > ### Values of Graph  ----------------------------------
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'Layer1/conv2d/kernel:0' shape=(3, 3, 1, 64) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'Layer1/conv2d/bias:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'Layer2/conv2d/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'Layer2/conv2d/bias:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'Layer3/conv2d/kernel:0' shape=(3, 3, 64, 128) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'Layer3/conv2d/bias:0' shape=(128,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'FC/dense/kernel:0' shape=(72576, 1024) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'FC/dense/bias:0' shape=(1024,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'dense/kernel:0' shape=(1024, 3) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-23 16:40:17,261 > 	<tf.Variable 'dense/bias:0' shape=(3,) dtype=float32_ref>
[INFO|asb_main.py:65] 2018-10-23 16:40:17,262 > --------------------------------------------------------------------
[INFO|asb_main.py:507] 2018-10-23 16:40:57,927 > Epoch:   1,  train_loss=0.227137518,  val_loss=0.384201854,  accuracy=0.867187500
[INFO|asb_main.py:507] 2018-10-23 16:41:37,591 > Epoch:   2,  train_loss=0.052293069,  val_loss=0.365226090,  accuracy=0.898437500
[INFO|asb_main.py:507] 2018-10-23 16:42:17,066 > Epoch:   3,  train_loss=0.019432223,  val_loss=0.451166034,  accuracy=0.898437500
[INFO|asb_main.py:507] 2018-10-23 16:42:56,529 > Epoch:   4,  train_loss=0.009086934,  val_loss=0.619993210,  accuracy=0.898437500
[INFO|asb_main.py:507] 2018-10-23 16:43:36,040 > Epoch:   5,  train_loss=0.005681818,  val_loss=0.770203948,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 16:44:15,574 > Epoch:   6,  train_loss=0.003811749,  val_loss=0.798856199,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 16:44:55,081 > Epoch:   7,  train_loss=0.002553422,  val_loss=0.907339573,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 16:45:34,675 > Epoch:   8,  train_loss=0.001453073,  val_loss=0.912128687,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:46:14,287 > Epoch:   9,  train_loss=0.001027615,  val_loss=1.046133757,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 16:46:53,820 > Epoch:  10,  train_loss=0.000996474,  val_loss=0.787108898,  accuracy=0.906250000
[INFO|asb_main.py:507] 2018-10-23 16:47:33,385 > Epoch:  11,  train_loss=0.000708350,  val_loss=0.964787126,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 16:48:13,004 > Epoch:  12,  train_loss=0.000430062,  val_loss=1.043817401,  accuracy=0.906250000
[INFO|asb_main.py:507] 2018-10-23 16:48:52,682 > Epoch:  13,  train_loss=0.000905068,  val_loss=0.999053299,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:49:32,182 > Epoch:  14,  train_loss=0.000984899,  val_loss=1.035223007,  accuracy=0.906250000
[INFO|asb_main.py:507] 2018-10-23 16:50:11,714 > Epoch:  15,  train_loss=0.000950245,  val_loss=1.293111801,  accuracy=0.914062500
[INFO|asb_main.py:507] 2018-10-23 16:50:51,271 > Epoch:  16,  train_loss=0.000966888,  val_loss=1.070947647,  accuracy=0.914062500
[INFO|asb_main.py:507] 2018-10-23 16:51:30,806 > Epoch:  17,  train_loss=0.001327324,  val_loss=1.009454846,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:52:10,318 > Epoch:  18,  train_loss=0.000900494,  val_loss=0.689173639,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 16:52:49,864 > Epoch:  19,  train_loss=0.000867485,  val_loss=0.677708626,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 16:53:29,373 > Epoch:  20,  train_loss=0.000735197,  val_loss=0.787746191,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:54:08,889 > Epoch:  21,  train_loss=0.000829967,  val_loss=0.740551054,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:54:48,434 > Epoch:  22,  train_loss=0.000361000,  val_loss=0.783647895,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:55:27,932 > Epoch:  23,  train_loss=0.000270805,  val_loss=0.794404626,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:56:07,447 > Epoch:  24,  train_loss=0.000502762,  val_loss=0.807583153,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:56:46,962 > Epoch:  25,  train_loss=0.000312194,  val_loss=0.828507841,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:57:26,520 > Epoch:  26,  train_loss=0.000542834,  val_loss=0.822297454,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:58:06,013 > Epoch:  27,  train_loss=0.001005343,  val_loss=0.821464777,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 16:58:45,515 > Epoch:  28,  train_loss=0.000251124,  val_loss=0.821711183,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 16:59:25,032 > Epoch:  29,  train_loss=0.000445270,  val_loss=0.821484387,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 17:00:04,540 > Epoch:  30,  train_loss=0.000379986,  val_loss=0.821042001,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 17:00:44,101 > Epoch:  31,  train_loss=0.000924033,  val_loss=0.850222766,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 17:01:23,646 > Epoch:  32,  train_loss=0.089989513,  val_loss=3.237759829,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 17:02:03,174 > Epoch:  33,  train_loss=0.093855205,  val_loss=1.317560434,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:02:42,700 > Epoch:  34,  train_loss=0.003430812,  val_loss=1.122306585,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 17:03:22,203 > Epoch:  35,  train_loss=0.001053887,  val_loss=1.108025312,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:04:01,727 > Epoch:  36,  train_loss=0.000509970,  val_loss=1.044443607,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:04:41,252 > Epoch:  37,  train_loss=0.001093978,  val_loss=1.109619498,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:05:20,782 > Epoch:  38,  train_loss=0.000498270,  val_loss=1.105167508,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:06:00,295 > Epoch:  39,  train_loss=0.000176785,  val_loss=1.087914348,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:06:39,803 > Epoch:  40,  train_loss=0.000444595,  val_loss=1.062560797,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:07:19,319 > Epoch:  41,  train_loss=0.000291969,  val_loss=1.060514212,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:07:58,861 > Epoch:  42,  train_loss=0.000543654,  val_loss=1.096204400,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:08:38,373 > Epoch:  43,  train_loss=0.000522850,  val_loss=1.090964317,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:09:17,902 > Epoch:  44,  train_loss=0.000142620,  val_loss=1.116692305,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:09:57,430 > Epoch:  45,  train_loss=0.000652339,  val_loss=1.050515652,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:10:36,950 > Epoch:  46,  train_loss=0.000470722,  val_loss=1.043153882,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:11:16,499 > Epoch:  47,  train_loss=0.000193595,  val_loss=1.047977448,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:11:56,055 > Epoch:  48,  train_loss=0.000160401,  val_loss=1.031301379,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:12:35,598 > Epoch:  49,  train_loss=0.000708115,  val_loss=1.029394984,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:13:15,100 > Epoch:  50,  train_loss=0.000884318,  val_loss=0.989642620,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:13:54,623 > Epoch:  51,  train_loss=0.000199217,  val_loss=1.007837296,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:14:34,160 > Epoch:  52,  train_loss=0.000373860,  val_loss=0.999555111,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:15:13,697 > Epoch:  53,  train_loss=0.000212367,  val_loss=1.021378994,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:15:53,296 > Epoch:  54,  train_loss=0.000483915,  val_loss=0.966768622,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:16:32,820 > Epoch:  55,  train_loss=0.000349016,  val_loss=0.977308095,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:17:12,356 > Epoch:  56,  train_loss=0.000082550,  val_loss=1.004799843,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:17:51,869 > Epoch:  57,  train_loss=0.000463997,  val_loss=0.911971688,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:18:31,360 > Epoch:  58,  train_loss=0.000104163,  val_loss=0.945182383,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:19:10,861 > Epoch:  59,  train_loss=0.000425239,  val_loss=0.917224288,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:19:50,363 > Epoch:  60,  train_loss=0.000405245,  val_loss=0.884172797,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:20:29,876 > Epoch:  61,  train_loss=0.000185932,  val_loss=0.870348811,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:21:09,408 > Epoch:  62,  train_loss=0.000249848,  val_loss=0.876131296,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:21:48,923 > Epoch:  63,  train_loss=0.000347843,  val_loss=0.881090283,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:22:28,446 > Epoch:  64,  train_loss=0.000351485,  val_loss=0.881605983,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:23:07,985 > Epoch:  65,  train_loss=0.000236721,  val_loss=0.878852904,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:23:47,487 > Epoch:  66,  train_loss=0.000347111,  val_loss=0.880995214,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:24:26,994 > Epoch:  67,  train_loss=0.000539759,  val_loss=0.935067177,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:25:06,496 > Epoch:  68,  train_loss=0.000197259,  val_loss=0.936096191,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:25:46,049 > Epoch:  69,  train_loss=0.000149451,  val_loss=0.928959250,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:26:25,594 > Epoch:  70,  train_loss=0.000178871,  val_loss=0.950454473,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:27:05,139 > Epoch:  71,  train_loss=0.000075647,  val_loss=0.948369205,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:27:44,662 > Epoch:  72,  train_loss=0.000342924,  val_loss=0.918380380,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:28:24,154 > Epoch:  73,  train_loss=0.000194332,  val_loss=0.927846313,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:29:03,682 > Epoch:  74,  train_loss=0.000281982,  val_loss=0.914764762,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:29:43,221 > Epoch:  75,  train_loss=0.000332660,  val_loss=0.886303067,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:30:22,732 > Epoch:  76,  train_loss=0.000282532,  val_loss=0.885301471,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:31:02,259 > Epoch:  77,  train_loss=0.000460952,  val_loss=0.864161909,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:31:41,777 > Epoch:  78,  train_loss=0.000068498,  val_loss=0.869214296,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:32:21,294 > Epoch:  79,  train_loss=0.000132366,  val_loss=0.880213022,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:33:00,797 > Epoch:  80,  train_loss=0.000127472,  val_loss=0.878075421,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:33:40,345 > Epoch:  81,  train_loss=0.000129640,  val_loss=0.877475262,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:34:19,845 > Epoch:  82,  train_loss=0.000132376,  val_loss=0.877857447,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:34:59,373 > Epoch:  83,  train_loss=0.000201614,  val_loss=0.875424325,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:35:38,892 > Epoch:  84,  train_loss=0.000118219,  val_loss=0.880566597,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:36:18,402 > Epoch:  85,  train_loss=0.000113235,  val_loss=0.881805301,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:36:57,923 > Epoch:  86,  train_loss=0.000220175,  val_loss=0.880242586,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:37:37,439 > Epoch:  87,  train_loss=0.000186613,  val_loss=0.879524410,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:38:16,954 > Epoch:  88,  train_loss=0.000198659,  val_loss=0.879633307,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:38:56,475 > Epoch:  89,  train_loss=0.000204434,  val_loss=0.879056811,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:39:35,990 > Epoch:  90,  train_loss=0.000120431,  val_loss=0.879018784,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:40:15,536 > Epoch:  91,  train_loss=0.000233568,  val_loss=1.421634078,  accuracy=0.898437500
[INFO|asb_main.py:507] 2018-10-23 17:40:55,069 > Epoch:  92,  train_loss=0.011089730,  val_loss=1.483196974,  accuracy=0.960937500
[INFO|asb_main.py:507] 2018-10-23 17:41:34,569 > Epoch:  93,  train_loss=0.006473843,  val_loss=1.457606077,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:42:14,090 > Epoch:  94,  train_loss=0.001013679,  val_loss=1.738989830,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:42:53,594 > Epoch:  95,  train_loss=0.002378048,  val_loss=1.593124032,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:43:33,106 > Epoch:  96,  train_loss=0.001152348,  val_loss=1.605142355,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:44:12,628 > Epoch:  97,  train_loss=0.001302450,  val_loss=1.478881598,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:44:52,123 > Epoch:  98,  train_loss=0.001330085,  val_loss=1.598725080,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:45:31,642 > Epoch:  99,  train_loss=0.002302434,  val_loss=1.042916775,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:46:11,174 > Epoch: 100,  train_loss=0.000734642,  val_loss=1.282594800,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:46:50,684 > Epoch: 101,  train_loss=0.000180392,  val_loss=1.312387705,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:47:30,189 > Epoch: 102,  train_loss=0.000300510,  val_loss=1.212723970,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:48:09,686 > Epoch: 103,  train_loss=0.000835697,  val_loss=1.044995546,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:48:49,188 > Epoch: 104,  train_loss=0.000584379,  val_loss=0.694267392,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:49:28,678 > Epoch: 105,  train_loss=0.003248964,  val_loss=1.146662951,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:50:08,198 > Epoch: 106,  train_loss=0.000358986,  val_loss=1.220801830,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:50:47,679 > Epoch: 107,  train_loss=0.000507705,  val_loss=1.292942524,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:51:27,161 > Epoch: 108,  train_loss=0.000415848,  val_loss=1.166023731,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 17:52:06,649 > Epoch: 109,  train_loss=0.001152017,  val_loss=1.122981787,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:52:46,135 > Epoch: 110,  train_loss=0.000242357,  val_loss=1.291659951,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:53:25,614 > Epoch: 111,  train_loss=0.000363422,  val_loss=1.256242871,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:54:05,114 > Epoch: 112,  train_loss=0.001038562,  val_loss=0.926813483,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:54:44,629 > Epoch: 113,  train_loss=0.000323627,  val_loss=1.059719801,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:55:24,134 > Epoch: 114,  train_loss=0.001207768,  val_loss=1.097487926,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:56:03,598 > Epoch: 115,  train_loss=0.000402836,  val_loss=0.962504148,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:56:43,026 > Epoch: 116,  train_loss=0.000015985,  val_loss=1.015857220,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:57:22,465 > Epoch: 117,  train_loss=0.000329895,  val_loss=0.900303245,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 17:58:01,939 > Epoch: 118,  train_loss=0.000470328,  val_loss=1.181639433,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:58:41,382 > Epoch: 119,  train_loss=0.000142824,  val_loss=1.189428329,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 17:59:20,877 > Epoch: 120,  train_loss=0.000830210,  val_loss=0.787399888,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:00:00,327 > Epoch: 121,  train_loss=0.000656508,  val_loss=0.775282860,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:00:39,833 > Epoch: 122,  train_loss=0.000650055,  val_loss=0.747723520,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:01:19,273 > Epoch: 123,  train_loss=0.000566362,  val_loss=1.152274013,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:01:58,717 > Epoch: 124,  train_loss=0.000105589,  val_loss=1.124285698,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:02:38,144 > Epoch: 125,  train_loss=0.000541870,  val_loss=0.889528513,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:03:17,614 > Epoch: 126,  train_loss=0.000465717,  val_loss=0.705696821,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:03:57,020 > Epoch: 127,  train_loss=0.000257434,  val_loss=0.852179945,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:04:36,469 > Epoch: 128,  train_loss=0.000286906,  val_loss=1.087172031,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:05:15,948 > Epoch: 129,  train_loss=0.000603443,  val_loss=1.128823519,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:05:55,459 > Epoch: 130,  train_loss=0.000394663,  val_loss=1.042594433,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:06:34,915 > Epoch: 131,  train_loss=0.000083988,  val_loss=1.101740122,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:07:14,395 > Epoch: 132,  train_loss=0.000623076,  val_loss=0.978412032,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:07:53,840 > Epoch: 133,  train_loss=0.000337880,  val_loss=0.967418313,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:08:33,317 > Epoch: 134,  train_loss=0.000507016,  val_loss=0.917900324,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:09:12,807 > Epoch: 135,  train_loss=0.000522074,  val_loss=0.834399581,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:09:52,311 > Epoch: 136,  train_loss=0.000973243,  val_loss=0.702060699,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:10:31,830 > Epoch: 137,  train_loss=0.000465393,  val_loss=0.740647674,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:11:11,331 > Epoch: 138,  train_loss=0.000437876,  val_loss=0.631324768,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:11:50,825 > Epoch: 139,  train_loss=0.000123069,  val_loss=0.761675000,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:12:30,342 > Epoch: 140,  train_loss=0.000103164,  val_loss=0.837627053,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:13:09,832 > Epoch: 141,  train_loss=0.000170572,  val_loss=0.807751417,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:13:49,347 > Epoch: 142,  train_loss=0.000246822,  val_loss=0.750909030,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:14:28,849 > Epoch: 143,  train_loss=0.000121043,  val_loss=0.892066181,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:15:08,324 > Epoch: 144,  train_loss=0.000110051,  val_loss=1.090647936,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:15:47,820 > Epoch: 145,  train_loss=0.000129928,  val_loss=1.078745127,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 18:16:27,326 > Epoch: 146,  train_loss=0.000136933,  val_loss=1.070017099,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 18:17:06,807 > Epoch: 147,  train_loss=0.000267188,  val_loss=1.012556076,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 18:17:46,311 > Epoch: 148,  train_loss=0.000148090,  val_loss=0.942613780,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 18:18:25,775 > Epoch: 149,  train_loss=0.000346025,  val_loss=0.958416700,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 18:19:05,261 > Epoch: 150,  train_loss=0.000078737,  val_loss=1.026973128,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:19:44,746 > Epoch: 151,  train_loss=0.000375636,  val_loss=0.841776729,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:20:24,273 > Epoch: 152,  train_loss=0.000170413,  val_loss=0.790285349,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:21:03,769 > Epoch: 153,  train_loss=0.000679921,  val_loss=0.814857602,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:21:43,262 > Epoch: 154,  train_loss=0.000238066,  val_loss=0.819070935,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:22:22,813 > Epoch: 155,  train_loss=0.000406851,  val_loss=0.786042333,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:23:02,324 > Epoch: 156,  train_loss=0.000221942,  val_loss=0.867336392,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:23:41,832 > Epoch: 157,  train_loss=0.000050875,  val_loss=0.914688826,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:24:21,343 > Epoch: 158,  train_loss=0.000330198,  val_loss=0.868718386,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:25:00,813 > Epoch: 159,  train_loss=0.000298255,  val_loss=0.830431461,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:25:40,295 > Epoch: 160,  train_loss=0.000261093,  val_loss=0.920778751,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:26:19,796 > Epoch: 161,  train_loss=0.000414690,  val_loss=0.886685491,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:26:59,287 > Epoch: 162,  train_loss=0.000183689,  val_loss=0.925477982,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:27:38,736 > Epoch: 163,  train_loss=0.000184499,  val_loss=0.936812878,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:28:18,219 > Epoch: 164,  train_loss=0.000215103,  val_loss=0.893980384,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:28:57,707 > Epoch: 165,  train_loss=0.000376029,  val_loss=0.775794685,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:29:37,217 > Epoch: 166,  train_loss=0.000358662,  val_loss=0.748571575,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:30:16,752 > Epoch: 167,  train_loss=0.000088848,  val_loss=0.788420022,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:30:56,248 > Epoch: 168,  train_loss=0.000136449,  val_loss=0.848478496,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:31:35,745 > Epoch: 169,  train_loss=0.000277558,  val_loss=0.809432089,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:32:15,252 > Epoch: 170,  train_loss=0.000291700,  val_loss=0.747203648,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:32:54,779 > Epoch: 171,  train_loss=0.000206745,  val_loss=0.728061080,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:33:34,248 > Epoch: 172,  train_loss=0.000106780,  val_loss=0.715031385,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:34:13,740 > Epoch: 173,  train_loss=0.000178056,  val_loss=0.690318346,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:34:53,258 > Epoch: 174,  train_loss=0.000131786,  val_loss=0.711718738,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:35:32,762 > Epoch: 175,  train_loss=0.000187086,  val_loss=0.656417012,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:36:12,275 > Epoch: 176,  train_loss=0.000164992,  val_loss=0.737992048,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 18:36:51,770 > Epoch: 177,  train_loss=0.000227319,  val_loss=0.709635735,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:37:31,284 > Epoch: 178,  train_loss=0.000122634,  val_loss=0.702868819,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:38:10,793 > Epoch: 179,  train_loss=0.000196069,  val_loss=0.714885592,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:38:50,288 > Epoch: 180,  train_loss=0.000110300,  val_loss=0.759858072,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:39:29,810 > Epoch: 181,  train_loss=0.000165703,  val_loss=0.755803287,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:40:09,324 > Epoch: 182,  train_loss=0.000103606,  val_loss=0.766358495,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:40:48,827 > Epoch: 183,  train_loss=0.000086575,  val_loss=0.788287818,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:41:28,340 > Epoch: 184,  train_loss=0.000198311,  val_loss=0.759656131,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:42:07,843 > Epoch: 185,  train_loss=0.000553347,  val_loss=0.797440827,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:42:47,347 > Epoch: 186,  train_loss=0.000097922,  val_loss=0.803965926,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:43:26,855 > Epoch: 187,  train_loss=0.000124450,  val_loss=0.801245332,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:44:06,373 > Epoch: 188,  train_loss=0.000111651,  val_loss=0.793567121,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:44:45,890 > Epoch: 189,  train_loss=0.000190587,  val_loss=0.790260434,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:45:25,387 > Epoch: 190,  train_loss=0.000372341,  val_loss=0.742179871,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:46:04,890 > Epoch: 191,  train_loss=0.000183706,  val_loss=0.743683338,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:46:44,409 > Epoch: 192,  train_loss=0.000118163,  val_loss=0.744630039,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:47:23,921 > Epoch: 193,  train_loss=0.000164922,  val_loss=0.744196773,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:48:03,479 > Epoch: 194,  train_loss=0.000086831,  val_loss=0.744119883,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:48:42,958 > Epoch: 195,  train_loss=0.000163407,  val_loss=0.749979734,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:49:22,456 > Epoch: 196,  train_loss=0.000152231,  val_loss=0.749006450,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:50:01,964 > Epoch: 197,  train_loss=0.000098154,  val_loss=0.751539469,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:50:41,436 > Epoch: 198,  train_loss=0.000260368,  val_loss=0.747716188,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:51:20,948 > Epoch: 199,  train_loss=0.000223522,  val_loss=0.743912637,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:52:00,442 > Epoch: 200,  train_loss=0.000123867,  val_loss=0.743872046,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:52:39,926 > Epoch: 201,  train_loss=0.000155667,  val_loss=0.747596562,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:53:19,437 > Epoch: 202,  train_loss=0.000111519,  val_loss=0.747412205,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:53:58,933 > Epoch: 203,  train_loss=0.000177587,  val_loss=0.746292651,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:54:38,416 > Epoch: 204,  train_loss=0.000207599,  val_loss=0.744029939,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:55:17,901 > Epoch: 205,  train_loss=0.000089233,  val_loss=0.745245039,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:55:57,405 > Epoch: 206,  train_loss=0.000209389,  val_loss=0.744542301,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:56:36,920 > Epoch: 207,  train_loss=0.000221558,  val_loss=0.744991601,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:57:16,407 > Epoch: 208,  train_loss=0.000100974,  val_loss=0.745587111,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:57:55,899 > Epoch: 209,  train_loss=0.000128316,  val_loss=0.745881379,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:58:35,427 > Epoch: 210,  train_loss=0.000122225,  val_loss=0.745834827,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:59:14,908 > Epoch: 211,  train_loss=0.000351091,  val_loss=0.785292983,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 18:59:54,387 > Epoch: 212,  train_loss=0.000402675,  val_loss=1.032075763,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 19:00:33,849 > Epoch: 213,  train_loss=0.000528813,  val_loss=0.932514668,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:01:13,316 > Epoch: 214,  train_loss=0.000184617,  val_loss=1.243009686,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:01:52,817 > Epoch: 215,  train_loss=0.001507051,  val_loss=1.653892517,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 19:02:32,298 > Epoch: 216,  train_loss=0.000848855,  val_loss=2.095679998,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 19:03:11,798 > Epoch: 217,  train_loss=0.008734078,  val_loss=1.745297194,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:03:51,305 > Epoch: 218,  train_loss=0.006753103,  val_loss=2.693832397,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:04:30,799 > Epoch: 219,  train_loss=0.001051438,  val_loss=2.943811893,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:05:10,287 > Epoch: 220,  train_loss=0.001654385,  val_loss=2.601584673,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:05:49,778 > Epoch: 221,  train_loss=0.002468345,  val_loss=2.091864586,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:06:29,269 > Epoch: 222,  train_loss=0.001699070,  val_loss=2.372861385,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:07:08,765 > Epoch: 223,  train_loss=0.001292764,  val_loss=1.908082247,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:07:48,262 > Epoch: 224,  train_loss=0.000012667,  val_loss=1.907934666,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:08:27,738 > Epoch: 225,  train_loss=0.001860288,  val_loss=2.053118229,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:09:07,251 > Epoch: 226,  train_loss=0.000403958,  val_loss=2.012858152,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:09:46,788 > Epoch: 227,  train_loss=0.000850207,  val_loss=1.783002973,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:10:26,283 > Epoch: 228,  train_loss=0.001669976,  val_loss=1.715326548,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:11:05,768 > Epoch: 229,  train_loss=0.001193515,  val_loss=1.756189108,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:11:45,263 > Epoch: 230,  train_loss=0.000949688,  val_loss=1.398753881,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:12:24,741 > Epoch: 231,  train_loss=0.000268517,  val_loss=1.776216149,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:13:04,224 > Epoch: 232,  train_loss=0.000603794,  val_loss=1.615499258,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:13:43,688 > Epoch: 233,  train_loss=0.000588720,  val_loss=1.605717659,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:14:23,175 > Epoch: 234,  train_loss=0.000803436,  val_loss=1.273504257,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:15:02,677 > Epoch: 235,  train_loss=0.000351886,  val_loss=1.144137621,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:15:42,185 > Epoch: 236,  train_loss=0.000670396,  val_loss=1.373353481,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:16:21,686 > Epoch: 237,  train_loss=0.000250077,  val_loss=1.344172716,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:17:01,163 > Epoch: 238,  train_loss=0.000938849,  val_loss=1.973231316,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 19:17:40,624 > Epoch: 239,  train_loss=0.000304532,  val_loss=1.883347034,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:18:20,104 > Epoch: 240,  train_loss=0.000306611,  val_loss=1.745741725,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:18:59,601 > Epoch: 241,  train_loss=0.000552253,  val_loss=1.404652119,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:19:39,084 > Epoch: 242,  train_loss=0.000272271,  val_loss=1.603876114,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:20:18,543 > Epoch: 243,  train_loss=0.000630692,  val_loss=1.363917112,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:20:58,018 > Epoch: 244,  train_loss=0.000058953,  val_loss=1.369920969,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:21:37,540 > Epoch: 245,  train_loss=0.000182089,  val_loss=1.469627023,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:22:17,006 > Epoch: 246,  train_loss=0.000219801,  val_loss=1.361053705,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:22:56,434 > Epoch: 247,  train_loss=0.000568701,  val_loss=1.424321055,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:23:35,860 > Epoch: 248,  train_loss=0.000280867,  val_loss=1.310774684,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:24:15,330 > Epoch: 249,  train_loss=0.000279346,  val_loss=1.204968691,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:24:54,799 > Epoch: 250,  train_loss=0.000293311,  val_loss=1.089261055,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:25:34,267 > Epoch: 251,  train_loss=0.000220322,  val_loss=1.174675941,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:26:13,703 > Epoch: 252,  train_loss=0.000390589,  val_loss=1.037650585,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:26:53,121 > Epoch: 253,  train_loss=0.000297473,  val_loss=0.913008332,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:27:32,557 > Epoch: 254,  train_loss=0.000245802,  val_loss=0.864202619,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:28:11,972 > Epoch: 255,  train_loss=0.000497639,  val_loss=0.695335329,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:28:51,419 > Epoch: 256,  train_loss=0.000335118,  val_loss=0.667404652,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:29:30,844 > Epoch: 257,  train_loss=0.000095933,  val_loss=0.756822586,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:30:10,271 > Epoch: 258,  train_loss=0.000813050,  val_loss=0.956943989,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:30:49,777 > Epoch: 259,  train_loss=0.000846733,  val_loss=1.204100847,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:31:29,244 > Epoch: 260,  train_loss=0.000461474,  val_loss=0.994016767,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:32:08,707 > Epoch: 261,  train_loss=0.000248115,  val_loss=0.977895260,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:32:48,149 > Epoch: 262,  train_loss=0.000677568,  val_loss=0.750512481,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 19:33:27,584 > Epoch: 263,  train_loss=0.000168049,  val_loss=0.939581990,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:34:07,011 > Epoch: 264,  train_loss=0.000160124,  val_loss=0.972328782,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:34:46,451 > Epoch: 265,  train_loss=0.000600717,  val_loss=0.826106191,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:35:25,873 > Epoch: 266,  train_loss=0.000334722,  val_loss=0.882712245,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:36:05,302 > Epoch: 267,  train_loss=0.000183479,  val_loss=0.986355782,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:36:44,742 > Epoch: 268,  train_loss=0.000379132,  val_loss=1.157585025,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:37:24,149 > Epoch: 269,  train_loss=0.000099807,  val_loss=1.258019209,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:38:03,558 > Epoch: 270,  train_loss=0.000508419,  val_loss=0.883512795,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:38:43,014 > Epoch: 271,  train_loss=0.000309971,  val_loss=0.880273104,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:39:22,471 > Epoch: 272,  train_loss=0.000339634,  val_loss=0.906811833,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:40:01,897 > Epoch: 273,  train_loss=0.000100623,  val_loss=0.977751613,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:40:41,326 > Epoch: 274,  train_loss=0.000721462,  val_loss=1.488185167,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:41:20,799 > Epoch: 275,  train_loss=0.000321365,  val_loss=1.633221388,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 19:42:00,280 > Epoch: 276,  train_loss=0.000761544,  val_loss=1.865288496,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:42:39,742 > Epoch: 277,  train_loss=0.000265721,  val_loss=1.740053892,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:43:19,201 > Epoch: 278,  train_loss=0.000500895,  val_loss=2.026259422,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:43:58,626 > Epoch: 279,  train_loss=0.000978053,  val_loss=2.340999126,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:44:38,095 > Epoch: 280,  train_loss=0.001112639,  val_loss=2.234179258,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:45:17,567 > Epoch: 281,  train_loss=0.000881422,  val_loss=1.876594901,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:45:57,026 > Epoch: 282,  train_loss=0.000975378,  val_loss=2.186846733,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:46:36,489 > Epoch: 283,  train_loss=0.001838564,  val_loss=1.393473744,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:47:15,954 > Epoch: 284,  train_loss=0.000081423,  val_loss=1.465913534,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:47:55,414 > Epoch: 285,  train_loss=0.000535973,  val_loss=1.397300005,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:48:34,882 > Epoch: 286,  train_loss=0.001153854,  val_loss=1.767124414,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:49:14,359 > Epoch: 287,  train_loss=0.000458507,  val_loss=1.688809276,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:49:53,805 > Epoch: 288,  train_loss=0.000426731,  val_loss=1.548444390,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 19:50:33,268 > Epoch: 289,  train_loss=0.000361658,  val_loss=1.444385886,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:51:12,671 > Epoch: 290,  train_loss=0.000201991,  val_loss=1.417483568,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:51:52,102 > Epoch: 291,  train_loss=0.000195134,  val_loss=1.408351779,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:52:31,532 > Epoch: 292,  train_loss=0.000233076,  val_loss=1.436604261,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:53:10,954 > Epoch: 293,  train_loss=0.000235353,  val_loss=1.389114738,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:53:50,377 > Epoch: 294,  train_loss=0.000325032,  val_loss=1.268978477,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:54:29,812 > Epoch: 295,  train_loss=0.000071364,  val_loss=1.361414194,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:55:09,249 > Epoch: 296,  train_loss=0.000107233,  val_loss=1.435439229,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:55:48,698 > Epoch: 297,  train_loss=0.000188970,  val_loss=1.314236283,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:56:28,140 > Epoch: 298,  train_loss=0.000284137,  val_loss=1.817673206,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:57:07,575 > Epoch: 299,  train_loss=0.000581337,  val_loss=1.596648932,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:57:47,017 > Epoch: 300,  train_loss=0.000265012,  val_loss=2.150007486,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:58:26,481 > Epoch: 301,  train_loss=0.000027274,  val_loss=2.142970800,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:59:05,939 > Epoch: 302,  train_loss=0.000250029,  val_loss=1.974778652,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 19:59:45,403 > Epoch: 303,  train_loss=0.000491113,  val_loss=1.494612813,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:00:24,883 > Epoch: 304,  train_loss=0.000128333,  val_loss=1.544345379,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:01:04,360 > Epoch: 305,  train_loss=0.000171415,  val_loss=1.550819874,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:01:43,815 > Epoch: 306,  train_loss=0.000040076,  val_loss=1.809098125,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:02:23,282 > Epoch: 307,  train_loss=0.000789862,  val_loss=1.371312857,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:03:02,715 > Epoch: 308,  train_loss=0.000669354,  val_loss=1.594754219,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:03:42,142 > Epoch: 309,  train_loss=0.000198686,  val_loss=1.436885953,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:04:21,568 > Epoch: 310,  train_loss=0.000376705,  val_loss=1.578254938,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:05:01,050 > Epoch: 311,  train_loss=0.000195474,  val_loss=1.493493199,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 20:05:40,549 > Epoch: 312,  train_loss=0.000113197,  val_loss=1.410246134,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:06:20,043 > Epoch: 313,  train_loss=0.000161580,  val_loss=1.688801169,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:06:59,530 > Epoch: 314,  train_loss=0.000032569,  val_loss=1.726540804,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:07:39,011 > Epoch: 315,  train_loss=0.000126024,  val_loss=1.685761213,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:08:18,479 > Epoch: 316,  train_loss=0.000160291,  val_loss=1.606641054,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:08:57,959 > Epoch: 317,  train_loss=0.000218809,  val_loss=1.758225679,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:09:37,444 > Epoch: 318,  train_loss=0.000272206,  val_loss=1.558205485,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:10:16,935 > Epoch: 319,  train_loss=0.000343459,  val_loss=1.374627829,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:10:56,435 > Epoch: 320,  train_loss=0.000146903,  val_loss=1.507307291,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:11:35,928 > Epoch: 321,  train_loss=0.000474149,  val_loss=1.453883410,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:12:15,421 > Epoch: 322,  train_loss=0.000147823,  val_loss=1.436884642,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:12:54,909 > Epoch: 323,  train_loss=0.000155909,  val_loss=1.435181379,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 20:13:34,381 > Epoch: 324,  train_loss=0.000139060,  val_loss=1.392516136,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 20:14:13,839 > Epoch: 325,  train_loss=0.000134974,  val_loss=1.524447083,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:14:53,305 > Epoch: 326,  train_loss=0.000041403,  val_loss=1.603105545,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:15:32,805 > Epoch: 327,  train_loss=0.000294092,  val_loss=1.533033848,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:16:12,280 > Epoch: 328,  train_loss=0.000521843,  val_loss=1.472400427,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:16:51,772 > Epoch: 329,  train_loss=0.000343840,  val_loss=1.213693976,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:17:31,285 > Epoch: 330,  train_loss=0.000767909,  val_loss=1.461214542,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 20:18:10,784 > Epoch: 331,  train_loss=0.000207651,  val_loss=1.225992680,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 20:18:50,314 > Epoch: 332,  train_loss=0.000317545,  val_loss=1.150940895,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 20:19:29,816 > Epoch: 333,  train_loss=0.000213380,  val_loss=1.350981832,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:20:09,312 > Epoch: 334,  train_loss=0.000175967,  val_loss=1.292338610,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:20:48,808 > Epoch: 335,  train_loss=0.000240836,  val_loss=1.215679765,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:21:28,272 > Epoch: 336,  train_loss=0.000598760,  val_loss=1.121675491,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 20:22:07,752 > Epoch: 337,  train_loss=0.000498546,  val_loss=0.935422182,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 20:22:47,184 > Epoch: 338,  train_loss=0.000221414,  val_loss=1.158815503,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:23:26,618 > Epoch: 339,  train_loss=0.000104801,  val_loss=1.109541535,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:24:06,058 > Epoch: 340,  train_loss=0.000170693,  val_loss=1.073002934,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:24:45,493 > Epoch: 341,  train_loss=0.000174025,  val_loss=1.009301543,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:25:24,922 > Epoch: 342,  train_loss=0.000255322,  val_loss=0.938062847,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:26:04,356 > Epoch: 343,  train_loss=0.000078322,  val_loss=0.939779282,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:26:43,786 > Epoch: 344,  train_loss=0.000193704,  val_loss=0.955356956,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:27:23,209 > Epoch: 345,  train_loss=0.000394391,  val_loss=1.029560566,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:28:02,640 > Epoch: 346,  train_loss=0.001065300,  val_loss=0.935943544,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:28:42,062 > Epoch: 347,  train_loss=0.000098842,  val_loss=0.927901983,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:29:21,480 > Epoch: 348,  train_loss=0.000081985,  val_loss=0.952711701,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:30:00,905 > Epoch: 349,  train_loss=0.000188301,  val_loss=0.959330916,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:30:40,335 > Epoch: 350,  train_loss=0.000072586,  val_loss=0.994161010,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:31:19,754 > Epoch: 351,  train_loss=0.000159262,  val_loss=0.882759571,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:31:59,193 > Epoch: 352,  train_loss=0.000058424,  val_loss=0.908030152,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:32:38,625 > Epoch: 353,  train_loss=0.000167568,  val_loss=0.926560581,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:33:18,038 > Epoch: 354,  train_loss=0.000120914,  val_loss=0.953190982,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:33:57,457 > Epoch: 355,  train_loss=0.001435293,  val_loss=1.122489214,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:34:36,903 > Epoch: 356,  train_loss=0.000179315,  val_loss=1.040539503,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:35:16,325 > Epoch: 357,  train_loss=0.000118770,  val_loss=1.069637179,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 20:35:55,749 > Epoch: 358,  train_loss=0.000126173,  val_loss=1.115105391,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 20:36:35,167 > Epoch: 359,  train_loss=0.000217273,  val_loss=1.057608128,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 20:37:14,579 > Epoch: 360,  train_loss=0.000304491,  val_loss=0.978603005,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 20:37:54,008 > Epoch: 361,  train_loss=0.000120441,  val_loss=1.076935053,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:38:33,439 > Epoch: 362,  train_loss=0.000105579,  val_loss=1.094909191,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:39:12,857 > Epoch: 363,  train_loss=0.000145016,  val_loss=1.073692799,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:39:52,287 > Epoch: 364,  train_loss=0.000101574,  val_loss=1.066019535,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:40:31,733 > Epoch: 365,  train_loss=0.000175892,  val_loss=1.036358356,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:41:11,153 > Epoch: 366,  train_loss=0.000145961,  val_loss=1.017119288,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:41:50,587 > Epoch: 367,  train_loss=0.000057378,  val_loss=1.054692388,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:42:30,024 > Epoch: 368,  train_loss=0.000277739,  val_loss=0.959707379,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:43:09,458 > Epoch: 369,  train_loss=0.000142681,  val_loss=0.944044232,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:43:48,881 > Epoch: 370,  train_loss=0.000172826,  val_loss=0.986478090,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:44:28,315 > Epoch: 371,  train_loss=0.000165865,  val_loss=0.942037404,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:45:07,746 > Epoch: 372,  train_loss=0.000089559,  val_loss=0.942014456,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:45:47,181 > Epoch: 373,  train_loss=0.000171111,  val_loss=0.963393271,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:46:26,603 > Epoch: 374,  train_loss=0.000014504,  val_loss=1.012813568,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:47:06,028 > Epoch: 375,  train_loss=0.000148320,  val_loss=1.072176933,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:47:45,458 > Epoch: 376,  train_loss=0.000257409,  val_loss=1.202934384,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:48:24,897 > Epoch: 377,  train_loss=0.000213202,  val_loss=1.181605697,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:49:04,306 > Epoch: 378,  train_loss=0.000404090,  val_loss=1.027642727,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:49:43,719 > Epoch: 379,  train_loss=0.000196957,  val_loss=0.993092477,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:50:23,136 > Epoch: 380,  train_loss=0.000221648,  val_loss=0.923472643,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:51:02,549 > Epoch: 381,  train_loss=0.000113832,  val_loss=0.925893247,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:51:41,973 > Epoch: 382,  train_loss=0.000462248,  val_loss=1.024641752,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:52:21,400 > Epoch: 383,  train_loss=0.000299411,  val_loss=0.991609871,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:53:00,822 > Epoch: 384,  train_loss=0.000073716,  val_loss=1.002461910,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:53:40,264 > Epoch: 385,  train_loss=0.000191300,  val_loss=0.975883961,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:54:19,720 > Epoch: 386,  train_loss=0.000196602,  val_loss=0.989018440,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:54:59,156 > Epoch: 387,  train_loss=0.000088007,  val_loss=0.989645779,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:55:38,596 > Epoch: 388,  train_loss=0.000299762,  val_loss=0.955283046,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:56:18,008 > Epoch: 389,  train_loss=0.000117055,  val_loss=0.942664981,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:56:57,445 > Epoch: 390,  train_loss=0.000134193,  val_loss=0.926144719,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:57:36,868 > Epoch: 391,  train_loss=0.000066085,  val_loss=0.917816699,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:58:16,296 > Epoch: 392,  train_loss=0.000190944,  val_loss=0.884681702,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 20:58:55,707 > Epoch: 393,  train_loss=0.000351314,  val_loss=0.886901498,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 20:59:35,139 > Epoch: 394,  train_loss=0.000076214,  val_loss=0.905431032,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:00:14,562 > Epoch: 395,  train_loss=0.000175375,  val_loss=0.889708996,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:00:54,041 > Epoch: 396,  train_loss=0.000147458,  val_loss=0.892277777,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:01:33,469 > Epoch: 397,  train_loss=0.000091962,  val_loss=0.889736652,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:02:12,877 > Epoch: 398,  train_loss=0.000128900,  val_loss=0.926023185,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:02:52,308 > Epoch: 399,  train_loss=0.000154279,  val_loss=0.951593399,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:03:31,710 > Epoch: 400,  train_loss=0.000139979,  val_loss=0.956833363,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:04:11,134 > Epoch: 401,  train_loss=0.000121329,  val_loss=0.978466034,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:04:50,554 > Epoch: 402,  train_loss=0.000162317,  val_loss=0.956233263,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:05:29,990 > Epoch: 403,  train_loss=0.000121042,  val_loss=0.960895658,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:06:09,402 > Epoch: 404,  train_loss=0.000229441,  val_loss=0.920581996,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:06:48,825 > Epoch: 405,  train_loss=0.000084584,  val_loss=0.933586359,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:07:28,240 > Epoch: 406,  train_loss=0.000112952,  val_loss=0.939521968,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:08:07,673 > Epoch: 407,  train_loss=0.000229099,  val_loss=0.918671727,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:08:47,080 > Epoch: 408,  train_loss=0.000151313,  val_loss=0.908688724,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:09:26,512 > Epoch: 409,  train_loss=0.000296628,  val_loss=0.958099067,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:10:05,928 > Epoch: 410,  train_loss=0.000095859,  val_loss=0.962401330,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:10:45,353 > Epoch: 411,  train_loss=0.000087992,  val_loss=0.959779322,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:11:24,784 > Epoch: 412,  train_loss=0.000169087,  val_loss=0.946936905,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:12:04,216 > Epoch: 413,  train_loss=0.000135433,  val_loss=0.935548782,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:12:43,635 > Epoch: 414,  train_loss=0.000210605,  val_loss=0.952484012,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:13:23,051 > Epoch: 415,  train_loss=0.000109730,  val_loss=0.952337027,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:14:02,475 > Epoch: 416,  train_loss=0.000147990,  val_loss=0.942883313,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:14:41,890 > Epoch: 417,  train_loss=0.000144014,  val_loss=0.940021753,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:15:21,293 > Epoch: 418,  train_loss=0.000112432,  val_loss=0.940586686,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:16:00,719 > Epoch: 419,  train_loss=0.000250990,  val_loss=0.941404581,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:16:40,137 > Epoch: 420,  train_loss=0.000092212,  val_loss=0.942267060,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:17:19,581 > Epoch: 421,  train_loss=0.000081611,  val_loss=0.946673989,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:17:58,981 > Epoch: 422,  train_loss=0.000116034,  val_loss=0.945985198,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:18:38,411 > Epoch: 423,  train_loss=0.000121744,  val_loss=0.943455994,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:19:17,831 > Epoch: 424,  train_loss=0.000075915,  val_loss=0.942643464,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:19:57,242 > Epoch: 425,  train_loss=0.000096795,  val_loss=0.943380415,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:20:36,666 > Epoch: 426,  train_loss=0.000088679,  val_loss=0.949940085,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:21:16,117 > Epoch: 427,  train_loss=0.000141421,  val_loss=0.951343179,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:21:55,532 > Epoch: 428,  train_loss=0.000133424,  val_loss=0.953531086,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:22:34,962 > Epoch: 429,  train_loss=0.000053839,  val_loss=0.957018852,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:23:14,407 > Epoch: 430,  train_loss=0.000162000,  val_loss=0.956557512,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:23:53,845 > Epoch: 431,  train_loss=0.000141828,  val_loss=0.975044727,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:24:33,270 > Epoch: 432,  train_loss=0.000111545,  val_loss=0.973544717,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:25:12,680 > Epoch: 433,  train_loss=0.000126239,  val_loss=0.973286211,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:25:52,121 > Epoch: 434,  train_loss=0.000060161,  val_loss=0.980057418,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:26:31,530 > Epoch: 435,  train_loss=0.000104161,  val_loss=0.980226040,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:27:10,973 > Epoch: 436,  train_loss=0.000119864,  val_loss=0.978445292,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:27:50,394 > Epoch: 437,  train_loss=0.000171755,  val_loss=0.974884033,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:28:29,819 > Epoch: 438,  train_loss=0.000107716,  val_loss=0.975531220,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:29:09,236 > Epoch: 439,  train_loss=0.000111540,  val_loss=0.975009739,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:29:48,664 > Epoch: 440,  train_loss=0.000076164,  val_loss=0.977617860,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:30:28,103 > Epoch: 441,  train_loss=0.000144689,  val_loss=0.978024125,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:31:07,542 > Epoch: 442,  train_loss=0.000155822,  val_loss=0.977110922,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:31:46,976 > Epoch: 443,  train_loss=0.000159438,  val_loss=0.976326585,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:32:26,414 > Epoch: 444,  train_loss=0.000200160,  val_loss=0.974593520,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:33:05,867 > Epoch: 445,  train_loss=0.000080612,  val_loss=0.974323511,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:33:45,319 > Epoch: 446,  train_loss=0.000064116,  val_loss=0.974814415,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:34:24,752 > Epoch: 447,  train_loss=0.000237493,  val_loss=0.973955870,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:35:04,174 > Epoch: 448,  train_loss=0.000086861,  val_loss=0.974194527,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:35:43,599 > Epoch: 449,  train_loss=0.000057741,  val_loss=0.974374592,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:36:23,025 > Epoch: 450,  train_loss=0.000232405,  val_loss=0.974291682,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:37:02,455 > Epoch: 451,  train_loss=0.000059508,  val_loss=0.974298477,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:37:41,893 > Epoch: 452,  train_loss=0.000075066,  val_loss=1.135391951,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:38:21,321 > Epoch: 453,  train_loss=0.000159295,  val_loss=1.065963745,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:39:00,800 > Epoch: 454,  train_loss=0.001381084,  val_loss=2.066827536,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:39:40,281 > Epoch: 455,  train_loss=0.002839359,  val_loss=2.226925135,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:40:19,756 > Epoch: 456,  train_loss=0.001074441,  val_loss=2.496208906,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:40:59,225 > Epoch: 457,  train_loss=0.001998330,  val_loss=3.020258904,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 21:41:38,725 > Epoch: 458,  train_loss=0.000205799,  val_loss=2.660390377,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:42:18,189 > Epoch: 459,  train_loss=0.000442389,  val_loss=2.923751354,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:42:57,673 > Epoch: 460,  train_loss=0.000715490,  val_loss=3.220969677,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:43:37,182 > Epoch: 461,  train_loss=0.001430424,  val_loss=3.498954773,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:44:16,668 > Epoch: 462,  train_loss=0.000039490,  val_loss=3.667711973,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:44:56,139 > Epoch: 463,  train_loss=0.000697087,  val_loss=3.171000242,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:45:35,641 > Epoch: 464,  train_loss=0.001105814,  val_loss=3.160826206,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 21:46:15,112 > Epoch: 465,  train_loss=0.000893717,  val_loss=3.550421715,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 21:46:54,600 > Epoch: 466,  train_loss=0.000455841,  val_loss=3.143585205,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:47:34,085 > Epoch: 467,  train_loss=0.001277826,  val_loss=2.932453156,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:48:13,563 > Epoch: 468,  train_loss=0.000058509,  val_loss=2.948604107,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:48:53,033 > Epoch: 469,  train_loss=0.000193434,  val_loss=3.029684067,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:49:32,482 > Epoch: 470,  train_loss=0.001635894,  val_loss=3.597078323,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:50:11,952 > Epoch: 471,  train_loss=0.004169012,  val_loss=4.132551670,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 21:50:51,417 > Epoch: 472,  train_loss=0.000079158,  val_loss=5.417916298,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:51:30,869 > Epoch: 473,  train_loss=0.002091630,  val_loss=4.499187469,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:52:10,327 > Epoch: 474,  train_loss=0.000234866,  val_loss=4.280464172,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:52:49,805 > Epoch: 475,  train_loss=0.001094458,  val_loss=4.625108719,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 21:53:29,290 > Epoch: 476,  train_loss=0.001659291,  val_loss=3.344114065,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:54:08,774 > Epoch: 477,  train_loss=0.000647105,  val_loss=4.459400177,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:54:48,247 > Epoch: 478,  train_loss=0.000785855,  val_loss=5.795097351,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 21:55:27,705 > Epoch: 479,  train_loss=0.000745649,  val_loss=5.179005623,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 21:56:07,149 > Epoch: 480,  train_loss=0.000260047,  val_loss=5.142785549,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 21:56:46,619 > Epoch: 481,  train_loss=0.001106499,  val_loss=4.286112785,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 21:57:26,084 > Epoch: 482,  train_loss=0.000918832,  val_loss=3.535394192,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 21:58:05,553 > Epoch: 483,  train_loss=0.000639917,  val_loss=2.738313198,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 21:58:45,030 > Epoch: 484,  train_loss=0.000773150,  val_loss=2.833604813,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 21:59:24,501 > Epoch: 485,  train_loss=0.000105528,  val_loss=3.092412472,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:00:03,977 > Epoch: 486,  train_loss=0.000324643,  val_loss=2.719483614,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:00:43,441 > Epoch: 487,  train_loss=0.000035877,  val_loss=2.853939533,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:01:22,894 > Epoch: 488,  train_loss=0.000749685,  val_loss=2.992190838,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:02:02,366 > Epoch: 489,  train_loss=0.000570697,  val_loss=3.945951700,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:02:41,850 > Epoch: 490,  train_loss=0.000155888,  val_loss=2.967021704,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:03:21,313 > Epoch: 491,  train_loss=0.002968595,  val_loss=4.348235130,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:04:00,785 > Epoch: 492,  train_loss=0.002438283,  val_loss=6.402276993,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:04:40,282 > Epoch: 493,  train_loss=0.011197700,  val_loss=3.008495331,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:05:19,759 > Epoch: 494,  train_loss=0.000616104,  val_loss=3.223309994,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:05:59,233 > Epoch: 495,  train_loss=0.002628952,  val_loss=2.678610086,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:06:38,673 > Epoch: 496,  train_loss=0.002501278,  val_loss=2.132556200,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:07:18,130 > Epoch: 497,  train_loss=0.002258615,  val_loss=2.100691795,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:07:57,577 > Epoch: 498,  train_loss=0.000455990,  val_loss=2.012735844,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:08:37,028 > Epoch: 499,  train_loss=0.000432367,  val_loss=2.671165466,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:09:16,471 > Epoch: 500,  train_loss=0.001801730,  val_loss=2.302560568,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:09:55,940 > Epoch: 501,  train_loss=0.000489895,  val_loss=2.747487545,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:10:35,352 > Epoch: 502,  train_loss=0.000537514,  val_loss=2.563637972,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:11:14,793 > Epoch: 503,  train_loss=0.000384512,  val_loss=2.400254965,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:11:54,222 > Epoch: 504,  train_loss=0.000374232,  val_loss=2.336055994,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:12:33,657 > Epoch: 505,  train_loss=0.000130961,  val_loss=2.288829088,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:13:13,115 > Epoch: 506,  train_loss=0.000377868,  val_loss=2.184383392,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:13:52,583 > Epoch: 507,  train_loss=0.000766082,  val_loss=2.805126190,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:14:32,054 > Epoch: 508,  train_loss=0.000404726,  val_loss=2.762367249,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:15:11,514 > Epoch: 509,  train_loss=0.000137366,  val_loss=2.620694876,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:15:50,956 > Epoch: 510,  train_loss=0.000369321,  val_loss=2.538467407,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:16:30,417 > Epoch: 511,  train_loss=0.000134678,  val_loss=2.423162937,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:17:09,882 > Epoch: 512,  train_loss=0.001230629,  val_loss=3.109401226,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:17:49,322 > Epoch: 513,  train_loss=0.000810769,  val_loss=2.859377861,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:18:28,796 > Epoch: 514,  train_loss=0.000423430,  val_loss=2.803311825,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:19:08,273 > Epoch: 515,  train_loss=0.002177069,  val_loss=2.490664005,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:19:47,765 > Epoch: 516,  train_loss=0.000584231,  val_loss=2.139527321,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 22:20:27,259 > Epoch: 517,  train_loss=0.000472517,  val_loss=1.891786814,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 22:21:06,735 > Epoch: 518,  train_loss=0.000355053,  val_loss=2.571437120,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:21:46,217 > Epoch: 519,  train_loss=0.000942725,  val_loss=2.660557270,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:22:25,683 > Epoch: 520,  train_loss=0.001659743,  val_loss=2.465997696,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 22:23:05,153 > Epoch: 521,  train_loss=0.000349166,  val_loss=2.407006979,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 22:23:44,655 > Epoch: 522,  train_loss=0.000170671,  val_loss=2.228195667,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 22:24:24,125 > Epoch: 523,  train_loss=0.001278298,  val_loss=1.703762054,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-23 22:25:03,620 > Epoch: 524,  train_loss=0.001207481,  val_loss=2.563734293,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:25:43,112 > Epoch: 525,  train_loss=0.000336372,  val_loss=2.685655594,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:26:22,594 > Epoch: 526,  train_loss=0.000071010,  val_loss=2.653983116,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:27:02,091 > Epoch: 527,  train_loss=0.000627248,  val_loss=2.927558422,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:27:41,579 > Epoch: 528,  train_loss=0.000301888,  val_loss=2.985064983,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:28:21,091 > Epoch: 529,  train_loss=0.000021290,  val_loss=3.055082798,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:29:00,570 > Epoch: 530,  train_loss=0.000637027,  val_loss=2.713164568,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:29:40,046 > Epoch: 531,  train_loss=0.001630157,  val_loss=3.357525349,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:30:19,552 > Epoch: 532,  train_loss=0.000506779,  val_loss=3.004003525,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:30:59,069 > Epoch: 533,  train_loss=0.000513476,  val_loss=2.320835352,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 22:31:38,576 > Epoch: 534,  train_loss=0.005686315,  val_loss=5.699339867,  accuracy=0.914062500
[INFO|asb_main.py:507] 2018-10-23 22:32:18,043 > Epoch: 535,  train_loss=0.000298225,  val_loss=6.251268864,  accuracy=0.906250000
[INFO|asb_main.py:507] 2018-10-23 22:32:57,531 > Epoch: 536,  train_loss=0.000968615,  val_loss=5.768867493,  accuracy=0.914062500
[INFO|asb_main.py:507] 2018-10-23 22:33:36,996 > Epoch: 537,  train_loss=0.000263625,  val_loss=4.917449474,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 22:34:16,477 > Epoch: 538,  train_loss=0.000073135,  val_loss=4.941123486,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:34:55,938 > Epoch: 539,  train_loss=0.000269769,  val_loss=5.571890831,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:35:35,416 > Epoch: 540,  train_loss=0.002933075,  val_loss=3.275984287,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:36:14,935 > Epoch: 541,  train_loss=0.000509036,  val_loss=3.296791077,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:36:54,425 > Epoch: 542,  train_loss=0.001356357,  val_loss=3.343618393,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:37:33,898 > Epoch: 543,  train_loss=0.000992647,  val_loss=3.128503799,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:38:13,347 > Epoch: 544,  train_loss=0.000445098,  val_loss=3.331667423,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:38:52,824 > Epoch: 545,  train_loss=0.000174261,  val_loss=3.380644798,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:39:32,273 > Epoch: 546,  train_loss=0.000047362,  val_loss=3.475266457,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:40:11,764 > Epoch: 547,  train_loss=0.000824654,  val_loss=2.961473227,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:40:51,236 > Epoch: 548,  train_loss=0.000191718,  val_loss=2.959830284,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:41:30,796 > Epoch: 549,  train_loss=0.000334404,  val_loss=2.731892586,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:42:10,278 > Epoch: 550,  train_loss=0.000121152,  val_loss=2.428788662,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:42:49,776 > Epoch: 551,  train_loss=0.000662738,  val_loss=2.940055370,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:43:29,254 > Epoch: 552,  train_loss=0.000108817,  val_loss=3.065334082,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:44:08,738 > Epoch: 553,  train_loss=0.000936335,  val_loss=3.621966362,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:44:48,216 > Epoch: 554,  train_loss=0.001460105,  val_loss=3.093410969,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:45:27,679 > Epoch: 555,  train_loss=0.000337479,  val_loss=3.487455845,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:46:07,151 > Epoch: 556,  train_loss=0.000343734,  val_loss=3.756743908,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:46:46,615 > Epoch: 557,  train_loss=0.000083037,  val_loss=3.819694042,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:47:26,056 > Epoch: 558,  train_loss=0.000499908,  val_loss=4.620427132,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:48:05,502 > Epoch: 559,  train_loss=0.000098831,  val_loss=5.352831364,  accuracy=0.921875000
[INFO|asb_main.py:507] 2018-10-23 22:48:44,974 > Epoch: 560,  train_loss=0.000482125,  val_loss=4.833389282,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:49:24,424 > Epoch: 561,  train_loss=0.000489450,  val_loss=4.402818203,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:50:03,883 > Epoch: 562,  train_loss=0.000542417,  val_loss=4.051859379,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:50:43,350 > Epoch: 563,  train_loss=0.000120584,  val_loss=3.907170296,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:51:22,797 > Epoch: 564,  train_loss=0.000715815,  val_loss=3.970105648,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:52:02,254 > Epoch: 565,  train_loss=0.000310318,  val_loss=4.278757572,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:52:41,729 > Epoch: 566,  train_loss=0.000657970,  val_loss=4.422244072,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:53:21,186 > Epoch: 567,  train_loss=0.000071087,  val_loss=4.541384697,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:54:00,639 > Epoch: 568,  train_loss=0.000067948,  val_loss=4.609687805,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 22:54:40,099 > Epoch: 569,  train_loss=0.003127230,  val_loss=3.561162710,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:55:19,552 > Epoch: 570,  train_loss=0.000184096,  val_loss=3.739211082,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:55:59,016 > Epoch: 571,  train_loss=0.000201907,  val_loss=3.506448746,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:56:38,478 > Epoch: 572,  train_loss=0.000108720,  val_loss=3.507759809,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:57:17,942 > Epoch: 573,  train_loss=0.000270796,  val_loss=3.462548494,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:57:57,411 > Epoch: 574,  train_loss=0.000242757,  val_loss=3.375864983,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:58:36,881 > Epoch: 575,  train_loss=0.000283651,  val_loss=3.198871851,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:59:16,345 > Epoch: 576,  train_loss=0.000483480,  val_loss=3.850170374,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 22:59:55,815 > Epoch: 577,  train_loss=0.000145629,  val_loss=4.097726822,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:00:35,274 > Epoch: 578,  train_loss=0.000313508,  val_loss=3.808424473,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:01:14,770 > Epoch: 579,  train_loss=0.000545928,  val_loss=3.464812279,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:01:54,243 > Epoch: 580,  train_loss=0.000072923,  val_loss=3.924262524,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:02:33,734 > Epoch: 581,  train_loss=0.000198818,  val_loss=3.621011257,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:03:13,209 > Epoch: 582,  train_loss=0.000301331,  val_loss=3.798474312,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:03:52,651 > Epoch: 583,  train_loss=0.000104018,  val_loss=3.842029572,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:04:32,110 > Epoch: 584,  train_loss=0.000639895,  val_loss=3.754818439,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:05:11,558 > Epoch: 585,  train_loss=0.000089131,  val_loss=3.823223591,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:05:51,017 > Epoch: 586,  train_loss=0.000235569,  val_loss=5.000839233,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:06:30,454 > Epoch: 587,  train_loss=0.000396294,  val_loss=6.076534271,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:07:09,919 > Epoch: 588,  train_loss=0.001246250,  val_loss=5.864366531,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:07:49,400 > Epoch: 589,  train_loss=0.000079453,  val_loss=5.927673817,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:08:28,877 > Epoch: 590,  train_loss=0.000170650,  val_loss=5.942866325,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:09:08,329 > Epoch: 591,  train_loss=0.000727709,  val_loss=5.102517128,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:09:47,791 > Epoch: 592,  train_loss=0.001954348,  val_loss=5.252318859,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:10:27,279 > Epoch: 593,  train_loss=0.000655303,  val_loss=4.902632713,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:11:06,774 > Epoch: 594,  train_loss=0.000097010,  val_loss=5.125260830,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:11:46,249 > Epoch: 595,  train_loss=0.001111048,  val_loss=7.389630318,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:12:25,706 > Epoch: 596,  train_loss=0.001315268,  val_loss=7.866897106,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:13:05,215 > Epoch: 597,  train_loss=0.001155749,  val_loss=7.893313408,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:13:44,678 > Epoch: 598,  train_loss=0.001140680,  val_loss=8.683871269,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:14:24,148 > Epoch: 599,  train_loss=0.000613342,  val_loss=9.680939674,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:15:03,579 > Epoch: 600,  train_loss=0.001824569,  val_loss=9.890009880,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:15:42,992 > Epoch: 601,  train_loss=0.000757323,  val_loss=9.387065887,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:16:22,410 > Epoch: 602,  train_loss=0.001548265,  val_loss=8.871969223,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:17:01,810 > Epoch: 603,  train_loss=0.001002997,  val_loss=7.926382065,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:17:41,230 > Epoch: 604,  train_loss=0.000740800,  val_loss=7.508011818,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:18:20,663 > Epoch: 605,  train_loss=0.000099426,  val_loss=7.689581871,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:19:00,072 > Epoch: 606,  train_loss=0.000288848,  val_loss=8.153126717,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:19:39,498 > Epoch: 607,  train_loss=0.001251582,  val_loss=8.284893036,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:20:18,919 > Epoch: 608,  train_loss=0.000395368,  val_loss=8.048908234,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:20:58,337 > Epoch: 609,  train_loss=0.000290874,  val_loss=7.671643257,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:21:37,743 > Epoch: 610,  train_loss=0.000526636,  val_loss=7.350910664,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:22:17,160 > Epoch: 611,  train_loss=0.000061712,  val_loss=7.471739769,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:22:56,577 > Epoch: 612,  train_loss=0.000267132,  val_loss=7.407376289,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:23:35,997 > Epoch: 613,  train_loss=0.000058373,  val_loss=7.370109558,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:24:15,416 > Epoch: 614,  train_loss=0.000427667,  val_loss=8.028761864,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:24:54,829 > Epoch: 615,  train_loss=0.000182940,  val_loss=7.910624981,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:25:34,250 > Epoch: 616,  train_loss=0.000400138,  val_loss=7.740211487,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:26:13,676 > Epoch: 617,  train_loss=0.001169181,  val_loss=7.197137833,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:26:53,146 > Epoch: 618,  train_loss=0.001277996,  val_loss=7.509853363,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:27:32,642 > Epoch: 619,  train_loss=0.000266172,  val_loss=7.687717438,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:28:12,124 > Epoch: 620,  train_loss=0.000561874,  val_loss=7.898184776,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:28:51,615 > Epoch: 621,  train_loss=0.000067853,  val_loss=7.972307205,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:29:31,104 > Epoch: 622,  train_loss=0.001310092,  val_loss=8.082728386,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:30:10,592 > Epoch: 623,  train_loss=0.000221445,  val_loss=8.427045822,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:30:50,071 > Epoch: 624,  train_loss=0.000041608,  val_loss=8.712015152,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:31:29,548 > Epoch: 625,  train_loss=0.001209672,  val_loss=7.791635036,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:32:09,050 > Epoch: 626,  train_loss=0.000050316,  val_loss=7.857846260,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:32:48,535 > Epoch: 627,  train_loss=0.000005359,  val_loss=7.863327026,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:33:28,038 > Epoch: 628,  train_loss=0.000516871,  val_loss=7.836848259,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:34:07,522 > Epoch: 629,  train_loss=0.000124157,  val_loss=8.083536148,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:34:46,997 > Epoch: 630,  train_loss=0.000642280,  val_loss=7.395256042,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:35:26,476 > Epoch: 631,  train_loss=0.001002445,  val_loss=6.618208885,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:36:05,963 > Epoch: 632,  train_loss=0.000054215,  val_loss=6.770393372,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:36:45,419 > Epoch: 633,  train_loss=0.000868368,  val_loss=7.214109421,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:37:24,893 > Epoch: 634,  train_loss=0.000309045,  val_loss=6.704590797,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:38:04,351 > Epoch: 635,  train_loss=0.000165450,  val_loss=6.795344353,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:38:43,829 > Epoch: 636,  train_loss=0.000120182,  val_loss=6.868944168,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:39:23,314 > Epoch: 637,  train_loss=0.000216900,  val_loss=6.740920067,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:40:02,782 > Epoch: 638,  train_loss=0.002152898,  val_loss=6.433269978,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:40:42,247 > Epoch: 639,  train_loss=0.000160265,  val_loss=6.274844170,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:41:21,749 > Epoch: 640,  train_loss=0.000671731,  val_loss=6.031663895,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-23 23:42:01,262 > Epoch: 641,  train_loss=0.001297116,  val_loss=5.923967838,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:42:40,744 > Epoch: 642,  train_loss=0.000304218,  val_loss=6.794379234,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:43:20,221 > Epoch: 643,  train_loss=0.000207093,  val_loss=6.451090336,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:43:59,712 > Epoch: 644,  train_loss=0.000502470,  val_loss=5.795161247,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:44:39,191 > Epoch: 645,  train_loss=0.000212938,  val_loss=5.557943821,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:45:18,680 > Epoch: 646,  train_loss=0.000424739,  val_loss=5.795976639,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:45:58,164 > Epoch: 647,  train_loss=0.001248325,  val_loss=5.773285389,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:46:37,625 > Epoch: 648,  train_loss=0.000296631,  val_loss=5.904513359,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:47:17,102 > Epoch: 649,  train_loss=0.000107149,  val_loss=5.949162483,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:47:56,558 > Epoch: 650,  train_loss=0.000278318,  val_loss=6.442824364,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:48:36,026 > Epoch: 651,  train_loss=0.000149239,  val_loss=6.652192116,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:49:15,490 > Epoch: 652,  train_loss=0.000202213,  val_loss=6.688965321,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:49:54,960 > Epoch: 653,  train_loss=0.000126195,  val_loss=6.669683933,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:50:34,415 > Epoch: 654,  train_loss=0.000141281,  val_loss=6.650339127,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:51:13,883 > Epoch: 655,  train_loss=0.000106863,  val_loss=6.800546646,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:51:53,399 > Epoch: 656,  train_loss=0.000367326,  val_loss=7.108733177,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:52:32,872 > Epoch: 657,  train_loss=0.000760488,  val_loss=6.914762974,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:53:12,356 > Epoch: 658,  train_loss=0.000269450,  val_loss=6.496734619,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:53:51,825 > Epoch: 659,  train_loss=0.000130768,  val_loss=6.426992416,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:54:31,299 > Epoch: 660,  train_loss=0.000491369,  val_loss=6.336780548,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:55:10,768 > Epoch: 661,  train_loss=0.000115230,  val_loss=6.418772697,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:55:50,255 > Epoch: 662,  train_loss=0.000063337,  val_loss=6.594021797,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:56:29,719 > Epoch: 663,  train_loss=0.000206268,  val_loss=6.262168884,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:57:09,202 > Epoch: 664,  train_loss=0.000435456,  val_loss=5.056670189,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:57:48,695 > Epoch: 665,  train_loss=0.000762466,  val_loss=4.516521454,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:58:28,184 > Epoch: 666,  train_loss=0.000261425,  val_loss=4.640972137,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-23 23:59:07,691 > Epoch: 667,  train_loss=0.000132477,  val_loss=4.778597355,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-23 23:59:47,177 > Epoch: 668,  train_loss=0.000340486,  val_loss=4.480001450,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:00:26,668 > Epoch: 669,  train_loss=0.000431409,  val_loss=5.510609150,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:01:06,133 > Epoch: 670,  train_loss=0.000362471,  val_loss=5.831375122,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:01:45,609 > Epoch: 671,  train_loss=0.001410354,  val_loss=7.279987335,  accuracy=0.906250000
[INFO|asb_main.py:507] 2018-10-24 00:02:25,111 > Epoch: 672,  train_loss=0.000269189,  val_loss=6.776723862,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:03:04,588 > Epoch: 673,  train_loss=0.000300807,  val_loss=6.754691124,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:03:44,049 > Epoch: 674,  train_loss=0.000284698,  val_loss=7.223318100,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:04:23,507 > Epoch: 675,  train_loss=0.000275754,  val_loss=7.125465393,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:05:02,980 > Epoch: 676,  train_loss=0.000151616,  val_loss=7.096518993,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:05:42,445 > Epoch: 677,  train_loss=0.002721154,  val_loss=7.262670517,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:06:21,908 > Epoch: 678,  train_loss=0.000266074,  val_loss=7.114191055,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:07:01,389 > Epoch: 679,  train_loss=0.000183184,  val_loss=7.173884392,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:07:40,864 > Epoch: 680,  train_loss=0.000275806,  val_loss=7.140327454,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:08:20,344 > Epoch: 681,  train_loss=0.000211008,  val_loss=7.044561386,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:08:59,834 > Epoch: 682,  train_loss=0.000123151,  val_loss=7.025033474,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:09:39,317 > Epoch: 683,  train_loss=0.000075550,  val_loss=7.148602962,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:10:18,791 > Epoch: 684,  train_loss=0.000066076,  val_loss=7.154964447,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:10:58,271 > Epoch: 685,  train_loss=0.000084042,  val_loss=7.076664925,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:11:37,765 > Epoch: 686,  train_loss=0.000272171,  val_loss=6.939072132,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:12:17,246 > Epoch: 687,  train_loss=0.000106528,  val_loss=6.956490517,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:12:56,709 > Epoch: 688,  train_loss=0.000168516,  val_loss=6.959221363,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:13:36,197 > Epoch: 689,  train_loss=0.000180462,  val_loss=6.948459625,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:14:15,685 > Epoch: 690,  train_loss=0.000297130,  val_loss=6.962660313,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:14:55,159 > Epoch: 691,  train_loss=0.000102222,  val_loss=7.100703239,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:15:34,649 > Epoch: 692,  train_loss=0.000286805,  val_loss=6.876890182,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:16:14,129 > Epoch: 693,  train_loss=0.000221302,  val_loss=6.812946320,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:16:53,617 > Epoch: 694,  train_loss=0.000173908,  val_loss=6.864016533,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:17:33,116 > Epoch: 695,  train_loss=0.000082609,  val_loss=6.901299477,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:18:12,595 > Epoch: 696,  train_loss=0.000181201,  val_loss=6.789315224,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:18:52,088 > Epoch: 697,  train_loss=0.000182224,  val_loss=6.727491379,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:19:31,576 > Epoch: 698,  train_loss=0.000085439,  val_loss=6.677981377,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:20:11,040 > Epoch: 699,  train_loss=0.000232086,  val_loss=6.468723297,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:20:50,517 > Epoch: 700,  train_loss=0.000087970,  val_loss=6.460267067,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:21:29,998 > Epoch: 701,  train_loss=0.000056933,  val_loss=6.581855297,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:22:09,479 > Epoch: 702,  train_loss=0.000058177,  val_loss=6.634781837,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:22:48,961 > Epoch: 703,  train_loss=0.000087260,  val_loss=6.650919437,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:23:28,444 > Epoch: 704,  train_loss=0.000284513,  val_loss=6.193805695,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:24:07,926 > Epoch: 705,  train_loss=0.000169839,  val_loss=6.079315662,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:24:47,416 > Epoch: 706,  train_loss=0.002122706,  val_loss=7.433917999,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:25:26,907 > Epoch: 707,  train_loss=0.000175688,  val_loss=7.359336376,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:26:06,372 > Epoch: 708,  train_loss=0.000311464,  val_loss=6.888309479,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:26:45,852 > Epoch: 709,  train_loss=0.000046155,  val_loss=6.793107986,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:27:25,332 > Epoch: 710,  train_loss=0.000301620,  val_loss=7.272313118,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:28:04,815 > Epoch: 711,  train_loss=0.000388984,  val_loss=7.010282993,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:28:44,300 > Epoch: 712,  train_loss=0.000070161,  val_loss=7.044785500,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:29:23,781 > Epoch: 713,  train_loss=0.000068132,  val_loss=7.054553032,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:30:03,254 > Epoch: 714,  train_loss=0.000117188,  val_loss=7.021720886,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:30:42,729 > Epoch: 715,  train_loss=0.000279037,  val_loss=6.826045990,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:31:22,224 > Epoch: 716,  train_loss=0.000253057,  val_loss=6.876082420,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:32:01,714 > Epoch: 717,  train_loss=0.000149750,  val_loss=6.982693672,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:32:41,183 > Epoch: 718,  train_loss=0.000044832,  val_loss=7.066027641,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:33:20,662 > Epoch: 719,  train_loss=0.000066807,  val_loss=7.184211254,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:34:00,163 > Epoch: 720,  train_loss=0.000090596,  val_loss=7.211379051,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:34:39,645 > Epoch: 721,  train_loss=0.000240456,  val_loss=6.825479507,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:35:19,146 > Epoch: 722,  train_loss=0.000985048,  val_loss=6.761761665,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:35:58,616 > Epoch: 723,  train_loss=0.000067161,  val_loss=6.791765213,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:36:38,111 > Epoch: 724,  train_loss=0.000409241,  val_loss=6.569989204,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:37:17,602 > Epoch: 725,  train_loss=0.000178479,  val_loss=6.578263283,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:37:57,104 > Epoch: 726,  train_loss=0.000079230,  val_loss=6.621042252,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:38:36,612 > Epoch: 727,  train_loss=0.000291841,  val_loss=6.587799072,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:39:16,101 > Epoch: 728,  train_loss=0.000115266,  val_loss=6.564996719,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:39:55,572 > Epoch: 729,  train_loss=0.000093377,  val_loss=6.556736946,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:40:35,054 > Epoch: 730,  train_loss=0.000115622,  val_loss=6.571039200,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:41:14,555 > Epoch: 731,  train_loss=0.000147462,  val_loss=6.426963806,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:41:54,052 > Epoch: 732,  train_loss=0.000119589,  val_loss=6.446413040,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:42:33,533 > Epoch: 733,  train_loss=0.000172149,  val_loss=6.349333763,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:43:13,038 > Epoch: 734,  train_loss=0.000202771,  val_loss=6.416175842,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:43:52,517 > Epoch: 735,  train_loss=0.000494451,  val_loss=6.430741310,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:44:31,999 > Epoch: 736,  train_loss=0.000875813,  val_loss=6.193943977,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:45:11,469 > Epoch: 737,  train_loss=0.000074018,  val_loss=6.229302883,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:45:50,948 > Epoch: 738,  train_loss=0.000296307,  val_loss=6.062520981,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:46:30,416 > Epoch: 739,  train_loss=0.000179855,  val_loss=6.046655655,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:47:09,891 > Epoch: 740,  train_loss=0.000137033,  val_loss=6.047402382,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:47:49,387 > Epoch: 741,  train_loss=0.000222301,  val_loss=5.972982407,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:48:28,887 > Epoch: 742,  train_loss=0.000074579,  val_loss=6.000343323,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:49:08,353 > Epoch: 743,  train_loss=0.000063226,  val_loss=6.000443459,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:49:47,849 > Epoch: 744,  train_loss=0.000123943,  val_loss=6.002902985,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:50:27,320 > Epoch: 745,  train_loss=0.000148081,  val_loss=6.031377316,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:51:06,797 > Epoch: 746,  train_loss=0.000119343,  val_loss=5.969213486,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:51:46,266 > Epoch: 747,  train_loss=0.000129859,  val_loss=5.987964630,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:52:25,750 > Epoch: 748,  train_loss=0.000168727,  val_loss=5.873507500,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:53:05,225 > Epoch: 749,  train_loss=0.000065699,  val_loss=5.934907436,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:53:44,697 > Epoch: 750,  train_loss=0.000114124,  val_loss=5.829023838,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:54:24,173 > Epoch: 751,  train_loss=0.000067018,  val_loss=5.899755955,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:55:03,667 > Epoch: 752,  train_loss=0.000127271,  val_loss=5.804379463,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:55:43,147 > Epoch: 753,  train_loss=0.000118931,  val_loss=5.808169365,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 00:56:22,636 > Epoch: 754,  train_loss=0.001088498,  val_loss=6.098073959,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:57:02,129 > Epoch: 755,  train_loss=0.000252733,  val_loss=6.178092957,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:57:41,621 > Epoch: 756,  train_loss=0.000190892,  val_loss=6.115460873,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:58:21,104 > Epoch: 757,  train_loss=0.000231806,  val_loss=5.979932785,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:59:00,579 > Epoch: 758,  train_loss=0.000112214,  val_loss=5.997533798,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 00:59:40,045 > Epoch: 759,  train_loss=0.000123056,  val_loss=6.086119652,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:00:19,521 > Epoch: 760,  train_loss=0.000086483,  val_loss=6.354866982,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:00:59,012 > Epoch: 761,  train_loss=0.000114939,  val_loss=6.414853096,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:01:38,495 > Epoch: 762,  train_loss=0.000092377,  val_loss=6.421456814,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:02:17,974 > Epoch: 763,  train_loss=0.000120427,  val_loss=6.332170963,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:02:57,458 > Epoch: 764,  train_loss=0.000097195,  val_loss=6.328628063,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:03:36,936 > Epoch: 765,  train_loss=0.000078038,  val_loss=6.293072701,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:04:16,411 > Epoch: 766,  train_loss=0.000235828,  val_loss=6.351152420,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:04:55,900 > Epoch: 767,  train_loss=0.000187139,  val_loss=6.396372795,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:05:35,378 > Epoch: 768,  train_loss=0.000155090,  val_loss=6.224198341,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:06:14,860 > Epoch: 769,  train_loss=0.000109659,  val_loss=6.224757195,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:06:54,338 > Epoch: 770,  train_loss=0.000086043,  val_loss=6.249563217,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:07:33,819 > Epoch: 771,  train_loss=0.000091851,  val_loss=6.225657463,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:08:13,304 > Epoch: 772,  train_loss=0.000216652,  val_loss=6.046904564,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:08:52,782 > Epoch: 773,  train_loss=0.000185740,  val_loss=5.863928318,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:09:32,255 > Epoch: 774,  train_loss=0.000120581,  val_loss=5.888258934,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:10:11,735 > Epoch: 775,  train_loss=0.000114977,  val_loss=6.026617050,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:10:51,307 > Epoch: 776,  train_loss=0.000079195,  val_loss=6.065016270,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:11:30,796 > Epoch: 777,  train_loss=0.000238904,  val_loss=6.141433716,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:12:10,276 > Epoch: 778,  train_loss=0.000296073,  val_loss=6.001865387,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:12:49,753 > Epoch: 779,  train_loss=0.000154972,  val_loss=6.126863480,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:13:29,209 > Epoch: 780,  train_loss=0.000092960,  val_loss=6.103662491,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:14:08,695 > Epoch: 781,  train_loss=0.000200015,  val_loss=5.969084740,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:14:48,178 > Epoch: 782,  train_loss=0.000064280,  val_loss=6.060583591,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:15:27,654 > Epoch: 783,  train_loss=0.000089831,  val_loss=6.110977173,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:16:07,143 > Epoch: 784,  train_loss=0.000103155,  val_loss=6.114477158,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:16:46,613 > Epoch: 785,  train_loss=0.000428232,  val_loss=5.672501564,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:17:26,086 > Epoch: 786,  train_loss=0.000095639,  val_loss=5.675008774,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:18:05,582 > Epoch: 787,  train_loss=0.000103773,  val_loss=5.699741364,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:18:45,062 > Epoch: 788,  train_loss=0.000137407,  val_loss=5.617783546,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:19:24,544 > Epoch: 789,  train_loss=0.000065595,  val_loss=5.660531998,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:20:04,017 > Epoch: 790,  train_loss=0.000061956,  val_loss=5.671799183,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:20:43,500 > Epoch: 791,  train_loss=0.000204482,  val_loss=5.488098145,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:21:22,975 > Epoch: 792,  train_loss=0.000053316,  val_loss=5.597434998,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:22:02,448 > Epoch: 793,  train_loss=0.000220894,  val_loss=5.358051300,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:22:41,930 > Epoch: 794,  train_loss=0.000091512,  val_loss=5.412776470,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:23:21,392 > Epoch: 795,  train_loss=0.000077442,  val_loss=5.456796169,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:24:00,875 > Epoch: 796,  train_loss=0.000093152,  val_loss=5.474073410,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:24:40,363 > Epoch: 797,  train_loss=0.000089299,  val_loss=5.566387177,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:25:19,854 > Epoch: 798,  train_loss=0.000185512,  val_loss=5.368656635,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:25:59,332 > Epoch: 799,  train_loss=0.000188950,  val_loss=5.012236595,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:26:38,826 > Epoch: 800,  train_loss=0.000115002,  val_loss=5.008337021,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:27:18,317 > Epoch: 801,  train_loss=0.000123825,  val_loss=4.926000595,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:27:57,807 > Epoch: 802,  train_loss=0.000077151,  val_loss=5.040163040,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:28:37,292 > Epoch: 803,  train_loss=0.000103673,  val_loss=5.073513985,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:29:16,778 > Epoch: 804,  train_loss=0.000103159,  val_loss=5.028323650,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:29:56,278 > Epoch: 805,  train_loss=0.000251480,  val_loss=5.066203117,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:30:35,761 > Epoch: 806,  train_loss=0.000109155,  val_loss=5.149649620,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:31:15,250 > Epoch: 807,  train_loss=0.000122944,  val_loss=5.034866810,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:31:54,721 > Epoch: 808,  train_loss=0.000176299,  val_loss=4.771612167,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:32:34,209 > Epoch: 809,  train_loss=0.000116690,  val_loss=4.744531155,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:33:13,700 > Epoch: 810,  train_loss=0.000060331,  val_loss=4.847107410,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:33:53,177 > Epoch: 811,  train_loss=0.000170314,  val_loss=4.715080261,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:34:32,670 > Epoch: 812,  train_loss=0.000126057,  val_loss=4.668993473,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:35:12,156 > Epoch: 813,  train_loss=0.000167749,  val_loss=4.920016289,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:35:51,637 > Epoch: 814,  train_loss=0.000174859,  val_loss=4.858154774,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:36:31,117 > Epoch: 815,  train_loss=0.000214675,  val_loss=4.861239433,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:37:10,603 > Epoch: 816,  train_loss=0.000101704,  val_loss=4.852910042,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:37:50,095 > Epoch: 817,  train_loss=0.000117695,  val_loss=4.857796669,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:38:29,580 > Epoch: 818,  train_loss=0.000166748,  val_loss=4.865920067,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:39:09,072 > Epoch: 819,  train_loss=0.000088992,  val_loss=4.907368183,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:39:48,556 > Epoch: 820,  train_loss=0.000117522,  val_loss=4.888948441,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:40:28,033 > Epoch: 821,  train_loss=0.000072080,  val_loss=4.928104401,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:41:07,522 > Epoch: 822,  train_loss=0.000107354,  val_loss=4.918110847,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:41:47,008 > Epoch: 823,  train_loss=0.000292200,  val_loss=4.723359108,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:42:26,497 > Epoch: 824,  train_loss=0.001273653,  val_loss=4.817461014,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:43:05,980 > Epoch: 825,  train_loss=0.000142382,  val_loss=4.784503937,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:43:45,524 > Epoch: 826,  train_loss=0.000080083,  val_loss=4.808535576,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:44:24,995 > Epoch: 827,  train_loss=0.000123086,  val_loss=4.829753876,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:45:04,406 > Epoch: 828,  train_loss=0.000103125,  val_loss=4.864844322,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:45:43,824 > Epoch: 829,  train_loss=0.000209135,  val_loss=4.782826424,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:46:23,246 > Epoch: 830,  train_loss=0.000099345,  val_loss=4.802677631,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:47:02,683 > Epoch: 831,  train_loss=0.000114863,  val_loss=4.811726570,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:47:42,166 > Epoch: 832,  train_loss=0.000200210,  val_loss=4.771556854,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:48:21,642 > Epoch: 833,  train_loss=0.000109318,  val_loss=4.770391464,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:49:01,093 > Epoch: 834,  train_loss=0.000158820,  val_loss=4.765081882,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:49:40,567 > Epoch: 835,  train_loss=0.000412799,  val_loss=5.250341415,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:50:19,984 > Epoch: 836,  train_loss=0.000079577,  val_loss=5.280072212,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:50:59,387 > Epoch: 837,  train_loss=0.000118812,  val_loss=5.294788361,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:51:38,841 > Epoch: 838,  train_loss=0.000105688,  val_loss=5.301178932,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:52:18,238 > Epoch: 839,  train_loss=0.000169123,  val_loss=5.272732258,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:52:57,681 > Epoch: 840,  train_loss=0.000205559,  val_loss=5.283496857,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:53:37,146 > Epoch: 841,  train_loss=0.000359061,  val_loss=5.268757820,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:54:16,601 > Epoch: 842,  train_loss=0.000137999,  val_loss=5.278282166,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:54:56,049 > Epoch: 843,  train_loss=0.000093014,  val_loss=5.250044346,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:55:35,449 > Epoch: 844,  train_loss=0.000075694,  val_loss=5.242552757,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:56:14,856 > Epoch: 845,  train_loss=0.000143250,  val_loss=5.248901367,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:56:54,277 > Epoch: 846,  train_loss=0.000090044,  val_loss=5.265447617,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:57:33,701 > Epoch: 847,  train_loss=0.000112314,  val_loss=5.285630703,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:58:13,186 > Epoch: 848,  train_loss=0.000144527,  val_loss=5.277602196,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:58:52,643 > Epoch: 849,  train_loss=0.000109692,  val_loss=5.274886608,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 01:59:32,121 > Epoch: 850,  train_loss=0.000082606,  val_loss=5.271640778,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:00:11,592 > Epoch: 851,  train_loss=0.000098357,  val_loss=5.281122208,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:00:51,012 > Epoch: 852,  train_loss=0.000126713,  val_loss=5.260717392,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:01:30,409 > Epoch: 853,  train_loss=0.000100747,  val_loss=5.297582150,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:02:09,814 > Epoch: 854,  train_loss=0.000142382,  val_loss=5.283122063,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:02:49,207 > Epoch: 855,  train_loss=0.000570204,  val_loss=5.239248276,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:03:28,686 > Epoch: 856,  train_loss=0.000166406,  val_loss=5.206634998,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:04:08,150 > Epoch: 857,  train_loss=0.000093111,  val_loss=5.213100433,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:04:47,602 > Epoch: 858,  train_loss=0.000082952,  val_loss=5.230736732,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:05:27,060 > Epoch: 859,  train_loss=0.000104033,  val_loss=5.237940788,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:06:06,464 > Epoch: 860,  train_loss=0.000230146,  val_loss=5.227022171,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:06:45,860 > Epoch: 861,  train_loss=0.000190981,  val_loss=5.210495949,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:07:25,272 > Epoch: 862,  train_loss=0.000139186,  val_loss=5.223883629,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:08:04,684 > Epoch: 863,  train_loss=0.000109137,  val_loss=5.246684551,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:08:44,151 > Epoch: 864,  train_loss=0.000163408,  val_loss=5.308537483,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:09:23,640 > Epoch: 865,  train_loss=0.000120462,  val_loss=5.301980972,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:10:03,098 > Epoch: 866,  train_loss=0.000509653,  val_loss=5.335965633,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:10:42,557 > Epoch: 867,  train_loss=0.000076797,  val_loss=5.344421387,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:11:21,965 > Epoch: 868,  train_loss=0.000161350,  val_loss=5.334901810,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:12:01,384 > Epoch: 869,  train_loss=0.000085788,  val_loss=5.343036652,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:12:40,791 > Epoch: 870,  train_loss=0.000141736,  val_loss=5.336531639,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:13:20,208 > Epoch: 871,  train_loss=0.000095684,  val_loss=5.335168839,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:13:59,683 > Epoch: 872,  train_loss=0.000085788,  val_loss=5.336744308,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:14:39,160 > Epoch: 873,  train_loss=0.000140199,  val_loss=5.330204010,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:15:18,644 > Epoch: 874,  train_loss=0.000104820,  val_loss=5.338671684,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:15:58,116 > Epoch: 875,  train_loss=0.000132115,  val_loss=5.333749771,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:16:37,531 > Epoch: 876,  train_loss=0.000081169,  val_loss=5.332092762,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:17:16,939 > Epoch: 877,  train_loss=0.000091620,  val_loss=5.344553947,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:17:56,339 > Epoch: 878,  train_loss=0.000129281,  val_loss=5.332587242,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:18:35,725 > Epoch: 879,  train_loss=0.000142123,  val_loss=5.328088284,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:19:15,188 > Epoch: 880,  train_loss=0.000081474,  val_loss=5.325135231,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:19:54,644 > Epoch: 881,  train_loss=0.000081439,  val_loss=5.343493938,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:20:34,096 > Epoch: 882,  train_loss=0.000168791,  val_loss=5.339560509,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:21:13,560 > Epoch: 883,  train_loss=0.000137972,  val_loss=5.311347485,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:21:52,957 > Epoch: 884,  train_loss=0.000139602,  val_loss=5.301873207,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:22:32,366 > Epoch: 885,  train_loss=0.000099317,  val_loss=5.315450191,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:23:11,788 > Epoch: 886,  train_loss=0.000088757,  val_loss=5.328721523,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:23:51,187 > Epoch: 887,  train_loss=0.000102133,  val_loss=5.331306458,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:24:30,654 > Epoch: 888,  train_loss=0.000163508,  val_loss=5.331103325,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:25:10,138 > Epoch: 889,  train_loss=0.000945911,  val_loss=5.369784355,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:25:49,594 > Epoch: 890,  train_loss=0.000107965,  val_loss=5.367904186,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:26:29,063 > Epoch: 891,  train_loss=0.000138529,  val_loss=5.363374710,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:27:08,460 > Epoch: 892,  train_loss=0.000069795,  val_loss=5.369264603,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:27:47,856 > Epoch: 893,  train_loss=0.000140686,  val_loss=5.363218307,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:28:27,259 > Epoch: 894,  train_loss=0.000174381,  val_loss=5.352533340,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:29:06,679 > Epoch: 895,  train_loss=0.000140240,  val_loss=5.341820717,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:29:46,157 > Epoch: 896,  train_loss=0.000498869,  val_loss=5.339436531,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:30:25,634 > Epoch: 897,  train_loss=0.000073108,  val_loss=5.344176292,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:31:05,080 > Epoch: 898,  train_loss=0.000078888,  val_loss=5.343668938,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:31:44,567 > Epoch: 899,  train_loss=0.000089258,  val_loss=5.343065262,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:32:23,959 > Epoch: 900,  train_loss=0.000103628,  val_loss=5.342467308,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:33:03,366 > Epoch: 901,  train_loss=0.000102704,  val_loss=5.343807220,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:33:42,766 > Epoch: 902,  train_loss=0.000124006,  val_loss=5.341919899,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:34:22,167 > Epoch: 903,  train_loss=0.000135689,  val_loss=5.340974331,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:35:01,640 > Epoch: 904,  train_loss=0.000100601,  val_loss=5.341975212,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:35:41,111 > Epoch: 905,  train_loss=0.000132864,  val_loss=5.339886665,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:36:20,585 > Epoch: 906,  train_loss=0.000108124,  val_loss=5.342575550,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:37:00,067 > Epoch: 907,  train_loss=0.000095064,  val_loss=5.342343330,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:37:39,471 > Epoch: 908,  train_loss=0.000112076,  val_loss=5.343451977,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:38:18,881 > Epoch: 909,  train_loss=0.000076125,  val_loss=5.347395897,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:38:58,281 > Epoch: 910,  train_loss=0.000094041,  val_loss=5.349211693,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:39:37,694 > Epoch: 911,  train_loss=0.000134417,  val_loss=5.351942062,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:40:17,252 > Epoch: 912,  train_loss=0.000234768,  val_loss=5.348686218,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:40:56,709 > Epoch: 913,  train_loss=0.000116006,  val_loss=5.347671032,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:41:36,164 > Epoch: 914,  train_loss=0.000115193,  val_loss=5.347988129,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:42:15,611 > Epoch: 915,  train_loss=0.000252956,  val_loss=5.349089622,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:42:55,021 > Epoch: 916,  train_loss=0.000186954,  val_loss=5.345619202,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:43:34,435 > Epoch: 917,  train_loss=0.000096133,  val_loss=5.348367691,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:44:13,832 > Epoch: 918,  train_loss=0.000364265,  val_loss=5.350952148,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:44:53,251 > Epoch: 919,  train_loss=0.000082479,  val_loss=5.351549625,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:45:32,717 > Epoch: 920,  train_loss=0.000094809,  val_loss=5.351304531,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:46:12,183 > Epoch: 921,  train_loss=0.000148934,  val_loss=5.350877285,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:46:51,649 > Epoch: 922,  train_loss=0.000106917,  val_loss=5.350705147,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:47:31,115 > Epoch: 923,  train_loss=0.000105558,  val_loss=5.350439548,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:48:10,522 > Epoch: 924,  train_loss=0.000120433,  val_loss=5.350431442,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:48:49,936 > Epoch: 925,  train_loss=0.000071585,  val_loss=5.350250244,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:49:29,343 > Epoch: 926,  train_loss=0.000129284,  val_loss=5.350133419,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:50:08,767 > Epoch: 927,  train_loss=0.000167465,  val_loss=5.350067616,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:50:48,250 > Epoch: 928,  train_loss=0.000121589,  val_loss=5.349988937,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:51:27,709 > Epoch: 929,  train_loss=0.000131767,  val_loss=5.349858284,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:52:07,160 > Epoch: 930,  train_loss=0.000120355,  val_loss=5.349799156,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:52:46,615 > Epoch: 931,  train_loss=0.000122339,  val_loss=5.349779129,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:53:26,021 > Epoch: 932,  train_loss=0.000125089,  val_loss=5.349775314,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:54:05,437 > Epoch: 933,  train_loss=0.000084029,  val_loss=5.349770546,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:54:44,847 > Epoch: 934,  train_loss=0.000098898,  val_loss=5.799264908,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:55:24,259 > Epoch: 935,  train_loss=0.000131352,  val_loss=4.868602753,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:56:03,713 > Epoch: 936,  train_loss=0.000327379,  val_loss=3.697615862,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 02:56:43,123 > Epoch: 937,  train_loss=0.000396084,  val_loss=3.310013056,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 02:57:22,513 > Epoch: 938,  train_loss=0.000131404,  val_loss=4.719670296,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:58:01,908 > Epoch: 939,  train_loss=0.000439554,  val_loss=4.206053734,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:58:41,262 > Epoch: 940,  train_loss=0.000147027,  val_loss=3.848374367,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 02:59:20,624 > Epoch: 941,  train_loss=0.001141285,  val_loss=3.424683094,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:00:00,037 > Epoch: 942,  train_loss=0.000974915,  val_loss=3.085230589,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:00:39,433 > Epoch: 943,  train_loss=0.000089885,  val_loss=3.421059370,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:01:18,866 > Epoch: 944,  train_loss=0.000243682,  val_loss=3.586209297,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 03:01:58,298 > Epoch: 945,  train_loss=0.000169843,  val_loss=3.886593342,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 03:02:37,764 > Epoch: 946,  train_loss=0.000384232,  val_loss=3.836541176,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 03:03:17,233 > Epoch: 947,  train_loss=0.000716019,  val_loss=3.194243908,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:03:56,601 > Epoch: 948,  train_loss=0.001474492,  val_loss=5.255069733,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:04:35,981 > Epoch: 949,  train_loss=0.000373296,  val_loss=3.831846714,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:05:15,396 > Epoch: 950,  train_loss=0.000346874,  val_loss=3.604519367,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:05:54,809 > Epoch: 951,  train_loss=0.002161883,  val_loss=3.265825987,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:06:34,284 > Epoch: 952,  train_loss=0.000954747,  val_loss=3.987343073,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:07:13,757 > Epoch: 953,  train_loss=0.005438421,  val_loss=4.688405037,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:07:53,202 > Epoch: 954,  train_loss=0.000096899,  val_loss=5.385629654,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:08:32,672 > Epoch: 955,  train_loss=0.001465109,  val_loss=5.124204159,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:09:12,096 > Epoch: 956,  train_loss=0.000568229,  val_loss=5.384784698,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:09:51,489 > Epoch: 957,  train_loss=0.000120510,  val_loss=5.362872124,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:10:30,887 > Epoch: 958,  train_loss=0.000058966,  val_loss=5.770700932,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:11:10,275 > Epoch: 959,  train_loss=0.000104690,  val_loss=5.905665398,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:11:49,718 > Epoch: 960,  train_loss=0.000235398,  val_loss=5.671090603,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:12:29,174 > Epoch: 961,  train_loss=0.001278349,  val_loss=4.950088024,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:13:08,616 > Epoch: 962,  train_loss=0.001977292,  val_loss=5.945732117,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:13:48,076 > Epoch: 963,  train_loss=0.000960511,  val_loss=4.256182194,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:14:27,482 > Epoch: 964,  train_loss=0.000053887,  val_loss=4.347440243,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:15:06,875 > Epoch: 965,  train_loss=0.006363833,  val_loss=5.739562988,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:15:46,259 > Epoch: 966,  train_loss=0.003586298,  val_loss=5.844095230,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:16:25,648 > Epoch: 967,  train_loss=0.001753995,  val_loss=4.691454887,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:17:05,077 > Epoch: 968,  train_loss=0.000230907,  val_loss=5.086948395,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:17:44,520 > Epoch: 969,  train_loss=0.000033139,  val_loss=5.122096539,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:18:23,960 > Epoch: 970,  train_loss=0.001564982,  val_loss=6.104854584,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:19:03,398 > Epoch: 971,  train_loss=0.001158905,  val_loss=5.374315262,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:19:42,777 > Epoch: 972,  train_loss=0.001047779,  val_loss=5.419005394,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:20:22,180 > Epoch: 973,  train_loss=0.001561411,  val_loss=5.913430214,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:21:01,561 > Epoch: 974,  train_loss=0.000259689,  val_loss=6.544800758,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:21:40,953 > Epoch: 975,  train_loss=0.000390822,  val_loss=6.071884155,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:22:20,415 > Epoch: 976,  train_loss=0.000116893,  val_loss=5.711111069,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:22:59,885 > Epoch: 977,  train_loss=0.000125004,  val_loss=5.572566032,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:23:39,312 > Epoch: 978,  train_loss=0.001597091,  val_loss=4.947092056,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:24:18,765 > Epoch: 979,  train_loss=0.000133475,  val_loss=4.986244202,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:24:58,160 > Epoch: 980,  train_loss=0.000294253,  val_loss=4.830633163,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:25:37,535 > Epoch: 981,  train_loss=0.001291678,  val_loss=5.649583340,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:26:16,975 > Epoch: 982,  train_loss=0.000958041,  val_loss=4.331875324,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:26:56,414 > Epoch: 983,  train_loss=0.000321933,  val_loss=4.102944374,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:27:35,888 > Epoch: 984,  train_loss=0.000641746,  val_loss=5.052929878,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:28:15,322 > Epoch: 985,  train_loss=0.000237058,  val_loss=4.927738190,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:28:54,782 > Epoch: 986,  train_loss=0.000350176,  val_loss=5.253621101,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:29:34,239 > Epoch: 987,  train_loss=0.000046269,  val_loss=5.361330509,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:30:13,634 > Epoch: 988,  train_loss=0.000932741,  val_loss=6.751379013,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:30:53,034 > Epoch: 989,  train_loss=0.001363727,  val_loss=5.049435616,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:31:32,462 > Epoch: 990,  train_loss=0.000095001,  val_loss=5.168642998,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:32:11,881 > Epoch: 991,  train_loss=0.000927710,  val_loss=5.833933830,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:32:51,343 > Epoch: 992,  train_loss=0.000243288,  val_loss=5.686050415,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:33:30,815 > Epoch: 993,  train_loss=0.000244349,  val_loss=5.675548553,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:34:10,287 > Epoch: 994,  train_loss=0.000394247,  val_loss=7.174135685,  accuracy=0.945312500
[INFO|asb_main.py:507] 2018-10-24 03:34:49,742 > Epoch: 995,  train_loss=0.001013789,  val_loss=7.655972958,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 03:35:29,134 > Epoch: 996,  train_loss=0.000452411,  val_loss=7.079331398,  accuracy=0.929687500
[INFO|asb_main.py:507] 2018-10-24 03:36:08,539 > Epoch: 997,  train_loss=0.005278543,  val_loss=4.885086536,  accuracy=0.937500000
[INFO|asb_main.py:507] 2018-10-24 03:36:47,924 > Epoch: 998,  train_loss=0.001075529,  val_loss=4.365581989,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:37:27,317 > Epoch: 999,  train_loss=0.000260464,  val_loss=4.794708252,  accuracy=0.953125000
[INFO|asb_main.py:507] 2018-10-24 03:38:06,757 > Epoch:1000,  train_loss=0.000493629,  val_loss=4.706933975,  accuracy=0.953125000
[INFO|asb_main.py:518] 2018-10-24 03:38:07,503 > ### model_201 Learning Finished!
[INFO|asb_main.py:1171] 2018-10-24 03:38:07,506 > model_201 Program end [ Total time : 10 Hour 57 Minute 52 Second ]
/home/sdsra/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
[INFO|asb_main.py:1080] 2018-10-24 16:50:28,106 > ### Loading data...
[INFO|preprocessing.py:27] 2018-10-24 16:50:29,438 > x_char_seq_ind=(21375, 500)
[INFO|preprocessing.py:28] 2018-10-24 16:50:29,438 > y shape=(21375,)
[INFO|preprocessing.py:27] 2018-10-24 16:50:29,450 > x_char_seq_ind=(147, 500)
[INFO|preprocessing.py:28] 2018-10-24 16:50:29,450 > y shape=(147,)
[INFO|asb_main.py:1091] 2018-10-24 16:50:29,451 > Train/Validate split: 21375/147
[INFO|asb_main.py:750] 2018-10-24 16:50:29,451 > ### model_212 Learning Start!
2018-10-24 16:50:29.451166: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2018-10-24 16:50:29.537666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-24 16:50:29.538200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.92GiB freeMemory: 10.70GiB
2018-10-24 16:50:29.538213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-10-24 16:50:29.713102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-24 16:50:29.713133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 
2018-10-24 16:50:29.713138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N 
2018-10-24 16:50:29.713284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10347 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
WARNING:tensorflow:From /home/sdsra/Documents/Char-level-tf/model.py:839: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
Instructions for updating:

Future major versions of TensorFlow will allow gradients to flow
into the labels input on backprop by default.

See @{tf.nn.softmax_cross_entropy_with_logits_v2}.

[INFO|asb_main.py:74] 2018-10-24 16:50:32,202 > Writing to /home/sdsra/Documents/Char-level-tf/runs/model_212_1540425032
[INFO|asb_main.py:47] 2018-10-24 16:50:32,229 > ### Parameters ----------------------------------
[INFO|asb_main.py:53] 2018-10-24 16:50:32,229 > 	epochs : 1000
[INFO|asb_main.py:53] 2018-10-24 16:50:32,229 > 	batch_size : 128
[INFO|asb_main.py:53] 2018-10-24 16:50:32,229 > 	learning_rate : 1e-06
[INFO|asb_main.py:53] 2018-10-24 16:50:32,229 > 	dropout_rate : 0.5
[INFO|asb_main.py:53] 2018-10-24 16:50:32,229 > 	alphabet : abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'"/\|_@#$%^&*~`+-=<>()[]{}

[INFO|asb_main.py:53] 2018-10-24 16:50:32,229 > 	char_max_length : 500
[INFO|asb_main.py:53] 2018-10-24 16:50:32,229 > 	num_of_classes : 3
[INFO|asb_main.py:53] 2018-10-24 16:50:32,229 > 	input_num_of_rows : 1
[INFO|asb_main.py:57] 2018-10-24 16:50:32,230 > ### Values of Graph  ----------------------------------
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer1/conv2d/kernel:0' shape=(3, 3, 1, 64) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer1/conv2d/bias:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer1/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer1/batch_normalization/beta:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer1/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer1/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer2/conv2d/kernel:0' shape=(3, 3, 64, 64) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer2/conv2d/bias:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer2/batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer2/batch_normalization/beta:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer2/batch_normalization/moving_mean:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer2/batch_normalization/moving_variance:0' shape=(64,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer3/conv2d/kernel:0' shape=(3, 3, 64, 128) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer3/conv2d/bias:0' shape=(128,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer3/batch_normalization/gamma:0' shape=(128,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer3/batch_normalization/beta:0' shape=(128,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer3/batch_normalization/moving_mean:0' shape=(128,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'Layer3/batch_normalization/moving_variance:0' shape=(128,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'FC/dense/kernel:0' shape=(72576, 1024) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'FC/dense/bias:0' shape=(1024,) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'dense/kernel:0' shape=(1024, 3) dtype=float32_ref>
[INFO|asb_main.py:61] 2018-10-24 16:50:32,230 > 	<tf.Variable 'dense/bias:0' shape=(3,) dtype=float32_ref>
[INFO|asb_main.py:65] 2018-10-24 16:50:32,230 > --------------------------------------------------------------------
[INFO|asb_main.py:823] 2018-10-24 16:51:21,334 > Epoch:   1,  train_loss=0.292321445,  val_loss=1.000775695,  accuracy=0.656250000
[INFO|asb_main.py:823] 2018-10-24 16:52:09,253 > Epoch:   2,  train_loss=0.095750609,  val_loss=0.804000974,  accuracy=0.593750000
[INFO|asb_main.py:823] 2018-10-24 16:52:57,150 > Epoch:   3,  train_loss=0.059368364,  val_loss=0.538685083,  accuracy=0.804687500
[INFO|asb_main.py:823] 2018-10-24 16:53:45,078 > Epoch:   4,  train_loss=0.044038839,  val_loss=0.354110479,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-24 16:54:32,948 > Epoch:   5,  train_loss=0.034783501,  val_loss=0.291605651,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 16:55:20,868 > Epoch:   6,  train_loss=0.028435981,  val_loss=0.271307170,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 16:56:08,774 > Epoch:   7,  train_loss=0.023716112,  val_loss=0.261282504,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 16:56:56,722 > Epoch:   8,  train_loss=0.020058641,  val_loss=0.254688621,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 16:57:44,611 > Epoch:   9,  train_loss=0.017156267,  val_loss=0.250393480,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 16:58:32,537 > Epoch:  10,  train_loss=0.014852128,  val_loss=0.247124821,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 16:59:20,498 > Epoch:  11,  train_loss=0.012980784,  val_loss=0.244333759,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:00:08,411 > Epoch:  12,  train_loss=0.011432637,  val_loss=0.242541790,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:00:56,310 > Epoch:  13,  train_loss=0.010138472,  val_loss=0.241788983,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:01:44,290 > Epoch:  14,  train_loss=0.009040262,  val_loss=0.241391510,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:02:32,251 > Epoch:  15,  train_loss=0.008105919,  val_loss=0.240937695,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:03:20,201 > Epoch:  16,  train_loss=0.007300253,  val_loss=0.240639389,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:04:08,205 > Epoch:  17,  train_loss=0.006596763,  val_loss=0.240388110,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:04:56,208 > Epoch:  18,  train_loss=0.005974610,  val_loss=0.240083367,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:05:44,152 > Epoch:  19,  train_loss=0.005417953,  val_loss=0.239987761,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:06:32,090 > Epoch:  20,  train_loss=0.004922290,  val_loss=0.239712998,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 17:07:20,026 > Epoch:  21,  train_loss=0.004483004,  val_loss=0.239825815,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 17:08:07,958 > Epoch:  22,  train_loss=0.004091671,  val_loss=0.240039304,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 17:08:55,919 > Epoch:  23,  train_loss=0.003740246,  val_loss=0.240284160,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 17:09:43,908 > Epoch:  24,  train_loss=0.003423436,  val_loss=0.240769327,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 17:10:31,889 > Epoch:  25,  train_loss=0.003135833,  val_loss=0.241391301,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 17:11:19,917 > Epoch:  26,  train_loss=0.002875297,  val_loss=0.242026389,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:12:07,815 > Epoch:  27,  train_loss=0.002637918,  val_loss=0.242742196,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:12:55,801 > Epoch:  28,  train_loss=0.002422035,  val_loss=0.243448585,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:13:43,719 > Epoch:  29,  train_loss=0.002225052,  val_loss=0.244626760,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:14:31,658 > Epoch:  30,  train_loss=0.002046624,  val_loss=0.245701522,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:15:19,650 > Epoch:  31,  train_loss=0.001884920,  val_loss=0.247002572,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:16:07,609 > Epoch:  32,  train_loss=0.001737918,  val_loss=0.248192132,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:16:55,597 > Epoch:  33,  train_loss=0.001603485,  val_loss=0.249321222,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:17:43,620 > Epoch:  34,  train_loss=0.001481325,  val_loss=0.250863403,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:18:31,542 > Epoch:  35,  train_loss=0.001369861,  val_loss=0.252319723,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:19:19,527 > Epoch:  36,  train_loss=0.001268300,  val_loss=0.253817558,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:20:07,414 > Epoch:  37,  train_loss=0.001175927,  val_loss=0.255510241,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:20:55,331 > Epoch:  38,  train_loss=0.001091661,  val_loss=0.257394671,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:21:43,302 > Epoch:  39,  train_loss=0.001014183,  val_loss=0.259117663,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:22:31,336 > Epoch:  40,  train_loss=0.000945086,  val_loss=0.261008739,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:23:19,312 > Epoch:  41,  train_loss=0.000880712,  val_loss=0.262868911,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:24:07,298 > Epoch:  42,  train_loss=0.000822077,  val_loss=0.264299691,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:24:55,182 > Epoch:  43,  train_loss=0.000768796,  val_loss=0.265976965,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:25:43,143 > Epoch:  44,  train_loss=0.000720454,  val_loss=0.268130749,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:26:31,086 > Epoch:  45,  train_loss=0.000676421,  val_loss=0.270144850,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:27:18,974 > Epoch:  46,  train_loss=0.000636649,  val_loss=0.271824449,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:28:07,004 > Epoch:  47,  train_loss=0.000600105,  val_loss=0.273930848,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:28:54,988 > Epoch:  48,  train_loss=0.000567681,  val_loss=0.275806814,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:29:42,996 > Epoch:  49,  train_loss=0.000537837,  val_loss=0.278140008,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:30:30,964 > Epoch:  50,  train_loss=0.000512483,  val_loss=0.279924572,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:31:18,901 > Epoch:  51,  train_loss=0.000486906,  val_loss=0.282185525,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:32:06,863 > Epoch:  52,  train_loss=0.000464599,  val_loss=0.284150422,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:32:54,766 > Epoch:  53,  train_loss=0.000444410,  val_loss=0.285699815,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:33:42,649 > Epoch:  54,  train_loss=0.000425917,  val_loss=0.287923157,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:34:30,663 > Epoch:  55,  train_loss=0.000411105,  val_loss=0.290115386,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:35:18,678 > Epoch:  56,  train_loss=0.000396635,  val_loss=0.292144239,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:36:06,639 > Epoch:  57,  train_loss=0.000382688,  val_loss=0.293596655,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 17:36:54,626 > Epoch:  58,  train_loss=0.000370617,  val_loss=0.295229375,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:37:42,523 > Epoch:  59,  train_loss=0.000357757,  val_loss=0.297644198,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:38:30,458 > Epoch:  60,  train_loss=0.000348022,  val_loss=0.299829841,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:39:18,383 > Epoch:  61,  train_loss=0.000338575,  val_loss=0.302380234,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:40:06,350 > Epoch:  62,  train_loss=0.000330842,  val_loss=0.304586172,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:40:54,444 > Epoch:  63,  train_loss=0.000321909,  val_loss=0.306236804,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:41:42,460 > Epoch:  64,  train_loss=0.000313985,  val_loss=0.307651341,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:42:30,505 > Epoch:  65,  train_loss=0.000307230,  val_loss=0.310473233,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:43:18,498 > Epoch:  66,  train_loss=0.000300523,  val_loss=0.312417656,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:44:06,443 > Epoch:  67,  train_loss=0.000295903,  val_loss=0.314541578,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:44:54,368 > Epoch:  68,  train_loss=0.000290583,  val_loss=0.316302329,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:45:42,318 > Epoch:  69,  train_loss=0.000284811,  val_loss=0.318550080,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:46:30,273 > Epoch:  70,  train_loss=0.000279660,  val_loss=0.320611298,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:47:18,258 > Epoch:  71,  train_loss=0.000275093,  val_loss=0.323184311,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:48:06,247 > Epoch:  72,  train_loss=0.000270834,  val_loss=0.325218946,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:48:54,240 > Epoch:  73,  train_loss=0.000268182,  val_loss=0.327161133,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:49:42,225 > Epoch:  74,  train_loss=0.000264564,  val_loss=0.329673946,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:50:30,162 > Epoch:  75,  train_loss=0.000261053,  val_loss=0.332487583,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:51:18,137 > Epoch:  76,  train_loss=0.000260392,  val_loss=0.334276795,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:52:06,036 > Epoch:  77,  train_loss=0.000255829,  val_loss=0.336895645,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:52:54,004 > Epoch:  78,  train_loss=0.000253440,  val_loss=0.339595616,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:53:41,983 > Epoch:  79,  train_loss=0.000251639,  val_loss=0.341597557,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:54:29,941 > Epoch:  80,  train_loss=0.000249056,  val_loss=0.344149411,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:55:17,982 > Epoch:  81,  train_loss=0.000245503,  val_loss=0.345887274,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:56:05,992 > Epoch:  82,  train_loss=0.000246075,  val_loss=0.348406792,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:56:53,923 > Epoch:  83,  train_loss=0.000241087,  val_loss=0.349986672,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:57:41,832 > Epoch:  84,  train_loss=0.000243687,  val_loss=0.351697773,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:58:29,816 > Epoch:  85,  train_loss=0.000237765,  val_loss=0.354503304,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 17:59:17,732 > Epoch:  86,  train_loss=0.000237635,  val_loss=0.356315553,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:00:05,740 > Epoch:  87,  train_loss=0.000235290,  val_loss=0.358839989,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:00:53,706 > Epoch:  88,  train_loss=0.000234048,  val_loss=0.361263126,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:01:41,672 > Epoch:  89,  train_loss=0.000234091,  val_loss=0.363256246,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:02:29,675 > Epoch:  90,  train_loss=0.000232882,  val_loss=0.364871800,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:03:17,616 > Epoch:  91,  train_loss=0.000231871,  val_loss=0.367642522,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:04:05,504 > Epoch:  92,  train_loss=0.000229225,  val_loss=0.368998230,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:04:53,463 > Epoch:  93,  train_loss=0.000227264,  val_loss=0.372341871,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:05:41,454 > Epoch:  94,  train_loss=0.000227724,  val_loss=0.374411762,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:06:29,431 > Epoch:  95,  train_loss=0.000226572,  val_loss=0.376824826,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:07:17,470 > Epoch:  96,  train_loss=0.000227537,  val_loss=0.378818214,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:08:05,421 > Epoch:  97,  train_loss=0.000228025,  val_loss=0.380190432,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:08:53,400 > Epoch:  98,  train_loss=0.000226829,  val_loss=0.383023083,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:09:41,378 > Epoch:  99,  train_loss=0.000228305,  val_loss=0.385129094,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:10:29,353 > Epoch: 100,  train_loss=0.000225587,  val_loss=0.387582302,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:11:17,334 > Epoch: 101,  train_loss=0.000226119,  val_loss=0.389973640,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:12:05,301 > Epoch: 102,  train_loss=0.000224190,  val_loss=0.392445475,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:12:53,284 > Epoch: 103,  train_loss=0.000224811,  val_loss=0.395208061,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:13:41,265 > Epoch: 104,  train_loss=0.000223448,  val_loss=0.395666838,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:14:29,254 > Epoch: 105,  train_loss=0.000224169,  val_loss=0.396771908,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:15:17,220 > Epoch: 106,  train_loss=0.000222453,  val_loss=0.398891151,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:16:05,130 > Epoch: 107,  train_loss=0.000221507,  val_loss=0.401074886,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:16:53,094 > Epoch: 108,  train_loss=0.000220268,  val_loss=0.404551625,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:17:41,018 > Epoch: 109,  train_loss=0.000220786,  val_loss=0.405181199,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:18:28,958 > Epoch: 110,  train_loss=0.000220555,  val_loss=0.406874478,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:19:16,943 > Epoch: 111,  train_loss=0.000218861,  val_loss=0.409041882,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:20:04,951 > Epoch: 112,  train_loss=0.000220449,  val_loss=0.411563933,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:20:52,919 > Epoch: 113,  train_loss=0.000219907,  val_loss=0.412849367,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:21:40,916 > Epoch: 114,  train_loss=0.000218079,  val_loss=0.415073335,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:22:28,839 > Epoch: 115,  train_loss=0.000218359,  val_loss=0.417628288,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:23:16,778 > Epoch: 116,  train_loss=0.000219125,  val_loss=0.418935508,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:24:04,729 > Epoch: 117,  train_loss=0.000219514,  val_loss=0.421007901,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:24:52,638 > Epoch: 118,  train_loss=0.000219414,  val_loss=0.421717942,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:25:40,651 > Epoch: 119,  train_loss=0.000217653,  val_loss=0.424240470,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:26:28,670 > Epoch: 120,  train_loss=0.000218511,  val_loss=0.426079273,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:27:16,666 > Epoch: 121,  train_loss=0.000217330,  val_loss=0.429590732,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:28:04,671 > Epoch: 122,  train_loss=0.000219102,  val_loss=0.430799544,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:28:52,577 > Epoch: 123,  train_loss=0.000215831,  val_loss=0.432537556,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:29:40,512 > Epoch: 124,  train_loss=0.000216298,  val_loss=0.434953511,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:30:28,401 > Epoch: 125,  train_loss=0.000217861,  val_loss=0.436769426,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:31:16,396 > Epoch: 126,  train_loss=0.000216420,  val_loss=0.439452469,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:32:04,365 > Epoch: 127,  train_loss=0.000217641,  val_loss=0.440759569,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:32:52,388 > Epoch: 128,  train_loss=0.000215996,  val_loss=0.443170130,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:33:40,402 > Epoch: 129,  train_loss=0.000215037,  val_loss=0.444076508,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:34:28,418 > Epoch: 130,  train_loss=0.000214549,  val_loss=0.446815223,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:35:16,308 > Epoch: 131,  train_loss=0.000214591,  val_loss=0.448513687,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:36:04,214 > Epoch: 132,  train_loss=0.000213865,  val_loss=0.450920910,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:36:52,100 > Epoch: 133,  train_loss=0.000217457,  val_loss=0.451816738,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:37:40,147 > Epoch: 134,  train_loss=0.000215827,  val_loss=0.454166949,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:38:28,147 > Epoch: 135,  train_loss=0.000217171,  val_loss=0.455619276,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:39:16,126 > Epoch: 136,  train_loss=0.000215370,  val_loss=0.456934750,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:40:04,135 > Epoch: 137,  train_loss=0.000214497,  val_loss=0.459450781,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:40:52,124 > Epoch: 138,  train_loss=0.000215162,  val_loss=0.461589098,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:41:40,038 > Epoch: 139,  train_loss=0.000215227,  val_loss=0.463251472,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:42:27,934 > Epoch: 140,  train_loss=0.000214119,  val_loss=0.464495331,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:43:15,835 > Epoch: 141,  train_loss=0.000215082,  val_loss=0.465964377,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:44:03,839 > Epoch: 142,  train_loss=0.000214501,  val_loss=0.468198687,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:44:51,820 > Epoch: 143,  train_loss=0.000214150,  val_loss=0.469648063,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:45:39,777 > Epoch: 144,  train_loss=0.000213897,  val_loss=0.471629858,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:46:27,751 > Epoch: 145,  train_loss=0.000213615,  val_loss=0.473095953,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:47:15,700 > Epoch: 146,  train_loss=0.000215180,  val_loss=0.475939959,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:48:03,655 > Epoch: 147,  train_loss=0.000217098,  val_loss=0.477298141,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:48:51,565 > Epoch: 148,  train_loss=0.000215541,  val_loss=0.478429556,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:49:39,484 > Epoch: 149,  train_loss=0.000214561,  val_loss=0.481122971,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:50:27,449 > Epoch: 150,  train_loss=0.000215355,  val_loss=0.483069181,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:51:15,440 > Epoch: 151,  train_loss=0.000215373,  val_loss=0.483727366,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:52:03,401 > Epoch: 152,  train_loss=0.000214723,  val_loss=0.487127721,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:52:51,437 > Epoch: 153,  train_loss=0.000215315,  val_loss=0.489482641,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:53:39,403 > Epoch: 154,  train_loss=0.000217827,  val_loss=0.491076112,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:54:27,385 > Epoch: 155,  train_loss=0.000218483,  val_loss=0.491703749,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:55:15,399 > Epoch: 156,  train_loss=0.000214598,  val_loss=0.492277086,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:56:03,323 > Epoch: 157,  train_loss=0.000213369,  val_loss=0.494102538,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:56:51,268 > Epoch: 158,  train_loss=0.000213088,  val_loss=0.496385753,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:57:39,308 > Epoch: 159,  train_loss=0.000214265,  val_loss=0.498750418,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:58:27,274 > Epoch: 160,  train_loss=0.000214631,  val_loss=0.498982906,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 18:59:15,293 > Epoch: 161,  train_loss=0.000213787,  val_loss=0.502357125,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 19:00:03,365 > Epoch: 162,  train_loss=0.000214187,  val_loss=0.504120708,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 19:00:51,287 > Epoch: 163,  train_loss=0.000216303,  val_loss=0.506214023,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 19:01:39,207 > Epoch: 164,  train_loss=0.000214887,  val_loss=0.509000778,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 19:02:27,136 > Epoch: 165,  train_loss=0.000218017,  val_loss=0.510778546,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 19:03:15,058 > Epoch: 166,  train_loss=0.000217265,  val_loss=0.512186348,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 19:04:03,047 > Epoch: 167,  train_loss=0.000216188,  val_loss=0.514700532,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 19:04:51,065 > Epoch: 168,  train_loss=0.000215810,  val_loss=0.515371680,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 19:05:39,081 > Epoch: 169,  train_loss=0.000214317,  val_loss=0.517325342,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 19:06:27,031 > Epoch: 170,  train_loss=0.000213722,  val_loss=0.518217683,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 19:07:14,967 > Epoch: 171,  train_loss=0.000213939,  val_loss=0.521180987,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 19:08:02,876 > Epoch: 172,  train_loss=0.000213743,  val_loss=0.523806989,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 19:08:50,888 > Epoch: 173,  train_loss=0.000215237,  val_loss=0.524841785,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 19:09:38,801 > Epoch: 174,  train_loss=0.000215194,  val_loss=0.526731908,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 19:10:26,762 > Epoch: 175,  train_loss=0.000214734,  val_loss=0.530338943,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 19:11:14,759 > Epoch: 176,  train_loss=0.000217155,  val_loss=0.531482279,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:12:02,721 > Epoch: 177,  train_loss=0.000215835,  val_loss=0.533235908,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:12:50,721 > Epoch: 178,  train_loss=0.000215289,  val_loss=0.534033358,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:13:38,679 > Epoch: 179,  train_loss=0.000214404,  val_loss=0.537358880,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:14:26,638 > Epoch: 180,  train_loss=0.000219066,  val_loss=0.538656473,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:15:14,568 > Epoch: 181,  train_loss=0.000220495,  val_loss=0.539300978,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:16:02,524 > Epoch: 182,  train_loss=0.000216924,  val_loss=0.540116310,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:16:50,532 > Epoch: 183,  train_loss=0.000216141,  val_loss=0.542913556,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:17:38,506 > Epoch: 184,  train_loss=0.000215926,  val_loss=0.544738531,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:18:26,497 > Epoch: 185,  train_loss=0.000216132,  val_loss=0.546855628,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:19:14,473 > Epoch: 186,  train_loss=0.000216558,  val_loss=0.549061477,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:20:02,429 > Epoch: 187,  train_loss=0.000216874,  val_loss=0.551926613,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:20:50,317 > Epoch: 188,  train_loss=0.000217038,  val_loss=0.552742481,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:21:38,239 > Epoch: 189,  train_loss=0.000216475,  val_loss=0.554102302,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:22:26,171 > Epoch: 190,  train_loss=0.000217475,  val_loss=0.557148814,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:23:14,147 > Epoch: 191,  train_loss=0.000218170,  val_loss=0.558860958,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:24:02,050 > Epoch: 192,  train_loss=0.000218413,  val_loss=0.562474787,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:24:49,999 > Epoch: 193,  train_loss=0.000220708,  val_loss=0.564619541,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:25:37,934 > Epoch: 194,  train_loss=0.000220354,  val_loss=0.565136313,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:26:25,925 > Epoch: 195,  train_loss=0.000219745,  val_loss=0.566817343,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:27:13,896 > Epoch: 196,  train_loss=0.000220555,  val_loss=0.568797469,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:28:01,901 > Epoch: 197,  train_loss=0.000220268,  val_loss=0.571249723,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:28:49,871 > Epoch: 198,  train_loss=0.000221431,  val_loss=0.572632611,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:29:37,767 > Epoch: 199,  train_loss=0.000219935,  val_loss=0.574279308,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:30:25,747 > Epoch: 200,  train_loss=0.000220769,  val_loss=0.576213539,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:31:13,692 > Epoch: 201,  train_loss=0.000220862,  val_loss=0.578238368,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:32:01,636 > Epoch: 202,  train_loss=0.000220907,  val_loss=0.579903066,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:32:49,591 > Epoch: 203,  train_loss=0.000221289,  val_loss=0.581308722,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:33:37,607 > Epoch: 204,  train_loss=0.000222360,  val_loss=0.584945738,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:34:25,578 > Epoch: 205,  train_loss=0.000225373,  val_loss=0.585393906,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:35:13,572 > Epoch: 206,  train_loss=0.000224396,  val_loss=0.586036563,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:36:01,516 > Epoch: 207,  train_loss=0.000222900,  val_loss=0.586768448,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:36:49,414 > Epoch: 208,  train_loss=0.000222678,  val_loss=0.588940084,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:37:37,342 > Epoch: 209,  train_loss=0.000225864,  val_loss=0.588821769,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:38:25,338 > Epoch: 210,  train_loss=0.000223198,  val_loss=0.590404212,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:39:13,308 > Epoch: 211,  train_loss=0.000221724,  val_loss=0.590803862,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:40:01,256 > Epoch: 212,  train_loss=0.000222969,  val_loss=0.593298018,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 19:40:49,217 > Epoch: 213,  train_loss=0.000225367,  val_loss=0.595226526,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:41:37,189 > Epoch: 214,  train_loss=0.000225251,  val_loss=0.597358882,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:42:25,123 > Epoch: 215,  train_loss=0.000223724,  val_loss=0.598475456,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:43:13,054 > Epoch: 216,  train_loss=0.000223823,  val_loss=0.600000262,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:44:01,060 > Epoch: 217,  train_loss=0.000224028,  val_loss=0.601011097,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:44:48,988 > Epoch: 218,  train_loss=0.000223778,  val_loss=0.602695465,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:45:36,992 > Epoch: 219,  train_loss=0.000226474,  val_loss=0.605476558,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:46:24,949 > Epoch: 220,  train_loss=0.000227630,  val_loss=0.606484115,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:47:12,932 > Epoch: 221,  train_loss=0.000225040,  val_loss=0.607554197,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:48:00,902 > Epoch: 222,  train_loss=0.000224138,  val_loss=0.607580543,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:48:48,823 > Epoch: 223,  train_loss=0.000225163,  val_loss=0.608831644,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:49:36,763 > Epoch: 224,  train_loss=0.000225157,  val_loss=0.612247109,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:50:24,694 > Epoch: 225,  train_loss=0.000227987,  val_loss=0.613676012,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:51:12,588 > Epoch: 226,  train_loss=0.000225993,  val_loss=0.613862753,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:52:00,575 > Epoch: 227,  train_loss=0.000226094,  val_loss=0.615588665,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:52:48,529 > Epoch: 228,  train_loss=0.000226313,  val_loss=0.617143929,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:53:36,512 > Epoch: 229,  train_loss=0.000226549,  val_loss=0.619277716,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:54:24,488 > Epoch: 230,  train_loss=0.000226660,  val_loss=0.618114233,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:55:12,445 > Epoch: 231,  train_loss=0.000226707,  val_loss=0.620791078,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:56:00,376 > Epoch: 232,  train_loss=0.000226761,  val_loss=0.621373117,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:56:48,275 > Epoch: 233,  train_loss=0.000225896,  val_loss=0.621170163,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:57:36,216 > Epoch: 234,  train_loss=0.000227551,  val_loss=0.624685585,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:58:24,182 > Epoch: 235,  train_loss=0.000228650,  val_loss=0.626726627,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 19:59:12,173 > Epoch: 236,  train_loss=0.000230212,  val_loss=0.627346992,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:00:00,138 > Epoch: 237,  train_loss=0.000228777,  val_loss=0.629512668,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:00:48,126 > Epoch: 238,  train_loss=0.000230596,  val_loss=0.629919171,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:01:36,125 > Epoch: 239,  train_loss=0.000229297,  val_loss=0.630815387,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:02:24,096 > Epoch: 240,  train_loss=0.000229476,  val_loss=0.632757306,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:03:12,006 > Epoch: 241,  train_loss=0.000231037,  val_loss=0.633973479,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:03:59,930 > Epoch: 242,  train_loss=0.000230805,  val_loss=0.634774327,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:04:47,850 > Epoch: 243,  train_loss=0.000229852,  val_loss=0.635435820,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:05:35,799 > Epoch: 244,  train_loss=0.000229580,  val_loss=0.639249861,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:06:23,781 > Epoch: 245,  train_loss=0.000230181,  val_loss=0.637304127,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:07:11,738 > Epoch: 246,  train_loss=0.000231538,  val_loss=0.639717638,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:07:59,757 > Epoch: 247,  train_loss=0.000232107,  val_loss=0.641532004,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:08:47,754 > Epoch: 248,  train_loss=0.000232184,  val_loss=0.642158151,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-24 20:09:35,627 > Epoch: 249,  train_loss=0.000233135,  val_loss=0.642061174,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 20:10:23,569 > Epoch: 250,  train_loss=0.000233405,  val_loss=0.643206835,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:11:11,482 > Epoch: 251,  train_loss=0.000233761,  val_loss=0.643051624,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:11:59,417 > Epoch: 252,  train_loss=0.000234047,  val_loss=0.645146132,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-24 20:12:47,387 > Epoch: 253,  train_loss=0.000235193,  val_loss=0.646281421,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:13:35,372 > Epoch: 254,  train_loss=0.000234992,  val_loss=0.646201670,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:14:23,416 > Epoch: 255,  train_loss=0.000234379,  val_loss=0.647015333,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:15:11,378 > Epoch: 256,  train_loss=0.000234448,  val_loss=0.647841692,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:15:59,296 > Epoch: 257,  train_loss=0.000237352,  val_loss=0.649734557,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:16:47,226 > Epoch: 258,  train_loss=0.000238042,  val_loss=0.651289105,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:17:35,161 > Epoch: 259,  train_loss=0.000238796,  val_loss=0.652003765,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:18:23,057 > Epoch: 260,  train_loss=0.000239285,  val_loss=0.654228091,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:19:11,063 > Epoch: 261,  train_loss=0.000241448,  val_loss=0.654926538,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:19:59,084 > Epoch: 262,  train_loss=0.000240722,  val_loss=0.655268729,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:20:47,088 > Epoch: 263,  train_loss=0.000243486,  val_loss=0.654912233,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:21:35,107 > Epoch: 264,  train_loss=0.000243017,  val_loss=0.657718539,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:22:23,027 > Epoch: 265,  train_loss=0.000243471,  val_loss=0.658221602,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:23:10,963 > Epoch: 266,  train_loss=0.000243368,  val_loss=0.657846749,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:23:58,900 > Epoch: 267,  train_loss=0.000246463,  val_loss=0.660981715,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:24:46,859 > Epoch: 268,  train_loss=0.000247630,  val_loss=0.660866737,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 20:25:34,850 > Epoch: 269,  train_loss=0.000247506,  val_loss=0.660519540,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:26:22,833 > Epoch: 270,  train_loss=0.000249278,  val_loss=0.661791503,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:27:10,840 > Epoch: 271,  train_loss=0.000249941,  val_loss=0.662495852,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:27:58,821 > Epoch: 272,  train_loss=0.000252352,  val_loss=0.662181139,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:28:46,795 > Epoch: 273,  train_loss=0.000253160,  val_loss=0.663754344,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:29:34,783 > Epoch: 274,  train_loss=0.000253984,  val_loss=0.665171742,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:30:22,702 > Epoch: 275,  train_loss=0.000256017,  val_loss=0.663603306,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:31:10,601 > Epoch: 276,  train_loss=0.000258208,  val_loss=0.665295720,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:31:58,618 > Epoch: 277,  train_loss=0.000261943,  val_loss=0.663656831,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:32:46,555 > Epoch: 278,  train_loss=0.000260989,  val_loss=0.664288044,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:33:34,514 > Epoch: 279,  train_loss=0.000263155,  val_loss=0.665478945,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:34:22,498 > Epoch: 280,  train_loss=0.000264663,  val_loss=0.664932430,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:35:10,468 > Epoch: 281,  train_loss=0.000265390,  val_loss=0.663605392,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:35:58,394 > Epoch: 282,  train_loss=0.000268118,  val_loss=0.663984656,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:36:46,321 > Epoch: 283,  train_loss=0.000268352,  val_loss=0.665686488,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:37:34,263 > Epoch: 284,  train_loss=0.000272971,  val_loss=0.665748954,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:38:22,266 > Epoch: 285,  train_loss=0.000274454,  val_loss=0.665916681,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:39:10,290 > Epoch: 286,  train_loss=0.000276171,  val_loss=0.666406929,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:39:58,272 > Epoch: 287,  train_loss=0.000278482,  val_loss=0.666539907,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:40:46,316 > Epoch: 288,  train_loss=0.000280774,  val_loss=0.665655375,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:41:34,317 > Epoch: 289,  train_loss=0.000285778,  val_loss=0.663359165,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:42:22,250 > Epoch: 290,  train_loss=0.000286104,  val_loss=0.664662004,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:43:10,165 > Epoch: 291,  train_loss=0.000287743,  val_loss=0.667652667,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:43:58,063 > Epoch: 292,  train_loss=0.000290296,  val_loss=0.666468799,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:44:46,077 > Epoch: 293,  train_loss=0.000294172,  val_loss=0.666410327,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:45:34,050 > Epoch: 294,  train_loss=0.000293624,  val_loss=0.665811181,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:46:22,033 > Epoch: 295,  train_loss=0.000294704,  val_loss=0.665743470,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:47:09,994 > Epoch: 296,  train_loss=0.000295309,  val_loss=0.664812446,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:47:57,900 > Epoch: 297,  train_loss=0.000295163,  val_loss=0.665914536,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:48:45,859 > Epoch: 298,  train_loss=0.000297910,  val_loss=0.666433930,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:49:33,756 > Epoch: 299,  train_loss=0.000297388,  val_loss=0.663626194,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:50:21,689 > Epoch: 300,  train_loss=0.000298078,  val_loss=0.664090872,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:51:09,690 > Epoch: 301,  train_loss=0.000299480,  val_loss=0.664917231,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:51:57,717 > Epoch: 302,  train_loss=0.000299287,  val_loss=0.664921165,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:52:45,742 > Epoch: 303,  train_loss=0.000296796,  val_loss=0.664227605,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:53:33,724 > Epoch: 304,  train_loss=0.000299864,  val_loss=0.665440202,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:54:21,692 > Epoch: 305,  train_loss=0.000299968,  val_loss=0.664225996,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:55:09,597 > Epoch: 306,  train_loss=0.000297784,  val_loss=0.665552258,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:55:57,533 > Epoch: 307,  train_loss=0.000301793,  val_loss=0.665121496,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:56:45,441 > Epoch: 308,  train_loss=0.000301992,  val_loss=0.665772200,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:57:33,395 > Epoch: 309,  train_loss=0.000301061,  val_loss=0.665261686,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:58:21,371 > Epoch: 310,  train_loss=0.000302418,  val_loss=0.665698051,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:59:09,336 > Epoch: 311,  train_loss=0.000302207,  val_loss=0.665045559,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 20:59:57,302 > Epoch: 312,  train_loss=0.000300814,  val_loss=0.665164828,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:00:45,269 > Epoch: 313,  train_loss=0.000302276,  val_loss=0.667977810,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:01:33,220 > Epoch: 314,  train_loss=0.000301901,  val_loss=0.664304495,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:02:21,180 > Epoch: 315,  train_loss=0.000300626,  val_loss=0.664874136,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:03:09,160 > Epoch: 316,  train_loss=0.000302378,  val_loss=0.665620267,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:03:57,143 > Epoch: 317,  train_loss=0.000303772,  val_loss=0.666101336,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:04:45,131 > Epoch: 318,  train_loss=0.000302566,  val_loss=0.665383816,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:05:33,129 > Epoch: 319,  train_loss=0.000302976,  val_loss=0.665076494,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:06:21,127 > Epoch: 320,  train_loss=0.000303680,  val_loss=0.666538239,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:07:09,059 > Epoch: 321,  train_loss=0.000302398,  val_loss=0.666275501,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:07:56,998 > Epoch: 322,  train_loss=0.000301486,  val_loss=0.666724861,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:08:44,928 > Epoch: 323,  train_loss=0.000302544,  val_loss=0.668043971,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:09:32,874 > Epoch: 324,  train_loss=0.000303621,  val_loss=0.667817652,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:10:20,875 > Epoch: 325,  train_loss=0.000301171,  val_loss=0.667039275,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:11:08,902 > Epoch: 326,  train_loss=0.000301048,  val_loss=0.667619765,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:11:56,905 > Epoch: 327,  train_loss=0.000302332,  val_loss=0.668273032,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:12:44,902 > Epoch: 328,  train_loss=0.000301593,  val_loss=0.668094635,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:13:32,857 > Epoch: 329,  train_loss=0.000301115,  val_loss=0.668337226,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:14:20,768 > Epoch: 330,  train_loss=0.000303682,  val_loss=0.668701351,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:15:08,692 > Epoch: 331,  train_loss=0.000303193,  val_loss=0.668932199,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:15:56,633 > Epoch: 332,  train_loss=0.000302295,  val_loss=0.667909205,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:16:44,610 > Epoch: 333,  train_loss=0.000300916,  val_loss=0.668660641,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:17:32,567 > Epoch: 334,  train_loss=0.000303328,  val_loss=0.668362617,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:18:20,542 > Epoch: 335,  train_loss=0.000300946,  val_loss=0.669215143,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:19:08,490 > Epoch: 336,  train_loss=0.000298892,  val_loss=0.669179618,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:19:56,393 > Epoch: 337,  train_loss=0.000299431,  val_loss=0.668387353,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:20:44,408 > Epoch: 338,  train_loss=0.000300029,  val_loss=0.668179095,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:21:32,356 > Epoch: 339,  train_loss=0.000301609,  val_loss=0.668166280,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:22:20,261 > Epoch: 340,  train_loss=0.000302245,  val_loss=0.668087125,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:23:08,241 > Epoch: 341,  train_loss=0.000301304,  val_loss=0.667518258,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:23:56,217 > Epoch: 342,  train_loss=0.000299742,  val_loss=0.667686164,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:24:44,230 > Epoch: 343,  train_loss=0.000301203,  val_loss=0.666972995,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:25:32,181 > Epoch: 344,  train_loss=0.000302660,  val_loss=0.668244839,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:26:20,146 > Epoch: 345,  train_loss=0.000301193,  val_loss=0.667412102,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:27:08,083 > Epoch: 346,  train_loss=0.000299557,  val_loss=0.669057488,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:27:55,991 > Epoch: 347,  train_loss=0.000302609,  val_loss=0.668167233,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:28:43,925 > Epoch: 348,  train_loss=0.000302564,  val_loss=0.669420719,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:29:31,919 > Epoch: 349,  train_loss=0.000300765,  val_loss=0.667536020,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:30:19,909 > Epoch: 350,  train_loss=0.000301067,  val_loss=0.668095112,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:31:07,907 > Epoch: 351,  train_loss=0.000301386,  val_loss=0.667904794,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:31:55,873 > Epoch: 352,  train_loss=0.000300034,  val_loss=0.669317484,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:32:43,826 > Epoch: 353,  train_loss=0.000298788,  val_loss=0.670372486,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:33:31,766 > Epoch: 354,  train_loss=0.000298577,  val_loss=0.670325935,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:34:19,715 > Epoch: 355,  train_loss=0.000298473,  val_loss=0.670299232,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:35:07,649 > Epoch: 356,  train_loss=0.000298630,  val_loss=0.669392467,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:35:55,642 > Epoch: 357,  train_loss=0.000299221,  val_loss=0.670566499,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:36:43,582 > Epoch: 358,  train_loss=0.000300579,  val_loss=0.672405243,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:37:31,563 > Epoch: 359,  train_loss=0.000299324,  val_loss=0.670274496,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:38:19,585 > Epoch: 360,  train_loss=0.000299015,  val_loss=0.670036912,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:39:07,588 > Epoch: 361,  train_loss=0.000298557,  val_loss=0.672064006,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:39:55,486 > Epoch: 362,  train_loss=0.000299743,  val_loss=0.670978069,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:40:43,404 > Epoch: 363,  train_loss=0.000299440,  val_loss=0.671162188,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:41:31,312 > Epoch: 364,  train_loss=0.000299921,  val_loss=0.670866013,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:42:19,292 > Epoch: 365,  train_loss=0.000299296,  val_loss=0.670847714,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:43:07,295 > Epoch: 366,  train_loss=0.000299470,  val_loss=0.671310544,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:43:55,293 > Epoch: 367,  train_loss=0.000299661,  val_loss=0.671080828,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:44:43,276 > Epoch: 368,  train_loss=0.000300148,  val_loss=0.671085238,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:45:31,269 > Epoch: 369,  train_loss=0.000299101,  val_loss=0.671489179,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:46:19,166 > Epoch: 370,  train_loss=0.000299463,  val_loss=0.672512829,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:47:07,054 > Epoch: 371,  train_loss=0.000301230,  val_loss=0.671904743,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:47:55,012 > Epoch: 372,  train_loss=0.000299479,  val_loss=0.670385659,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:48:43,011 > Epoch: 373,  train_loss=0.000300695,  val_loss=0.672123075,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:49:30,958 > Epoch: 374,  train_loss=0.000299412,  val_loss=0.670410872,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:50:19,004 > Epoch: 375,  train_loss=0.000297950,  val_loss=0.671695113,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:51:06,953 > Epoch: 376,  train_loss=0.000298154,  val_loss=0.672169268,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:51:54,845 > Epoch: 377,  train_loss=0.000299053,  val_loss=0.671385765,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:52:42,780 > Epoch: 378,  train_loss=0.000298370,  val_loss=0.672239482,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:53:30,725 > Epoch: 379,  train_loss=0.000298178,  val_loss=0.672753453,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:54:18,638 > Epoch: 380,  train_loss=0.000299501,  val_loss=0.669269264,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:55:06,589 > Epoch: 381,  train_loss=0.000299174,  val_loss=0.672392368,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:55:54,561 > Epoch: 382,  train_loss=0.000297815,  val_loss=0.671784043,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:56:42,521 > Epoch: 383,  train_loss=0.000297351,  val_loss=0.670646369,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:57:30,555 > Epoch: 384,  train_loss=0.000296557,  val_loss=0.671712935,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:58:18,447 > Epoch: 385,  train_loss=0.000296557,  val_loss=0.672546923,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:59:06,483 > Epoch: 386,  train_loss=0.000297800,  val_loss=0.672256947,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 21:59:54,412 > Epoch: 387,  train_loss=0.000298048,  val_loss=0.672502577,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:00:42,332 > Epoch: 388,  train_loss=0.000297737,  val_loss=0.672118902,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:01:30,349 > Epoch: 389,  train_loss=0.000297410,  val_loss=0.673515797,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:02:18,316 > Epoch: 390,  train_loss=0.000297346,  val_loss=0.674714565,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:03:06,297 > Epoch: 391,  train_loss=0.000300671,  val_loss=0.673700333,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:03:54,368 > Epoch: 392,  train_loss=0.000299717,  val_loss=0.674666107,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:04:42,267 > Epoch: 393,  train_loss=0.000301363,  val_loss=0.673793018,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:05:30,172 > Epoch: 394,  train_loss=0.000298994,  val_loss=0.672876358,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:06:18,121 > Epoch: 395,  train_loss=0.000297690,  val_loss=0.674890995,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:07:06,023 > Epoch: 396,  train_loss=0.000298554,  val_loss=0.673821628,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:07:54,013 > Epoch: 397,  train_loss=0.000298003,  val_loss=0.674891233,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:08:41,996 > Epoch: 398,  train_loss=0.000298609,  val_loss=0.674793184,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:09:29,962 > Epoch: 399,  train_loss=0.000297242,  val_loss=0.675715446,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:10:17,938 > Epoch: 400,  train_loss=0.000297517,  val_loss=0.674861550,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:11:05,876 > Epoch: 401,  train_loss=0.000296911,  val_loss=0.675826788,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:11:53,841 > Epoch: 402,  train_loss=0.000296105,  val_loss=0.674933076,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:12:41,731 > Epoch: 403,  train_loss=0.000296055,  val_loss=0.673899889,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:13:29,684 > Epoch: 404,  train_loss=0.000296286,  val_loss=0.673337817,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:14:17,714 > Epoch: 405,  train_loss=0.000295444,  val_loss=0.674369216,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:15:05,703 > Epoch: 406,  train_loss=0.000295178,  val_loss=0.676439464,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:15:53,707 > Epoch: 407,  train_loss=0.000295183,  val_loss=0.676356733,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:16:41,707 > Epoch: 408,  train_loss=0.000295332,  val_loss=0.676330328,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:17:29,662 > Epoch: 409,  train_loss=0.000295865,  val_loss=0.676139951,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:18:17,593 > Epoch: 410,  train_loss=0.000296527,  val_loss=0.676604867,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:19:05,531 > Epoch: 411,  train_loss=0.000295010,  val_loss=0.676153183,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:19:53,435 > Epoch: 412,  train_loss=0.000296795,  val_loss=0.677754939,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:20:41,419 > Epoch: 413,  train_loss=0.000295844,  val_loss=0.679126263,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:21:29,401 > Epoch: 414,  train_loss=0.000295582,  val_loss=0.678683579,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:22:17,389 > Epoch: 415,  train_loss=0.000296598,  val_loss=0.678637624,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:23:05,364 > Epoch: 416,  train_loss=0.000297158,  val_loss=0.679873288,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:23:53,326 > Epoch: 417,  train_loss=0.000295048,  val_loss=0.681098282,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:24:41,236 > Epoch: 418,  train_loss=0.000291962,  val_loss=0.681150854,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:25:29,174 > Epoch: 419,  train_loss=0.000296547,  val_loss=0.681660712,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:26:17,053 > Epoch: 420,  train_loss=0.000295433,  val_loss=0.681812763,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:27:05,019 > Epoch: 421,  train_loss=0.000296024,  val_loss=0.680862784,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:27:53,023 > Epoch: 422,  train_loss=0.000297141,  val_loss=0.680259764,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:28:41,003 > Epoch: 423,  train_loss=0.000295223,  val_loss=0.679699183,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:29:28,995 > Epoch: 424,  train_loss=0.000291175,  val_loss=0.682365656,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:30:16,902 > Epoch: 425,  train_loss=0.000293290,  val_loss=0.681139886,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:31:04,813 > Epoch: 426,  train_loss=0.000294147,  val_loss=0.681234002,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:31:52,773 > Epoch: 427,  train_loss=0.000294623,  val_loss=0.683056355,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:32:40,707 > Epoch: 428,  train_loss=0.000295306,  val_loss=0.681429684,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:33:28,656 > Epoch: 429,  train_loss=0.000294348,  val_loss=0.682733655,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:34:16,697 > Epoch: 430,  train_loss=0.000292796,  val_loss=0.681953430,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:35:04,673 > Epoch: 431,  train_loss=0.000293043,  val_loss=0.682237387,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:35:52,616 > Epoch: 432,  train_loss=0.000293112,  val_loss=0.682452440,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:36:40,605 > Epoch: 433,  train_loss=0.000293796,  val_loss=0.682320058,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:37:28,551 > Epoch: 434,  train_loss=0.000294862,  val_loss=0.682519436,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:38:16,451 > Epoch: 435,  train_loss=0.000292639,  val_loss=0.682825089,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:39:04,356 > Epoch: 436,  train_loss=0.000291340,  val_loss=0.682520092,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:39:52,348 > Epoch: 437,  train_loss=0.000292637,  val_loss=0.683323264,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:40:40,334 > Epoch: 438,  train_loss=0.000292618,  val_loss=0.683354974,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:41:28,339 > Epoch: 439,  train_loss=0.000292468,  val_loss=0.683524609,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:42:16,315 > Epoch: 440,  train_loss=0.000292186,  val_loss=0.682138145,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:43:04,237 > Epoch: 441,  train_loss=0.000293181,  val_loss=0.683016062,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:43:52,136 > Epoch: 442,  train_loss=0.000293860,  val_loss=0.683363914,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:44:40,111 > Epoch: 443,  train_loss=0.000291993,  val_loss=0.683960080,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:45:28,038 > Epoch: 444,  train_loss=0.000287039,  val_loss=0.684581757,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:46:16,035 > Epoch: 445,  train_loss=0.000292303,  val_loss=0.684185982,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:47:04,055 > Epoch: 446,  train_loss=0.000291166,  val_loss=0.685346365,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:47:52,021 > Epoch: 447,  train_loss=0.000288965,  val_loss=0.684234321,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:48:40,030 > Epoch: 448,  train_loss=0.000288535,  val_loss=0.684142411,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:49:27,956 > Epoch: 449,  train_loss=0.000290324,  val_loss=0.683679283,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:50:15,860 > Epoch: 450,  train_loss=0.000287049,  val_loss=0.684635043,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:51:03,754 > Epoch: 451,  train_loss=0.000286798,  val_loss=0.683503807,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:51:51,658 > Epoch: 452,  train_loss=0.000289251,  val_loss=0.685635567,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:52:39,665 > Epoch: 453,  train_loss=0.000286644,  val_loss=0.685171843,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:53:27,658 > Epoch: 454,  train_loss=0.000286551,  val_loss=0.685675979,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:54:15,657 > Epoch: 455,  train_loss=0.000288765,  val_loss=0.687282920,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:55:03,608 > Epoch: 456,  train_loss=0.000286628,  val_loss=0.687094569,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:55:51,532 > Epoch: 457,  train_loss=0.000287775,  val_loss=0.688701272,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:56:39,494 > Epoch: 458,  train_loss=0.000289389,  val_loss=0.689777792,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:57:27,428 > Epoch: 459,  train_loss=0.000287414,  val_loss=0.689755201,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:58:15,357 > Epoch: 460,  train_loss=0.000286833,  val_loss=0.688821018,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:59:03,393 > Epoch: 461,  train_loss=0.000289591,  val_loss=0.688325465,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 22:59:51,387 > Epoch: 462,  train_loss=0.000286223,  val_loss=0.688410342,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:00:39,390 > Epoch: 463,  train_loss=0.000285719,  val_loss=0.689514518,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:01:27,418 > Epoch: 464,  train_loss=0.000288608,  val_loss=0.690957844,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:02:15,313 > Epoch: 465,  train_loss=0.000285443,  val_loss=0.689663589,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:03:03,293 > Epoch: 466,  train_loss=0.000286004,  val_loss=0.690344632,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:03:51,208 > Epoch: 467,  train_loss=0.000288807,  val_loss=0.690557599,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:04:39,230 > Epoch: 468,  train_loss=0.000285398,  val_loss=0.690702081,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:05:27,216 > Epoch: 469,  train_loss=0.000284511,  val_loss=0.689406753,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:06:15,169 > Epoch: 470,  train_loss=0.000289429,  val_loss=0.690171838,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:07:03,190 > Epoch: 471,  train_loss=0.000287441,  val_loss=0.691074431,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:07:51,196 > Epoch: 472,  train_loss=0.000286156,  val_loss=0.692712545,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:08:39,113 > Epoch: 473,  train_loss=0.000290371,  val_loss=0.691775680,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:09:27,041 > Epoch: 474,  train_loss=0.000294107,  val_loss=0.691176534,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:10:14,947 > Epoch: 475,  train_loss=0.000290589,  val_loss=0.691690922,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:11:02,869 > Epoch: 476,  train_loss=0.000291140,  val_loss=0.690669656,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:11:50,831 > Epoch: 477,  train_loss=0.000291311,  val_loss=0.690685630,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:12:38,870 > Epoch: 478,  train_loss=0.000285225,  val_loss=0.690665305,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:13:26,907 > Epoch: 479,  train_loss=0.000287540,  val_loss=0.690893829,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:14:14,875 > Epoch: 480,  train_loss=0.000291891,  val_loss=0.690963089,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:15:02,825 > Epoch: 481,  train_loss=0.000290276,  val_loss=0.690010846,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:15:50,793 > Epoch: 482,  train_loss=0.000287295,  val_loss=0.692009926,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:16:38,710 > Epoch: 483,  train_loss=0.000287747,  val_loss=0.691552103,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:17:26,647 > Epoch: 484,  train_loss=0.000289637,  val_loss=0.691990077,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:18:14,655 > Epoch: 485,  train_loss=0.000284416,  val_loss=0.691834211,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:19:02,584 > Epoch: 486,  train_loss=0.000285512,  val_loss=0.693034530,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:19:50,518 > Epoch: 487,  train_loss=0.000288842,  val_loss=0.693460047,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:20:38,451 > Epoch: 488,  train_loss=0.000291616,  val_loss=0.693112850,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:21:26,413 > Epoch: 489,  train_loss=0.000287593,  val_loss=0.692273378,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:22:14,423 > Epoch: 490,  train_loss=0.000288036,  val_loss=0.692671537,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:23:02,449 > Epoch: 491,  train_loss=0.000289477,  val_loss=0.695745409,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:23:50,403 > Epoch: 492,  train_loss=0.000286597,  val_loss=0.694964170,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:24:38,325 > Epoch: 493,  train_loss=0.000286884,  val_loss=0.696061611,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:25:26,248 > Epoch: 494,  train_loss=0.000289875,  val_loss=0.695523322,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:26:14,227 > Epoch: 495,  train_loss=0.000291512,  val_loss=0.694597244,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:27:02,147 > Epoch: 496,  train_loss=0.000286750,  val_loss=0.695130348,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:27:50,156 > Epoch: 497,  train_loss=0.000287990,  val_loss=0.694127083,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:28:38,106 > Epoch: 498,  train_loss=0.000290695,  val_loss=0.693959832,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:29:26,124 > Epoch: 499,  train_loss=0.000293985,  val_loss=0.694009125,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:30:14,090 > Epoch: 500,  train_loss=0.000295327,  val_loss=0.692171931,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:31:02,058 > Epoch: 501,  train_loss=0.000291880,  val_loss=0.691052556,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:31:49,966 > Epoch: 502,  train_loss=0.000293417,  val_loss=0.691814899,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:32:37,867 > Epoch: 503,  train_loss=0.000288622,  val_loss=0.694209993,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:33:25,808 > Epoch: 504,  train_loss=0.000289992,  val_loss=0.692090988,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:34:13,787 > Epoch: 505,  train_loss=0.000291830,  val_loss=0.695318520,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:35:01,794 > Epoch: 506,  train_loss=0.000296525,  val_loss=0.694433570,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:35:49,839 > Epoch: 507,  train_loss=0.000288825,  val_loss=0.696826994,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:36:37,815 > Epoch: 508,  train_loss=0.000291704,  val_loss=0.695093811,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:37:25,723 > Epoch: 509,  train_loss=0.000293086,  val_loss=0.694485188,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:38:13,652 > Epoch: 510,  train_loss=0.000294208,  val_loss=0.694991410,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:39:01,551 > Epoch: 511,  train_loss=0.000288398,  val_loss=0.694560766,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:39:49,467 > Epoch: 512,  train_loss=0.000289064,  val_loss=0.694289565,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:40:37,502 > Epoch: 513,  train_loss=0.000291228,  val_loss=0.692940414,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:41:25,548 > Epoch: 514,  train_loss=0.000293194,  val_loss=0.693815947,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:42:13,521 > Epoch: 515,  train_loss=0.000286096,  val_loss=0.694832504,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:43:01,480 > Epoch: 516,  train_loss=0.000287882,  val_loss=0.694943547,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:43:49,410 > Epoch: 517,  train_loss=0.000291284,  val_loss=0.696100950,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:44:37,290 > Epoch: 518,  train_loss=0.000291894,  val_loss=0.696310759,  accuracy=0.914062500
[INFO|asb_main.py:823] 2018-10-24 23:45:25,253 > Epoch: 519,  train_loss=0.000285317,  val_loss=0.699694932,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:46:13,160 > Epoch: 520,  train_loss=0.000287134,  val_loss=0.699279904,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:47:01,110 > Epoch: 521,  train_loss=0.000290099,  val_loss=0.701079786,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:47:49,133 > Epoch: 522,  train_loss=0.000292537,  val_loss=0.700193405,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:48:37,085 > Epoch: 523,  train_loss=0.000285741,  val_loss=0.702970803,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:49:25,039 > Epoch: 524,  train_loss=0.000287538,  val_loss=0.704016030,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:50:12,957 > Epoch: 525,  train_loss=0.000290380,  val_loss=0.703356862,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:51:00,868 > Epoch: 526,  train_loss=0.000292631,  val_loss=0.703164876,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:51:48,853 > Epoch: 527,  train_loss=0.000285189,  val_loss=0.704993606,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:52:36,789 > Epoch: 528,  train_loss=0.000287560,  val_loss=0.703046381,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:53:24,781 > Epoch: 529,  train_loss=0.000289448,  val_loss=0.701783657,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:54:12,743 > Epoch: 530,  train_loss=0.000290816,  val_loss=0.705963016,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:55:00,689 > Epoch: 531,  train_loss=0.000283686,  val_loss=0.709697425,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:55:48,659 > Epoch: 532,  train_loss=0.000286825,  val_loss=0.707984328,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:56:36,679 > Epoch: 533,  train_loss=0.000288437,  val_loss=0.709700227,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:57:24,583 > Epoch: 534,  train_loss=0.000287921,  val_loss=0.712277949,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:58:12,511 > Epoch: 535,  train_loss=0.000293633,  val_loss=0.711507678,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:59:00,410 > Epoch: 536,  train_loss=0.000287085,  val_loss=0.714624107,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-24 23:59:48,404 > Epoch: 537,  train_loss=0.000287958,  val_loss=0.712683976,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:00:36,371 > Epoch: 538,  train_loss=0.000289129,  val_loss=0.714138389,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:01:24,414 > Epoch: 539,  train_loss=0.000291530,  val_loss=0.715836346,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:02:12,380 > Epoch: 540,  train_loss=0.000283821,  val_loss=0.719565988,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:03:00,265 > Epoch: 541,  train_loss=0.000286054,  val_loss=0.716508210,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:03:48,236 > Epoch: 542,  train_loss=0.000288038,  val_loss=0.717307031,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:04:36,162 > Epoch: 543,  train_loss=0.000290268,  val_loss=0.717002630,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:05:24,081 > Epoch: 544,  train_loss=0.000290762,  val_loss=0.718924522,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:06:12,025 > Epoch: 545,  train_loss=0.000286584,  val_loss=0.725626051,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:07:00,052 > Epoch: 546,  train_loss=0.000289505,  val_loss=0.721950173,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:07:48,041 > Epoch: 547,  train_loss=0.000289518,  val_loss=0.721386254,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:08:36,022 > Epoch: 548,  train_loss=0.000290638,  val_loss=0.722737789,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:09:23,976 > Epoch: 549,  train_loss=0.000284644,  val_loss=0.726534367,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:10:11,881 > Epoch: 550,  train_loss=0.000286081,  val_loss=0.726491809,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:10:59,798 > Epoch: 551,  train_loss=0.000289415,  val_loss=0.727091312,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:11:47,714 > Epoch: 552,  train_loss=0.000290643,  val_loss=0.723825693,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:12:35,678 > Epoch: 553,  train_loss=0.000281631,  val_loss=0.733494043,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:13:23,708 > Epoch: 554,  train_loss=0.000284365,  val_loss=0.735401392,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:14:11,704 > Epoch: 555,  train_loss=0.000288939,  val_loss=0.732737005,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:14:59,660 > Epoch: 556,  train_loss=0.000289084,  val_loss=0.733125210,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:15:47,562 > Epoch: 557,  train_loss=0.000290913,  val_loss=0.732118189,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:16:35,492 > Epoch: 558,  train_loss=0.000283603,  val_loss=0.742125630,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:17:23,391 > Epoch: 559,  train_loss=0.000287363,  val_loss=0.737722397,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:18:11,303 > Epoch: 560,  train_loss=0.000287978,  val_loss=0.740884721,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:18:59,309 > Epoch: 561,  train_loss=0.000289602,  val_loss=0.742308557,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:19:47,275 > Epoch: 562,  train_loss=0.000281458,  val_loss=0.754403234,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:20:35,247 > Epoch: 563,  train_loss=0.000285784,  val_loss=0.749635458,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:21:23,234 > Epoch: 564,  train_loss=0.000287048,  val_loss=0.752724946,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:22:11,127 > Epoch: 565,  train_loss=0.000288324,  val_loss=0.754937112,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:22:59,033 > Epoch: 566,  train_loss=0.000290019,  val_loss=0.757127881,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:23:46,918 > Epoch: 567,  train_loss=0.000279503,  val_loss=0.765524268,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:24:34,827 > Epoch: 568,  train_loss=0.000283468,  val_loss=0.763734281,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:25:22,792 > Epoch: 569,  train_loss=0.000285329,  val_loss=0.759684682,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:26:10,761 > Epoch: 570,  train_loss=0.000287061,  val_loss=0.763341427,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:26:58,727 > Epoch: 571,  train_loss=0.000287833,  val_loss=0.761216402,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:27:46,747 > Epoch: 572,  train_loss=0.000288669,  val_loss=0.760892868,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:28:34,652 > Epoch: 573,  train_loss=0.000280588,  val_loss=0.774034739,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:29:22,597 > Epoch: 574,  train_loss=0.000282512,  val_loss=0.767985642,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:30:10,510 > Epoch: 575,  train_loss=0.000283225,  val_loss=0.771053553,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:30:58,450 > Epoch: 576,  train_loss=0.000285466,  val_loss=0.769990087,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:31:46,426 > Epoch: 577,  train_loss=0.000286880,  val_loss=0.772624433,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:32:34,441 > Epoch: 578,  train_loss=0.000288241,  val_loss=0.772786736,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:33:22,447 > Epoch: 579,  train_loss=0.000279134,  val_loss=0.780955434,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:34:10,425 > Epoch: 580,  train_loss=0.000280335,  val_loss=0.779936612,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:34:58,345 > Epoch: 581,  train_loss=0.000282955,  val_loss=0.783063173,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:35:46,216 > Epoch: 582,  train_loss=0.000284388,  val_loss=0.785203338,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:36:34,121 > Epoch: 583,  train_loss=0.000286584,  val_loss=0.785896778,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:37:22,079 > Epoch: 584,  train_loss=0.000286989,  val_loss=0.782213867,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:38:10,129 > Epoch: 585,  train_loss=0.000277191,  val_loss=0.793323874,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:38:58,146 > Epoch: 586,  train_loss=0.000279665,  val_loss=0.791875899,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:39:46,195 > Epoch: 587,  train_loss=0.000280080,  val_loss=0.788859904,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:40:34,156 > Epoch: 588,  train_loss=0.000283016,  val_loss=0.793385506,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:41:22,050 > Epoch: 589,  train_loss=0.000284019,  val_loss=0.791052639,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:42:09,959 > Epoch: 590,  train_loss=0.000285125,  val_loss=0.791084766,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:42:57,861 > Epoch: 591,  train_loss=0.000276314,  val_loss=0.808224916,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:43:45,781 > Epoch: 592,  train_loss=0.000279668,  val_loss=0.803790867,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:44:33,769 > Epoch: 593,  train_loss=0.000282378,  val_loss=0.803595781,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:45:21,701 > Epoch: 594,  train_loss=0.000283210,  val_loss=0.805772901,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:46:09,703 > Epoch: 595,  train_loss=0.000285001,  val_loss=0.805647016,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:46:57,622 > Epoch: 596,  train_loss=0.000276170,  val_loss=0.816308260,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:47:45,542 > Epoch: 597,  train_loss=0.000280503,  val_loss=0.815891504,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:48:33,466 > Epoch: 598,  train_loss=0.000281942,  val_loss=0.816325307,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:49:21,419 > Epoch: 599,  train_loss=0.000285478,  val_loss=0.816503704,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:50:09,394 > Epoch: 600,  train_loss=0.000287252,  val_loss=0.811523676,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:50:57,384 > Epoch: 601,  train_loss=0.000276551,  val_loss=0.828006983,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:51:45,339 > Epoch: 602,  train_loss=0.000277736,  val_loss=0.821270823,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:52:33,313 > Epoch: 603,  train_loss=0.000280177,  val_loss=0.818786323,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:53:21,227 > Epoch: 604,  train_loss=0.000281854,  val_loss=0.819776893,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:54:09,150 > Epoch: 605,  train_loss=0.000283657,  val_loss=0.820297360,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:54:57,101 > Epoch: 606,  train_loss=0.000286347,  val_loss=0.820329607,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:55:45,097 > Epoch: 607,  train_loss=0.000276822,  val_loss=0.831974387,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:56:33,120 > Epoch: 608,  train_loss=0.000277374,  val_loss=0.831113279,  accuracy=0.906250000
[INFO|asb_main.py:823] 2018-10-25 00:57:21,070 > Epoch: 609,  train_loss=0.000282740,  val_loss=0.837289691,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-25 00:58:09,060 > Epoch: 610,  train_loss=0.000283804,  val_loss=0.836737216,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-25 00:58:56,957 > Epoch: 611,  train_loss=0.000284490,  val_loss=0.841830671,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-25 00:59:44,913 > Epoch: 612,  train_loss=0.000287396,  val_loss=0.839558363,  accuracy=0.898437500
[INFO|asb_main.py:823] 2018-10-25 01:00:32,895 > Epoch: 613,  train_loss=0.000274888,  val_loss=0.863567472,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-25 01:01:20,894 > Epoch: 614,  train_loss=0.000279658,  val_loss=0.867951632,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-25 01:02:08,915 > Epoch: 615,  train_loss=0.000281340,  val_loss=0.855989099,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-25 01:02:56,875 > Epoch: 616,  train_loss=0.000281996,  val_loss=0.860933304,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-25 01:03:44,852 > Epoch: 617,  train_loss=0.000282818,  val_loss=0.864608049,  accuracy=0.890625000
[INFO|asb_main.py:823] 2018-10-25 01:04:32,790 > Epoch: 618,  train_loss=0.000284497,  val_loss=0.868668795,  accuracy=0.882812500
[INFO|asb_main.py:823] 2018-10-25 01:05:20,715 > Epoch: 619,  train_loss=0.000274579,  val_loss=0.898561835,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:06:08,615 > Epoch: 620,  train_loss=0.000279216,  val_loss=0.901147246,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:06:56,551 > Epoch: 621,  train_loss=0.000280829,  val_loss=0.903476775,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:07:44,539 > Epoch: 622,  train_loss=0.000282731,  val_loss=0.903426468,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:08:32,529 > Epoch: 623,  train_loss=0.000283603,  val_loss=0.904746413,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:09:20,510 > Epoch: 624,  train_loss=0.000273603,  val_loss=0.944587708,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:10:08,514 > Epoch: 625,  train_loss=0.000277371,  val_loss=0.933911443,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:10:56,475 > Epoch: 626,  train_loss=0.000280574,  val_loss=0.941639721,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:11:44,359 > Epoch: 627,  train_loss=0.000280965,  val_loss=0.949724257,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:12:32,301 > Epoch: 628,  train_loss=0.000283270,  val_loss=0.948147893,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:13:20,246 > Epoch: 629,  train_loss=0.000284016,  val_loss=0.947907805,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:14:08,141 > Epoch: 630,  train_loss=0.000274113,  val_loss=0.998304069,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:14:56,140 > Epoch: 631,  train_loss=0.000278887,  val_loss=0.977295041,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:15:44,115 > Epoch: 632,  train_loss=0.000279787,  val_loss=0.981191933,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:16:32,090 > Epoch: 633,  train_loss=0.000281843,  val_loss=0.981779397,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:17:20,067 > Epoch: 634,  train_loss=0.000284089,  val_loss=0.974792004,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:18:08,009 > Epoch: 635,  train_loss=0.000284029,  val_loss=0.978819191,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:18:55,931 > Epoch: 636,  train_loss=0.000274347,  val_loss=1.013016701,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:19:43,842 > Epoch: 637,  train_loss=0.000278492,  val_loss=0.999282300,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:20:31,759 > Epoch: 638,  train_loss=0.000279978,  val_loss=0.997499585,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:21:19,787 > Epoch: 639,  train_loss=0.000281528,  val_loss=1.000823498,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:22:07,732 > Epoch: 640,  train_loss=0.000282497,  val_loss=1.000837564,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:22:55,705 > Epoch: 641,  train_loss=0.000283787,  val_loss=1.005056620,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:23:43,660 > Epoch: 642,  train_loss=0.000273756,  val_loss=1.048115969,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:24:31,569 > Epoch: 643,  train_loss=0.000276536,  val_loss=1.033350348,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:25:19,511 > Epoch: 644,  train_loss=0.000277360,  val_loss=1.028266907,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:26:07,444 > Epoch: 645,  train_loss=0.000278843,  val_loss=1.029206038,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:26:55,424 > Epoch: 646,  train_loss=0.000281619,  val_loss=1.027052522,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 01:27:43,397 > Epoch: 647,  train_loss=0.000282386,  val_loss=1.026831508,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:28:31,413 > Epoch: 648,  train_loss=0.000282778,  val_loss=1.018606305,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:29:19,415 > Epoch: 649,  train_loss=0.000271868,  val_loss=1.061711073,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:30:07,447 > Epoch: 650,  train_loss=0.000276743,  val_loss=1.046720028,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:30:55,368 > Epoch: 651,  train_loss=0.000276131,  val_loss=1.058719397,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:31:43,290 > Epoch: 652,  train_loss=0.000281954,  val_loss=1.056341052,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:32:31,240 > Epoch: 653,  train_loss=0.000281390,  val_loss=1.055589676,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:33:19,164 > Epoch: 654,  train_loss=0.000281959,  val_loss=1.054725409,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:34:07,166 > Epoch: 655,  train_loss=0.000283125,  val_loss=1.054696441,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:34:55,158 > Epoch: 656,  train_loss=0.000270816,  val_loss=1.099161386,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:35:43,141 > Epoch: 657,  train_loss=0.000276283,  val_loss=1.094425321,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:36:31,112 > Epoch: 658,  train_loss=0.000279149,  val_loss=1.095523477,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:37:19,010 > Epoch: 659,  train_loss=0.000278933,  val_loss=1.103639960,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:38:06,917 > Epoch: 660,  train_loss=0.000281531,  val_loss=1.106885791,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:38:54,854 > Epoch: 661,  train_loss=0.000281524,  val_loss=1.099717140,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:39:42,886 > Epoch: 662,  train_loss=0.000282446,  val_loss=1.105581760,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:40:30,885 > Epoch: 663,  train_loss=0.000269779,  val_loss=1.153921723,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:41:18,869 > Epoch: 664,  train_loss=0.000276077,  val_loss=1.143802643,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:42:06,883 > Epoch: 665,  train_loss=0.000277218,  val_loss=1.142634153,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:42:54,891 > Epoch: 666,  train_loss=0.000279037,  val_loss=1.143112063,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:43:42,794 > Epoch: 667,  train_loss=0.000280543,  val_loss=1.139437318,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:44:30,727 > Epoch: 668,  train_loss=0.000279581,  val_loss=1.148478985,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:45:18,635 > Epoch: 669,  train_loss=0.000281532,  val_loss=1.140795946,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:46:06,617 > Epoch: 670,  train_loss=0.000268038,  val_loss=1.180998564,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:46:54,597 > Epoch: 671,  train_loss=0.000273820,  val_loss=1.183821559,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:47:42,563 > Epoch: 672,  train_loss=0.000274980,  val_loss=1.188700438,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:48:30,544 > Epoch: 673,  train_loss=0.000277141,  val_loss=1.192294359,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:49:18,517 > Epoch: 674,  train_loss=0.000277364,  val_loss=1.196017742,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:50:06,480 > Epoch: 675,  train_loss=0.000278990,  val_loss=1.209070086,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:50:54,445 > Epoch: 676,  train_loss=0.000279444,  val_loss=1.216278195,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:51:42,377 > Epoch: 677,  train_loss=0.000268280,  val_loss=1.275973320,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:52:30,283 > Epoch: 678,  train_loss=0.000273950,  val_loss=1.252656937,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:53:18,291 > Epoch: 679,  train_loss=0.000275659,  val_loss=1.255001068,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:54:06,315 > Epoch: 680,  train_loss=0.000277802,  val_loss=1.257995605,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:54:54,276 > Epoch: 681,  train_loss=0.000277892,  val_loss=1.252879858,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:55:42,248 > Epoch: 682,  train_loss=0.000279488,  val_loss=1.253228664,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:56:30,151 > Epoch: 683,  train_loss=0.000280335,  val_loss=1.257055879,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:57:18,125 > Epoch: 684,  train_loss=0.000266498,  val_loss=1.299756169,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:58:06,042 > Epoch: 685,  train_loss=0.000275415,  val_loss=1.287877321,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:58:53,929 > Epoch: 686,  train_loss=0.000274960,  val_loss=1.291672945,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 01:59:41,909 > Epoch: 687,  train_loss=0.000277020,  val_loss=1.293232560,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:00:29,894 > Epoch: 688,  train_loss=0.000277434,  val_loss=1.298852205,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:01:17,877 > Epoch: 689,  train_loss=0.000279281,  val_loss=1.303338647,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:02:05,867 > Epoch: 690,  train_loss=0.000279918,  val_loss=1.297328234,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:02:53,769 > Epoch: 691,  train_loss=0.000265221,  val_loss=1.348087788,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:03:41,726 > Epoch: 692,  train_loss=0.000273506,  val_loss=1.329777956,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:04:29,654 > Epoch: 693,  train_loss=0.000274029,  val_loss=1.335939407,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:05:17,611 > Epoch: 694,  train_loss=0.000277451,  val_loss=1.340239286,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:06:05,576 > Epoch: 695,  train_loss=0.000277640,  val_loss=1.330663085,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:06:53,590 > Epoch: 696,  train_loss=0.000278776,  val_loss=1.338813305,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:07:41,562 > Epoch: 697,  train_loss=0.000279457,  val_loss=1.334361792,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:08:29,520 > Epoch: 698,  train_loss=0.000281084,  val_loss=1.338103533,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:09:17,464 > Epoch: 699,  train_loss=0.000270148,  val_loss=1.386686206,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:10:05,419 > Epoch: 700,  train_loss=0.000274451,  val_loss=1.374766350,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:10:53,333 > Epoch: 701,  train_loss=0.000272960,  val_loss=1.374407530,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:11:41,252 > Epoch: 702,  train_loss=0.000276131,  val_loss=1.378804922,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:12:29,213 > Epoch: 703,  train_loss=0.000279991,  val_loss=1.370432973,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:13:17,216 > Epoch: 704,  train_loss=0.000279865,  val_loss=1.366735816,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:14:05,195 > Epoch: 705,  train_loss=0.000280337,  val_loss=1.360334396,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:14:53,207 > Epoch: 706,  train_loss=0.000269107,  val_loss=1.414965391,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:15:41,154 > Epoch: 707,  train_loss=0.000274346,  val_loss=1.416900396,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:16:29,093 > Epoch: 708,  train_loss=0.000277761,  val_loss=1.401609421,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:17:16,985 > Epoch: 709,  train_loss=0.000275464,  val_loss=1.398793221,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:18:04,906 > Epoch: 710,  train_loss=0.000278241,  val_loss=1.394064665,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:18:52,891 > Epoch: 711,  train_loss=0.000279156,  val_loss=1.392850757,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:19:40,883 > Epoch: 712,  train_loss=0.000280620,  val_loss=1.391663671,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:20:28,884 > Epoch: 713,  train_loss=0.000268283,  val_loss=1.441270828,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:21:16,859 > Epoch: 714,  train_loss=0.000274444,  val_loss=1.426341295,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:22:04,757 > Epoch: 715,  train_loss=0.000277847,  val_loss=1.423944473,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:22:52,703 > Epoch: 716,  train_loss=0.000279010,  val_loss=1.418181419,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:23:40,596 > Epoch: 717,  train_loss=0.000276090,  val_loss=1.418746710,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:24:28,542 > Epoch: 718,  train_loss=0.000279435,  val_loss=1.412194252,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:25:16,541 > Epoch: 719,  train_loss=0.000279188,  val_loss=1.419614792,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:26:04,521 > Epoch: 720,  train_loss=0.000267869,  val_loss=1.473490357,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:26:52,519 > Epoch: 721,  train_loss=0.000273971,  val_loss=1.460435390,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:27:40,555 > Epoch: 722,  train_loss=0.000274155,  val_loss=1.451795459,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:28:28,487 > Epoch: 723,  train_loss=0.000275879,  val_loss=1.452075124,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:29:16,447 > Epoch: 724,  train_loss=0.000277045,  val_loss=1.455662131,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:30:04,336 > Epoch: 725,  train_loss=0.000278103,  val_loss=1.452643514,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:30:52,269 > Epoch: 726,  train_loss=0.000278616,  val_loss=1.448875546,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:31:40,236 > Epoch: 727,  train_loss=0.000266327,  val_loss=1.499278665,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:32:28,279 > Epoch: 728,  train_loss=0.000271910,  val_loss=1.481737137,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:33:16,249 > Epoch: 729,  train_loss=0.000273087,  val_loss=1.479646444,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:34:04,216 > Epoch: 730,  train_loss=0.000274042,  val_loss=1.482862592,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:34:52,192 > Epoch: 731,  train_loss=0.000275471,  val_loss=1.476353407,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:35:40,097 > Epoch: 732,  train_loss=0.000278158,  val_loss=1.481408000,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:36:28,002 > Epoch: 733,  train_loss=0.000279434,  val_loss=1.488883734,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:37:15,928 > Epoch: 734,  train_loss=0.000266254,  val_loss=1.551767468,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:38:03,940 > Epoch: 735,  train_loss=0.000270571,  val_loss=1.546801686,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:38:51,928 > Epoch: 736,  train_loss=0.000272776,  val_loss=1.538764596,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:39:39,953 > Epoch: 737,  train_loss=0.000275080,  val_loss=1.536371708,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:40:27,929 > Epoch: 738,  train_loss=0.000276392,  val_loss=1.528444290,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:41:15,842 > Epoch: 739,  train_loss=0.000276206,  val_loss=1.539299488,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:42:03,756 > Epoch: 740,  train_loss=0.000277385,  val_loss=1.536620617,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:42:51,701 > Epoch: 741,  train_loss=0.000278344,  val_loss=1.534027457,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:43:39,645 > Epoch: 742,  train_loss=0.000264609,  val_loss=1.585754275,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:44:27,605 > Epoch: 743,  train_loss=0.000266749,  val_loss=1.577627301,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:45:15,602 > Epoch: 744,  train_loss=0.000270747,  val_loss=1.573673964,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:46:03,623 > Epoch: 745,  train_loss=0.000272143,  val_loss=1.579225659,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:46:51,639 > Epoch: 746,  train_loss=0.000274659,  val_loss=1.568560600,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:47:39,582 > Epoch: 747,  train_loss=0.000275920,  val_loss=1.567282438,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:48:27,529 > Epoch: 748,  train_loss=0.000276292,  val_loss=1.566314101,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:49:15,422 > Epoch: 749,  train_loss=0.000276101,  val_loss=1.573130369,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:50:03,344 > Epoch: 750,  train_loss=0.000261591,  val_loss=1.643188238,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:50:51,368 > Epoch: 751,  train_loss=0.000265529,  val_loss=1.630776882,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:51:39,330 > Epoch: 752,  train_loss=0.000269365,  val_loss=1.619602561,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:52:27,313 > Epoch: 753,  train_loss=0.000271579,  val_loss=1.613627315,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:53:15,289 > Epoch: 754,  train_loss=0.000273449,  val_loss=1.616863489,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:54:03,252 > Epoch: 755,  train_loss=0.000274498,  val_loss=1.610700607,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:54:51,195 > Epoch: 756,  train_loss=0.000275286,  val_loss=1.619942784,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:55:39,143 > Epoch: 757,  train_loss=0.000275150,  val_loss=1.626264215,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:56:27,078 > Epoch: 758,  train_loss=0.000260586,  val_loss=1.687969565,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:57:15,060 > Epoch: 759,  train_loss=0.000270849,  val_loss=1.678027153,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:58:03,020 > Epoch: 760,  train_loss=0.000269458,  val_loss=1.679766893,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:58:51,156 > Epoch: 761,  train_loss=0.000270016,  val_loss=1.693306923,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 02:59:39,145 > Epoch: 762,  train_loss=0.000275170,  val_loss=1.688828349,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:00:27,139 > Epoch: 763,  train_loss=0.000275903,  val_loss=1.694701076,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:01:15,050 > Epoch: 764,  train_loss=0.000276328,  val_loss=1.697480679,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:02:02,956 > Epoch: 765,  train_loss=0.000275742,  val_loss=1.700205564,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:02:50,951 > Epoch: 766,  train_loss=0.000259984,  val_loss=1.769521236,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:03:38,974 > Epoch: 767,  train_loss=0.000268290,  val_loss=1.762863874,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:04:26,931 > Epoch: 768,  train_loss=0.000266004,  val_loss=1.765420437,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:05:14,951 > Epoch: 769,  train_loss=0.000269508,  val_loss=1.760465980,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:06:02,933 > Epoch: 770,  train_loss=0.000271377,  val_loss=1.754301786,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:06:50,891 > Epoch: 771,  train_loss=0.000272522,  val_loss=1.764096737,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:07:38,824 > Epoch: 772,  train_loss=0.000273566,  val_loss=1.759342194,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:08:26,745 > Epoch: 773,  train_loss=0.000274574,  val_loss=1.769575834,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:09:14,774 > Epoch: 774,  train_loss=0.000258573,  val_loss=1.844983578,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:10:02,787 > Epoch: 775,  train_loss=0.000269260,  val_loss=1.843348861,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:10:50,781 > Epoch: 776,  train_loss=0.000269745,  val_loss=1.837699652,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:11:38,738 > Epoch: 777,  train_loss=0.000268356,  val_loss=1.854577065,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:12:26,756 > Epoch: 778,  train_loss=0.000269904,  val_loss=1.854450345,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:13:14,702 > Epoch: 779,  train_loss=0.000272597,  val_loss=1.852726698,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:14:02,657 > Epoch: 780,  train_loss=0.000273343,  val_loss=1.846212864,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:14:50,627 > Epoch: 781,  train_loss=0.000274005,  val_loss=1.848890901,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:15:38,628 > Epoch: 782,  train_loss=0.000260701,  val_loss=1.914352298,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:16:26,633 > Epoch: 783,  train_loss=0.000268954,  val_loss=1.891407967,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:17:14,612 > Epoch: 784,  train_loss=0.000270037,  val_loss=1.884916067,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:18:02,608 > Epoch: 785,  train_loss=0.000273421,  val_loss=1.869367480,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:18:50,620 > Epoch: 786,  train_loss=0.000270396,  val_loss=1.876898170,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:19:38,593 > Epoch: 787,  train_loss=0.000272861,  val_loss=1.875267148,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:20:26,548 > Epoch: 788,  train_loss=0.000274161,  val_loss=1.867861152,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:21:14,456 > Epoch: 789,  train_loss=0.000257701,  val_loss=1.934069872,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:22:02,414 > Epoch: 790,  train_loss=0.000265552,  val_loss=1.918266773,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:22:50,394 > Epoch: 791,  train_loss=0.000267084,  val_loss=1.904494166,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:23:38,389 > Epoch: 792,  train_loss=0.000269248,  val_loss=1.902610779,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:24:26,368 > Epoch: 793,  train_loss=0.000270839,  val_loss=1.905039549,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:25:14,370 > Epoch: 794,  train_loss=0.000267204,  val_loss=1.914701223,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:26:02,307 > Epoch: 795,  train_loss=0.000270716,  val_loss=1.915944815,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:26:50,202 > Epoch: 796,  train_loss=0.000271804,  val_loss=1.904363275,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:27:38,147 > Epoch: 797,  train_loss=0.000259639,  val_loss=1.971958280,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:28:26,082 > Epoch: 798,  train_loss=0.000268378,  val_loss=1.956897259,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:29:14,129 > Epoch: 799,  train_loss=0.000270641,  val_loss=1.940607071,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:30:02,104 > Epoch: 800,  train_loss=0.000271452,  val_loss=1.931466460,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:30:50,139 > Epoch: 801,  train_loss=0.000272463,  val_loss=1.945957422,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:31:38,144 > Epoch: 802,  train_loss=0.000272894,  val_loss=1.950215578,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:32:26,072 > Epoch: 803,  train_loss=0.000272747,  val_loss=1.946649313,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:33:13,983 > Epoch: 804,  train_loss=0.000272843,  val_loss=1.943386436,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:34:01,975 > Epoch: 805,  train_loss=0.000258996,  val_loss=2.012796879,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:34:49,867 > Epoch: 806,  train_loss=0.000264055,  val_loss=2.005912066,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:35:37,825 > Epoch: 807,  train_loss=0.000265832,  val_loss=2.009737730,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:36:25,865 > Epoch: 808,  train_loss=0.000272161,  val_loss=2.000422239,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:37:13,840 > Epoch: 809,  train_loss=0.000272897,  val_loss=1.994104505,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:38:01,803 > Epoch: 810,  train_loss=0.000272675,  val_loss=1.988152385,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:38:49,708 > Epoch: 811,  train_loss=0.000273323,  val_loss=1.980559349,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:39:37,650 > Epoch: 812,  train_loss=0.000273986,  val_loss=1.980505943,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:40:25,549 > Epoch: 813,  train_loss=0.000274214,  val_loss=1.991520643,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:41:13,453 > Epoch: 814,  train_loss=0.000260303,  val_loss=2.058189869,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:42:01,475 > Epoch: 815,  train_loss=0.000261425,  val_loss=2.069068193,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:42:49,454 > Epoch: 816,  train_loss=0.000264785,  val_loss=2.050048351,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:43:37,412 > Epoch: 817,  train_loss=0.000266345,  val_loss=2.055197001,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:44:25,396 > Epoch: 818,  train_loss=0.000269039,  val_loss=2.049503803,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:45:13,316 > Epoch: 819,  train_loss=0.000270151,  val_loss=2.038232327,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:46:01,206 > Epoch: 820,  train_loss=0.000271038,  val_loss=2.025281191,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:46:49,126 > Epoch: 821,  train_loss=0.000270751,  val_loss=2.020327568,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:47:37,061 > Epoch: 822,  train_loss=0.000271275,  val_loss=2.032303572,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:48:25,102 > Epoch: 823,  train_loss=0.000253373,  val_loss=2.113516808,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:49:13,083 > Epoch: 824,  train_loss=0.000258199,  val_loss=2.102114916,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:50:01,017 > Epoch: 825,  train_loss=0.000262717,  val_loss=2.100289583,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:50:49,013 > Epoch: 826,  train_loss=0.000265298,  val_loss=2.093104601,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:51:36,947 > Epoch: 827,  train_loss=0.000267306,  val_loss=2.102210045,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:52:24,873 > Epoch: 828,  train_loss=0.000267716,  val_loss=2.097617626,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:53:12,825 > Epoch: 829,  train_loss=0.000269817,  val_loss=2.091517925,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:54:00,745 > Epoch: 830,  train_loss=0.000271265,  val_loss=2.098010778,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:54:48,767 > Epoch: 831,  train_loss=0.000272326,  val_loss=2.084186554,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:55:36,749 > Epoch: 832,  train_loss=0.000257436,  val_loss=2.151921272,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:56:24,694 > Epoch: 833,  train_loss=0.000264899,  val_loss=2.135144711,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:57:12,662 > Epoch: 834,  train_loss=0.000263384,  val_loss=2.140698910,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:58:00,582 > Epoch: 835,  train_loss=0.000265451,  val_loss=2.134645224,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:58:48,531 > Epoch: 836,  train_loss=0.000267382,  val_loss=2.143297195,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 03:59:36,458 > Epoch: 837,  train_loss=0.000268022,  val_loss=2.138576508,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:00:24,383 > Epoch: 838,  train_loss=0.000268957,  val_loss=2.131431103,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:01:12,359 > Epoch: 839,  train_loss=0.000270365,  val_loss=2.132467508,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:02:00,303 > Epoch: 840,  train_loss=0.000269962,  val_loss=2.119559050,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:02:48,287 > Epoch: 841,  train_loss=0.000254130,  val_loss=2.197967768,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:03:36,301 > Epoch: 842,  train_loss=0.000263465,  val_loss=2.191796541,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:04:24,247 > Epoch: 843,  train_loss=0.000260632,  val_loss=2.202698708,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:05:12,152 > Epoch: 844,  train_loss=0.000264560,  val_loss=2.197656393,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:06:00,070 > Epoch: 845,  train_loss=0.000265937,  val_loss=2.184316397,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:06:47,962 > Epoch: 846,  train_loss=0.000267492,  val_loss=2.186453104,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:07:35,924 > Epoch: 847,  train_loss=0.000269307,  val_loss=2.184671640,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:08:23,911 > Epoch: 848,  train_loss=0.000269839,  val_loss=2.188005447,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:09:11,891 > Epoch: 849,  train_loss=0.000270140,  val_loss=2.193485737,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:09:59,883 > Epoch: 850,  train_loss=0.000256008,  val_loss=2.261900902,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:10:47,775 > Epoch: 851,  train_loss=0.000263468,  val_loss=2.248787880,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:11:35,747 > Epoch: 852,  train_loss=0.000260631,  val_loss=2.263178587,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:12:23,653 > Epoch: 853,  train_loss=0.000263673,  val_loss=2.256847382,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:13:11,615 > Epoch: 854,  train_loss=0.000265986,  val_loss=2.260326385,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:13:59,628 > Epoch: 855,  train_loss=0.000267831,  val_loss=2.257845879,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:14:47,629 > Epoch: 856,  train_loss=0.000268357,  val_loss=2.251390696,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:15:35,616 > Epoch: 857,  train_loss=0.000269282,  val_loss=2.247313023,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:16:23,578 > Epoch: 858,  train_loss=0.000255083,  val_loss=2.317642689,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:17:11,553 > Epoch: 859,  train_loss=0.000261411,  val_loss=2.305756092,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:17:59,491 > Epoch: 860,  train_loss=0.000263577,  val_loss=2.303068638,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:18:47,392 > Epoch: 861,  train_loss=0.000261071,  val_loss=2.307912350,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:19:35,337 > Epoch: 862,  train_loss=0.000267353,  val_loss=2.295116663,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:20:23,315 > Epoch: 863,  train_loss=0.000267790,  val_loss=2.298324347,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:21:11,272 > Epoch: 864,  train_loss=0.000268556,  val_loss=2.288134813,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:21:59,270 > Epoch: 865,  train_loss=0.000269755,  val_loss=2.272496462,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:22:47,277 > Epoch: 866,  train_loss=0.000269780,  val_loss=2.267954826,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:23:35,216 > Epoch: 867,  train_loss=0.000256366,  val_loss=2.342566252,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:24:23,166 > Epoch: 868,  train_loss=0.000259808,  val_loss=2.329210758,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:25:11,087 > Epoch: 869,  train_loss=0.000262916,  val_loss=2.337169647,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:25:59,040 > Epoch: 870,  train_loss=0.000258813,  val_loss=2.351512432,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:26:47,035 > Epoch: 871,  train_loss=0.000262211,  val_loss=2.359677792,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:27:35,019 > Epoch: 872,  train_loss=0.000265120,  val_loss=2.353287220,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:28:23,014 > Epoch: 873,  train_loss=0.000268013,  val_loss=2.339465380,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:29:10,989 > Epoch: 874,  train_loss=0.000267653,  val_loss=2.330246925,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:29:58,935 > Epoch: 875,  train_loss=0.000267379,  val_loss=2.337965488,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:30:46,861 > Epoch: 876,  train_loss=0.000268457,  val_loss=2.324946880,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:31:34,796 > Epoch: 877,  train_loss=0.000254876,  val_loss=2.401723385,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:32:22,703 > Epoch: 878,  train_loss=0.000261749,  val_loss=2.386355162,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:33:10,714 > Epoch: 879,  train_loss=0.000260069,  val_loss=2.382665157,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:33:58,702 > Epoch: 880,  train_loss=0.000263188,  val_loss=2.368102312,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:34:46,705 > Epoch: 881,  train_loss=0.000265404,  val_loss=2.361967564,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:35:34,751 > Epoch: 882,  train_loss=0.000267456,  val_loss=2.348660707,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:36:22,702 > Epoch: 883,  train_loss=0.000268020,  val_loss=2.348920584,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:37:10,618 > Epoch: 884,  train_loss=0.000267960,  val_loss=2.346707344,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:37:58,540 > Epoch: 885,  train_loss=0.000268586,  val_loss=2.348715067,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:38:46,440 > Epoch: 886,  train_loss=0.000251674,  val_loss=2.429289818,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:39:34,418 > Epoch: 887,  train_loss=0.000261700,  val_loss=2.409826756,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:40:22,373 > Epoch: 888,  train_loss=0.000257113,  val_loss=2.408303261,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:41:10,367 > Epoch: 889,  train_loss=0.000258059,  val_loss=2.412158728,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:41:58,382 > Epoch: 890,  train_loss=0.000267355,  val_loss=2.401718378,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:42:46,288 > Epoch: 891,  train_loss=0.000267878,  val_loss=2.396384716,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:43:34,187 > Epoch: 892,  train_loss=0.000269004,  val_loss=2.392898560,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:44:22,194 > Epoch: 893,  train_loss=0.000267883,  val_loss=2.374444485,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:45:10,165 > Epoch: 894,  train_loss=0.000268344,  val_loss=2.383493423,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:45:58,163 > Epoch: 895,  train_loss=0.000268123,  val_loss=2.384984255,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:46:46,183 > Epoch: 896,  train_loss=0.000254513,  val_loss=2.456414461,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:47:34,233 > Epoch: 897,  train_loss=0.000260503,  val_loss=2.439495325,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:48:22,187 > Epoch: 898,  train_loss=0.000255651,  val_loss=2.446220875,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:49:10,071 > Epoch: 899,  train_loss=0.000259476,  val_loss=2.445319176,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:49:58,021 > Epoch: 900,  train_loss=0.000262475,  val_loss=2.439903021,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:50:45,972 > Epoch: 901,  train_loss=0.000264225,  val_loss=2.438965797,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:51:33,865 > Epoch: 902,  train_loss=0.000265889,  val_loss=2.433135986,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:52:21,860 > Epoch: 903,  train_loss=0.000266249,  val_loss=2.436080456,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:53:09,817 > Epoch: 904,  train_loss=0.000266759,  val_loss=2.435302258,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:53:57,850 > Epoch: 905,  train_loss=0.000251175,  val_loss=2.508535862,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:54:45,853 > Epoch: 906,  train_loss=0.000260217,  val_loss=2.499522686,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 04:55:33,783 > Epoch: 907,  train_loss=0.000262090,  val_loss=2.495200872,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:56:21,716 > Epoch: 908,  train_loss=0.000257658,  val_loss=2.500212431,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:57:09,644 > Epoch: 909,  train_loss=0.000261755,  val_loss=2.495929241,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:57:57,556 > Epoch: 910,  train_loss=0.000263513,  val_loss=2.499724388,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:58:45,513 > Epoch: 911,  train_loss=0.000264633,  val_loss=2.503449917,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 04:59:33,506 > Epoch: 912,  train_loss=0.000265236,  val_loss=2.501873732,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:00:21,486 > Epoch: 913,  train_loss=0.000247801,  val_loss=2.584995508,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:01:09,431 > Epoch: 914,  train_loss=0.000257961,  val_loss=2.571532488,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:01:57,360 > Epoch: 915,  train_loss=0.000259608,  val_loss=2.569899082,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:02:45,301 > Epoch: 916,  train_loss=0.000260960,  val_loss=2.557605982,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:03:33,223 > Epoch: 917,  train_loss=0.000263485,  val_loss=2.564827919,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:04:21,153 > Epoch: 918,  train_loss=0.000259672,  val_loss=2.574062824,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:05:09,133 > Epoch: 919,  train_loss=0.000262259,  val_loss=2.579522848,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:05:57,147 > Epoch: 920,  train_loss=0.000264779,  val_loss=2.572043657,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:06:45,154 > Epoch: 921,  train_loss=0.000264486,  val_loss=2.578410864,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:07:33,110 > Epoch: 922,  train_loss=0.000265389,  val_loss=2.574977636,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:08:21,031 > Epoch: 923,  train_loss=0.000250016,  val_loss=2.643273592,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:09:08,949 > Epoch: 924,  train_loss=0.000256752,  val_loss=2.633422136,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:09:56,851 > Epoch: 925,  train_loss=0.000258757,  val_loss=2.621796608,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:10:44,738 > Epoch: 926,  train_loss=0.000260517,  val_loss=2.612316847,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:11:32,765 > Epoch: 927,  train_loss=0.000261806,  val_loss=2.618657112,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:12:20,705 > Epoch: 928,  train_loss=0.000263823,  val_loss=2.594515085,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:13:08,676 > Epoch: 929,  train_loss=0.000259575,  val_loss=2.617143869,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:13:56,648 > Epoch: 930,  train_loss=0.000262258,  val_loss=2.617526054,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:14:44,604 > Epoch: 931,  train_loss=0.000263251,  val_loss=2.605601788,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:15:32,527 > Epoch: 932,  train_loss=0.000264255,  val_loss=2.606784344,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:16:20,484 > Epoch: 933,  train_loss=0.000250523,  val_loss=2.681126595,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:17:08,395 > Epoch: 934,  train_loss=0.000256474,  val_loss=2.674141884,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:17:56,371 > Epoch: 935,  train_loss=0.000259824,  val_loss=2.655089378,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:18:44,403 > Epoch: 936,  train_loss=0.000261785,  val_loss=2.647251844,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:19:32,358 > Epoch: 937,  train_loss=0.000263081,  val_loss=2.665660381,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:20:20,383 > Epoch: 938,  train_loss=0.000266462,  val_loss=2.647528887,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:21:08,282 > Epoch: 939,  train_loss=0.000265479,  val_loss=2.638120174,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:21:56,172 > Epoch: 940,  train_loss=0.000265745,  val_loss=2.632370710,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:22:44,157 > Epoch: 941,  train_loss=0.000265377,  val_loss=2.621309519,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:23:32,058 > Epoch: 942,  train_loss=0.000267066,  val_loss=2.610127211,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:24:20,030 > Epoch: 943,  train_loss=0.000253036,  val_loss=2.688955307,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:25:08,075 > Epoch: 944,  train_loss=0.000255625,  val_loss=2.691654205,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 05:25:56,046 > Epoch: 945,  train_loss=0.000253347,  val_loss=2.696335554,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 05:26:44,015 > Epoch: 946,  train_loss=0.000257649,  val_loss=2.689596653,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:27:31,989 > Epoch: 947,  train_loss=0.000259898,  val_loss=2.681439877,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:28:19,921 > Epoch: 948,  train_loss=0.000262093,  val_loss=2.674373388,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:29:07,896 > Epoch: 949,  train_loss=0.000263012,  val_loss=2.678287029,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:29:55,806 > Epoch: 950,  train_loss=0.000263722,  val_loss=2.685055494,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:30:43,810 > Epoch: 951,  train_loss=0.000263930,  val_loss=2.677761316,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:31:31,799 > Epoch: 952,  train_loss=0.000263775,  val_loss=2.671344757,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:32:19,815 > Epoch: 953,  train_loss=0.000262923,  val_loss=2.676559210,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:33:07,812 > Epoch: 954,  train_loss=0.000264189,  val_loss=2.675301313,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:33:55,733 > Epoch: 955,  train_loss=0.000249428,  val_loss=2.760250330,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:34:43,697 > Epoch: 956,  train_loss=0.000251822,  val_loss=2.766778946,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:35:31,636 > Epoch: 957,  train_loss=0.000257389,  val_loss=2.746604443,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:36:19,562 > Epoch: 958,  train_loss=0.000258636,  val_loss=2.743509531,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:37:07,558 > Epoch: 959,  train_loss=0.000261005,  val_loss=2.746407747,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:37:55,522 > Epoch: 960,  train_loss=0.000262571,  val_loss=2.731220245,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:38:43,551 > Epoch: 961,  train_loss=0.000263423,  val_loss=2.724062204,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:39:31,541 > Epoch: 962,  train_loss=0.000263963,  val_loss=2.713828802,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:40:19,477 > Epoch: 963,  train_loss=0.000264313,  val_loss=2.705365896,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:41:07,393 > Epoch: 964,  train_loss=0.000263361,  val_loss=2.716758013,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:41:55,298 > Epoch: 965,  train_loss=0.000264043,  val_loss=2.713638067,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:42:43,295 > Epoch: 966,  train_loss=0.000250575,  val_loss=2.787658691,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:43:31,297 > Epoch: 967,  train_loss=0.000250471,  val_loss=2.790449619,  accuracy=0.875000000
[INFO|asb_main.py:823] 2018-10-25 05:44:19,284 > Epoch: 968,  train_loss=0.000251653,  val_loss=2.786250591,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:45:07,292 > Epoch: 969,  train_loss=0.000256184,  val_loss=2.778819084,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:45:55,279 > Epoch: 970,  train_loss=0.000259166,  val_loss=2.770809650,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:46:43,282 > Epoch: 971,  train_loss=0.000260958,  val_loss=2.767773867,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:47:31,201 > Epoch: 972,  train_loss=0.000261797,  val_loss=2.764242649,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:48:19,117 > Epoch: 973,  train_loss=0.000262976,  val_loss=2.772175312,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:49:07,082 > Epoch: 974,  train_loss=0.000262955,  val_loss=2.758796215,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:49:55,079 > Epoch: 975,  train_loss=0.000262818,  val_loss=2.751908064,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:50:43,029 > Epoch: 976,  train_loss=0.000247652,  val_loss=2.823798895,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:51:31,032 > Epoch: 977,  train_loss=0.000256649,  val_loss=2.801217794,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:52:19,012 > Epoch: 978,  train_loss=0.000254439,  val_loss=2.800710917,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:53:06,917 > Epoch: 979,  train_loss=0.000254072,  val_loss=2.810698271,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:53:54,889 > Epoch: 980,  train_loss=0.000257373,  val_loss=2.807418108,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:54:42,832 > Epoch: 981,  train_loss=0.000259779,  val_loss=2.801639795,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:55:30,797 > Epoch: 982,  train_loss=0.000260388,  val_loss=2.798726797,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:56:18,763 > Epoch: 983,  train_loss=0.000261076,  val_loss=2.791943550,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:57:06,731 > Epoch: 984,  train_loss=0.000261068,  val_loss=2.787122011,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:57:54,709 > Epoch: 985,  train_loss=0.000244505,  val_loss=2.878011703,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:58:42,650 > Epoch: 986,  train_loss=0.000252162,  val_loss=2.874022007,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 05:59:30,551 > Epoch: 987,  train_loss=0.000248129,  val_loss=2.891947269,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:00:18,479 > Epoch: 988,  train_loss=0.000254001,  val_loss=2.884363651,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:01:06,367 > Epoch: 989,  train_loss=0.000259918,  val_loss=2.865893364,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:01:54,323 > Epoch: 990,  train_loss=0.000260727,  val_loss=2.842421770,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:02:42,327 > Epoch: 991,  train_loss=0.000261962,  val_loss=2.830844879,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:03:30,300 > Epoch: 992,  train_loss=0.000263031,  val_loss=2.822908640,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:04:18,293 > Epoch: 993,  train_loss=0.000262488,  val_loss=2.822708607,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:05:06,313 > Epoch: 994,  train_loss=0.000261852,  val_loss=2.819433212,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:05:54,250 > Epoch: 995,  train_loss=0.000264551,  val_loss=2.810564041,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:06:42,157 > Epoch: 996,  train_loss=0.000251093,  val_loss=2.886585474,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:07:30,095 > Epoch: 997,  train_loss=0.000254030,  val_loss=2.874415398,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:08:18,039 > Epoch: 998,  train_loss=0.000249301,  val_loss=2.885294437,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:09:06,021 > Epoch: 999,  train_loss=0.000251481,  val_loss=2.872968912,  accuracy=0.867187500
[INFO|asb_main.py:823] 2018-10-25 06:09:53,989 > Epoch:1000,  train_loss=0.000255097,  val_loss=2.873640776,  accuracy=0.867187500
[INFO|asb_main.py:833] 2018-10-25 06:09:53,989 > ### model_212 Learning Finished!
[INFO|asb_main.py:1235] 2018-10-25 06:09:53,992 > model_212 Program end [ Total time : 13 Hour 19 Minute 24 Second ]
